
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="diffusion-models-intro.html">
      
      
        <link rel="next" href="vit-dit.html">
      
      
        
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.2">
    
    
      
        <title>Transformer - AI Notes</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.484c7ddc.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#transformer" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../index.html" title="AI Notes" class="md-header__button md-logo" aria-label="AI Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            AI Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Transformer
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../index.html" title="AI Notes" class="md-nav__button md-logo" aria-label="AI Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    AI Notes
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Posts
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Posts
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="diffusion-models-intro.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Diffusion Models Intro
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Transformer
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="transformer.html" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Transformer
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#what-is-the-model-mathematical-representation" class="md-nav__link">
    <span class="md-ellipsis">
      
        What is the model? (Mathematical representation)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="What is the model? (Mathematical representation)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#conditional-sequence-modeling-seq2seq-translation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Conditional sequence modeling (seq2seq / translation)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#autoregressive-language-modeling-decoder-only" class="md-nav__link">
    <span class="md-ellipsis">
      
        Autoregressive language modeling (decoder-only)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#discussion" class="md-nav__link">
    <span class="md-ellipsis">
      
        Discussion
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention-self-attention-multi-head" class="md-nav__link">
    <span class="md-ellipsis">
      
        Attention / Self-Attention / Multi-Head
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Attention / Self-Attention / Multi-Head">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#scaled-dot-product-attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Scaled dot-product attention
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-attention-vs-cross-attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Self-attention vs cross-attention
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-the-1sqrtd_k-scale" class="md-nav__link">
    <span class="md-ellipsis">
      
        Why the \(1/\sqrt{d_k}\) scale?
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Why the \(1/\sqrt{d_k}\) scale?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-to-kill-the-sqrtd_k-where-to-put-the-scale" class="md-nav__link">
    <span class="md-ellipsis">
      
        How to “kill” the \(\sqrt{d_k}\) (where to put the scale)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#does-this-relate-to-temperature-or-top-k" class="md-nav__link">
    <span class="md-ellipsis">
      
        Does this relate to temperature or top-\(k\)?
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-head-attention-mha" class="md-nav__link">
    <span class="md-ellipsis">
      
        Multi-head attention (MHA)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#minimal-pytorch-code-self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Minimal PyTorch code (self-attention)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#notes-on-compute" class="md-nav__link">
    <span class="md-ellipsis">
      
        Notes on compute
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#discussion-compute" class="md-nav__link">
    <span class="md-ellipsis">
      
        Discussion (Compute)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#normalization-layernorm-rmsnorm" class="md-nav__link">
    <span class="md-ellipsis">
      
        Normalization (LayerNorm / RMSNorm)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Normalization (LayerNorm / RMSNorm)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#layernorm-ln" class="md-nav__link">
    <span class="md-ellipsis">
      
        LayerNorm (LN)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pre-ln-vs-post-ln-where-ln-sits" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pre-LN vs Post-LN (where LN sits)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rmsnorm-common-modern-variant" class="md-nav__link">
    <span class="md-ellipsis">
      
        RMSNorm (common modern variant)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#positional-encoding-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        Positional encoding / embeddings
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Positional encoding / embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#original-sinusoidal-positional-encoding-vaswani-et-al-2017" class="md-nav__link">
    <span class="md-ellipsis">
      
        Original sinusoidal positional encoding (Vaswani et al., 2017)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#angle-phase-interpretation-why-sincos-helps" class="md-nav__link">
    <span class="md-ellipsis">
      
        “Angle / phase” interpretation (why sin/cos helps)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rope-rotary-positional-embedding" class="md-nav__link">
    <span class="md-ellipsis">
      
        RoPE (Rotary Positional Embedding)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#causal-masking-and-generative-vs-discriminative-discussion" class="md-nav__link">
    <span class="md-ellipsis">
      
        Causal masking and “generative vs discriminative” discussion
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Causal masking and “generative vs discriminative” discussion">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-the-causal-mask" class="md-nav__link">
    <span class="md-ellipsis">
      
        What is the causal mask?
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-does-causal-masking-make-it-a-generative-model" class="md-nav__link">
    <span class="md-ellipsis">
      
        Why does causal masking make it a generative model?
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax-does-not-decide-generative-vs-discriminative" class="md-nav__link">
    <span class="md-ellipsis">
      
        Softmax does not decide generative vs discriminative
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#where-does-bert-fit" class="md-nav__link">
    <span class="md-ellipsis">
      
        Where does BERT fit?
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#minimal-code-an-attention-block-with-residual-pathway" class="md-nav__link">
    <span class="md-ellipsis">
      
        Minimal code: an attention block (with residual pathway)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Minimal code: an attention block (with residual pathway)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#passway-the-resnet-style-skip-connection-in-this-context" class="md-nav__link">
    <span class="md-ellipsis">
      
        “Passway” = the ResNet-style skip connection (in this context)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relation-to-odes-depth-as-time-intuition" class="md-nav__link">
    <span class="md-ellipsis">
      
        Relation to ODEs (depth-as-time intuition)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#guide-how-far-this-is-from-gpt-3-level-detail-and-whats-next" class="md-nav__link">
    <span class="md-ellipsis">
      
        Guide: how far this is from GPT-3-level detail (and what’s next)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      
        References
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="vit-dit.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ViT and DiT
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#what-is-the-model-mathematical-representation" class="md-nav__link">
    <span class="md-ellipsis">
      
        What is the model? (Mathematical representation)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="What is the model? (Mathematical representation)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#conditional-sequence-modeling-seq2seq-translation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Conditional sequence modeling (seq2seq / translation)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#autoregressive-language-modeling-decoder-only" class="md-nav__link">
    <span class="md-ellipsis">
      
        Autoregressive language modeling (decoder-only)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#discussion" class="md-nav__link">
    <span class="md-ellipsis">
      
        Discussion
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention-self-attention-multi-head" class="md-nav__link">
    <span class="md-ellipsis">
      
        Attention / Self-Attention / Multi-Head
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Attention / Self-Attention / Multi-Head">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#scaled-dot-product-attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Scaled dot-product attention
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-attention-vs-cross-attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Self-attention vs cross-attention
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-the-1sqrtd_k-scale" class="md-nav__link">
    <span class="md-ellipsis">
      
        Why the \(1/\sqrt{d_k}\) scale?
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Why the \(1/\sqrt{d_k}\) scale?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-to-kill-the-sqrtd_k-where-to-put-the-scale" class="md-nav__link">
    <span class="md-ellipsis">
      
        How to “kill” the \(\sqrt{d_k}\) (where to put the scale)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#does-this-relate-to-temperature-or-top-k" class="md-nav__link">
    <span class="md-ellipsis">
      
        Does this relate to temperature or top-\(k\)?
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-head-attention-mha" class="md-nav__link">
    <span class="md-ellipsis">
      
        Multi-head attention (MHA)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#minimal-pytorch-code-self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Minimal PyTorch code (self-attention)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#notes-on-compute" class="md-nav__link">
    <span class="md-ellipsis">
      
        Notes on compute
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#discussion-compute" class="md-nav__link">
    <span class="md-ellipsis">
      
        Discussion (Compute)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#normalization-layernorm-rmsnorm" class="md-nav__link">
    <span class="md-ellipsis">
      
        Normalization (LayerNorm / RMSNorm)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Normalization (LayerNorm / RMSNorm)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#layernorm-ln" class="md-nav__link">
    <span class="md-ellipsis">
      
        LayerNorm (LN)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pre-ln-vs-post-ln-where-ln-sits" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pre-LN vs Post-LN (where LN sits)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rmsnorm-common-modern-variant" class="md-nav__link">
    <span class="md-ellipsis">
      
        RMSNorm (common modern variant)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#positional-encoding-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        Positional encoding / embeddings
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Positional encoding / embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#original-sinusoidal-positional-encoding-vaswani-et-al-2017" class="md-nav__link">
    <span class="md-ellipsis">
      
        Original sinusoidal positional encoding (Vaswani et al., 2017)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#angle-phase-interpretation-why-sincos-helps" class="md-nav__link">
    <span class="md-ellipsis">
      
        “Angle / phase” interpretation (why sin/cos helps)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rope-rotary-positional-embedding" class="md-nav__link">
    <span class="md-ellipsis">
      
        RoPE (Rotary Positional Embedding)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#causal-masking-and-generative-vs-discriminative-discussion" class="md-nav__link">
    <span class="md-ellipsis">
      
        Causal masking and “generative vs discriminative” discussion
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Causal masking and “generative vs discriminative” discussion">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-the-causal-mask" class="md-nav__link">
    <span class="md-ellipsis">
      
        What is the causal mask?
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-does-causal-masking-make-it-a-generative-model" class="md-nav__link">
    <span class="md-ellipsis">
      
        Why does causal masking make it a generative model?
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax-does-not-decide-generative-vs-discriminative" class="md-nav__link">
    <span class="md-ellipsis">
      
        Softmax does not decide generative vs discriminative
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#where-does-bert-fit" class="md-nav__link">
    <span class="md-ellipsis">
      
        Where does BERT fit?
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#minimal-code-an-attention-block-with-residual-pathway" class="md-nav__link">
    <span class="md-ellipsis">
      
        Minimal code: an attention block (with residual pathway)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Minimal code: an attention block (with residual pathway)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#passway-the-resnet-style-skip-connection-in-this-context" class="md-nav__link">
    <span class="md-ellipsis">
      
        “Passway” = the ResNet-style skip connection (in this context)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relation-to-odes-depth-as-time-intuition" class="md-nav__link">
    <span class="md-ellipsis">
      
        Relation to ODEs (depth-as-time intuition)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#guide-how-far-this-is-from-gpt-3-level-detail-and-whats-next" class="md-nav__link">
    <span class="md-ellipsis">
      
        Guide: how far this is from GPT-3-level detail (and what’s next)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      
        References
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="transformer">Transformer<a class="headerlink" href="#transformer" title="Permanent link">&para;</a></h1>
<ul>
<li>Paper: https://arxiv.org/abs/1706.03762 (Vaswani et al., 2017, <em>Attention Is All You Need</em>)</li>
<li>Original code (Tensor2Tensor): https://github.com/tensorflow/tensor2tensor</li>
</ul>
<p>Scope note: we focus on techniques relevant to modern LLMs (primarily decoder-only / autoregressive Transformers); encoder-only architectures (e.g., BERT) and purely supervised encoder models are out of scope here.</p>
<h2 id="what-is-the-model-mathematical-representation">What is the model? (Mathematical representation)<a class="headerlink" href="#what-is-the-model-mathematical-representation" title="Permanent link">&para;</a></h2>
<p>In standard NLP usage, you plug the Transformer into a familiar likelihood:</p>
<h3 id="conditional-sequence-modeling-seq2seq-translation">Conditional sequence modeling (seq2seq / translation)<a class="headerlink" href="#conditional-sequence-modeling-seq2seq-translation" title="Permanent link">&para;</a></h3>
<ul>
<li>Definition (seq2seq): https://en.wikipedia.org/wiki/Seq2seq</li>
</ul>
<p>Given a source sequence <span class="arithmatex">\(x\)</span> and a target sequence <span class="arithmatex">\(y = (y_1, \ldots, y_T)\)</span>, we model</p>
<div class="arithmatex">\[
p_\theta(y \mid x) = \prod_{t=1}^{T} p_\theta\bigl(y_t \mid y_{&lt;t}, x\bigr)
\]</div>
<p>Training is maximum likelihood (cross-entropy) with teacher forcing:</p>
<div class="arithmatex">\[
θ^* \,=\, \arg\max_\theta \prod_{(x,y)\in\mathcal{D}} p_\theta(y\mid x)
\,=\, \arg\max_\theta \sum_{(x,y)\in\mathcal{D}} \log p_\theta(y\mid x)
\,=\, \arg\max_\theta \sum_{(x,y)\in\mathcal{D}} \sum_{t=1}^{T} \log p_\theta\bigl(y_t \mid y_{&lt;t}, x\bigr)
\]</div>
<h3 id="autoregressive-language-modeling-decoder-only">Autoregressive language modeling (decoder-only)<a class="headerlink" href="#autoregressive-language-modeling-decoder-only" title="Permanent link">&para;</a></h3>
<p>If there is no conditioning input <span class="arithmatex">\(x\)</span>, the same factorization becomes</p>
<div class="arithmatex">\[
p_\theta(y) = \prod_{t=1}^{T} p_\theta\bigl(y_t \mid y_{&lt;t}\bigr)
\]</div>
<p>This is the “AR” view: the Transformer block is just a parameterization of the conditional distributions via masked self-attention.</p>
<h3 id="discussion">Discussion<a class="headerlink" href="#discussion" title="Permanent link">&para;</a></h3>
<ul>
<li>No explicit latent probabilistic state: unlike classical sequence models such as HMMs or state-space/Markov processes, the standard Transformer LM does not introduce a stochastic hidden state <span class="arithmatex">\(z_t\)</span> with an explicit transition <span class="arithmatex">\(p(z_t\mid z_{t-1})\)</span>. The “hidden states” in a Transformer are deterministic neural representations produced by forward computation, so there is no marginalization/inference over latent variables in the probabilistic formulation.</li>
<li>Truncation is usually practical, not theoretical: the chain-rule factorization conditions on the entire prefix <span class="arithmatex">\(y_{&lt;t}\)</span> in principle. In practice, models operate under a finite context window (and sometimes additional approximations such as sparse/blocked attention or external memory), which effectively limits what information the model can condition on.</li>
<li>Decoding choices: implementations often expose top-<span class="arithmatex">\(k\)</span> (or top-<span class="arithmatex">\(p\)</span>) sampling, applied at the final next-token distribution (logits/softmax); this usually does not reduce the forward-pass compute per step, but can simplify the sampling step by restricting to a subset of the vocabulary.</li>
<li>Beam search: an alternative decoding procedure that keeps multiple partial hypotheses and selects the highest-scoring sequence under the model, rather than sampling a single next token.</li>
<li>Why self-attention matters next: self-attention is the key computation that constructs a context-dependent representation of the prefix, which is then used to parameterize each conditional distribution <span class="arithmatex">\(p_\theta(y_t\mid \cdot)\)</span>.</li>
</ul>
<h2 id="attention-self-attention-multi-head">Attention / Self-Attention / Multi-Head<a class="headerlink" href="#attention-self-attention-multi-head" title="Permanent link">&para;</a></h2>
<h3 id="scaled-dot-product-attention">Scaled dot-product attention<a class="headerlink" href="#scaled-dot-product-attention" title="Permanent link">&para;</a></h3>
<p>Attention takes queries <span class="arithmatex">\(Q\)</span>, keys <span class="arithmatex">\(K\)</span>, values <span class="arithmatex">\(V\)</span>, and returns a weighted sum of values. The weights are content-dependent similarities between queries and keys.</p>
<div class="arithmatex">\[
\mathrm{Attn}(Q,K,V;M) \,=\, \mathrm{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}} + M\right) V
\]</div>
<ul>
<li><span class="arithmatex">\(d_k\)</span> is the key/query dimension per head.</li>
<li><span class="arithmatex">\(M\)</span> is an additive mask (typically <span class="arithmatex">\(0\)</span> for allowed positions and <span class="arithmatex">\(-\infty\)</span> for disallowed ones). For decoder-only LLMs, <span class="arithmatex">\(M\)</span> encodes the causal constraint “do not attend to the future”.</li>
</ul>
<p><img src="../assets/figures/fig1.png" alt="Fig 1. Scaled dot-product attention" width="50%" /></p>
<h3 id="self-attention-vs-cross-attention">Self-attention vs cross-attention<a class="headerlink" href="#self-attention-vs-cross-attention" title="Permanent link">&para;</a></h3>
<ul>
<li>Self-attention: <span class="arithmatex">\(Q,K,V\)</span> are all computed from the same sequence representation <span class="arithmatex">\(X\)</span>.</li>
<li>Cross-attention (encoder-decoder): <span class="arithmatex">\(Q\)</span> comes from the decoder states, while <span class="arithmatex">\(K,V\)</span> come from the encoder outputs.</li>
</ul>
<p>In practice you form <span class="arithmatex">\(Q,K,V\)</span> by learned linear projections, e.g. <span class="arithmatex">\(Q = XW_Q\)</span>, <span class="arithmatex">\(K = XW_K\)</span>, <span class="arithmatex">\(V = XW_V\)</span>.</p>
<h3 id="why-the-1sqrtd_k-scale">Why the <span class="arithmatex">\(1/\sqrt{d_k}\)</span> scale?<a class="headerlink" href="#why-the-1sqrtd_k-scale" title="Permanent link">&para;</a></h3>
<p>Without scaling, the dot products <span class="arithmatex">\(QK^\top\)</span> tend to grow with <span class="arithmatex">\(d_k\)</span>, pushing softmax into saturation and making optimization harder. The scale roughly stabilizes the magnitude.</p>
<p>A common (approximate) variance argument is:</p>
<div class="arithmatex">\[
s \,=\, q^\top k \,=\, \sum_{i=1}^{d_k} q_i k_i
\]</div>
<p>Assume components are roughly zero-mean with unit variance and weakly correlated, e.g. <span class="arithmatex">\(q_i, k_i\)</span> behave like i.i.d. with <span class="arithmatex">\(\mathbb{E}[q_i]=\mathbb{E}[k_i]=0\)</span>, <span class="arithmatex">\(\mathrm{Var}(q_i)=\mathrm{Var}(k_i)=1\)</span>. Then <span class="arithmatex">\(\mathbb{E}[q_i k_i]=0\)</span> and</p>
<div class="arithmatex">\[
\mathrm{Var}(s) \approx \sum_{i=1}^{d_k} \mathrm{Var}(q_i k_i) \approx d_k.
\]</div>
<p>So <span class="arithmatex">\(s\)</span> has typical scale <span class="arithmatex">\(\sqrt{d_k}\)</span>. Dividing by <span class="arithmatex">\(\sqrt{d_k}\)</span> makes the logits fed to softmax stay <span class="arithmatex">\(\mathcal{O}(1)\)</span>:</p>
<div class="arithmatex">\[
\frac{s}{\sqrt{d_k}} \text{ has } \mathrm{Var}\!\left(\frac{s}{\sqrt{d_k}}\right) \approx 1.
\]</div>
<p>This is not a fully rigorous derivation (the learned projections and LayerNorm change the exact statistics), but it explains why scaling helps: it reduces softmax saturation and keeps gradients healthier.</p>
<p>Concrete example: if <span class="arithmatex">\(d_k=64\)</span>, the unscaled dot-product has a typical magnitude around <span class="arithmatex">\(\sqrt{64}=8\)</span>. Softmax over numbers with magnitude <span class="arithmatex">\(\sim 8\)</span> can become very peaky early in training; scaling brings those logits back to a gentler range.</p>
<h4 id="how-to-kill-the-sqrtd_k-where-to-put-the-scale">How to “kill” the <span class="arithmatex">\(\sqrt{d_k}\)</span> (where to put the scale)<a class="headerlink" href="#how-to-kill-the-sqrtd_k-where-to-put-the-scale" title="Permanent link">&para;</a></h4>
<p>You can’t really make the <span class="arithmatex">\(\sqrt{d_k}\)</span> issue disappear; you can only <em>move the scale</em> so the attention logits stay <span class="arithmatex">\(\mathcal{O}(1)\)</span>.
The standard choice is the explicit <span class="arithmatex">\(1/\sqrt{d_k}\)</span>, but there are equivalent alternatives:</p>
<ul>
<li>
<p>Use an explicit (possibly learnable) temperature: <span class="arithmatex">\(\mathrm{softmax}((QK^T)\cdot \alpha + M)V\)</span>, where <span class="arithmatex">\(\alpha\)</span> is a scalar (or per-head scalar). Initialize <span class="arithmatex">\(\alpha=1/\sqrt{d_k}\)</span>. Some implementations keep <span class="arithmatex">\(\alpha\)</span> fixed; others learn it (a learnable “temperature” <span class="arithmatex">\(τ\)</span>).</p>
</li>
<li>
<p>Normalize queries/keys (cosine attention): replace <span class="arithmatex">\(q,k\)</span> with <span class="arithmatex">\(\hat q=q/\|q\|\)</span> and <span class="arithmatex">\(\hat k=k/\|k\|\)</span>. Then <span class="arithmatex">\(\hat q\cdot \hat k\in[-1,1]\)</span>, so logits no longer grow with <span class="arithmatex">\(d_k\)</span>. In practice you often re-introduce a learnable scale <span class="arithmatex">\(g\)</span> (temperature) because <span class="arithmatex">\([-1,1]\)</span> may be too “cold” for sharp attention.</p>
</li>
<li>
<p>Bake the scale into the projections: if the components of <span class="arithmatex">\(q\)</span> and <span class="arithmatex">\(k\)</span> have variance <span class="arithmatex">\(\sigma^2\)</span>, then <span class="arithmatex">\(\mathrm{Var}(q\cdot k) \approx d_k\,\sigma^4\)</span>. Picking <span class="arithmatex">\(\sigma^2\approx 1/\sqrt{d_k}\)</span> (i.e., std <span class="arithmatex">\(\approx d_k^{-1/4}\)</span> for both <span class="arithmatex">\(q\)</span> and <span class="arithmatex">\(k\)</span>) keeps <span class="arithmatex">\(q\cdot k\)</span> at <span class="arithmatex">\(\mathcal{O}(1)\)</span> even without dividing by <span class="arithmatex">\(\sqrt{d_k}\)</span>. This is equivalent to putting a factor <span class="arithmatex">\(d_k^{-1/4}\)</span> into both <span class="arithmatex">\(W_Q\)</span> and <span class="arithmatex">\(W_K\)</span>.</p>
</li>
</ul>
<p>Important nuance: LayerNorm/RMSNorm stabilizes the <em>residual stream</em> scale, but it does not automatically control the statistics of
the attention logits in a way that removes the need for a temperature/scale.</p>
<h4 id="does-this-relate-to-temperature-or-top-k">Does this relate to temperature or top-<span class="arithmatex">\(k\)</span>?<a class="headerlink" href="#does-this-relate-to-temperature-or-top-k" title="Permanent link">&para;</a></h4>
<ul>
<li>Temperature: you can view the attention softmax as having a fixed “temperature” <span class="arithmatex">\(\tau\)</span>, since <span class="arithmatex">\(\mathrm{softmax}(s/\sqrt{d_k})\)</span> is equivalent to <span class="arithmatex">\(\mathrm{softmax}(s/\tau)\)</span> with <span class="arithmatex">\(\tau=\sqrt{d_k}\)</span>. This is separate from decoding temperature, which is applied to the final vocabulary logits for next-token sampling.</li>
<li>Top-<span class="arithmatex">\(k\)</span>: top-<span class="arithmatex">\(k\)</span> sampling is a decoding-time heuristic applied to the vocabulary distribution (keep the top <span class="arithmatex">\(k\)</span> tokens, renormalize). Standard attention does not use top-<span class="arithmatex">\(k\)</span> over positions; it computes a full distribution over allowed positions. (There are research/engineering variants like sparse/top-<span class="arithmatex">\(k\)</span> attention, but that’s a different idea.)</li>
</ul>
<h3 id="multi-head-attention-mha">Multi-head attention (MHA)<a class="headerlink" href="#multi-head-attention-mha" title="Permanent link">&para;</a></h3>
<p>Multi-head attention runs several attention “heads” in parallel with different learned projections, then concatenates their outputs:</p>
<div class="arithmatex">\[
\mathrm{MHA}(X) \,=\, \mathrm{Concat}\bigl(\mathrm{Attn}(Q_1,K_1,V_1;M),\ldots,\mathrm{Attn}(Q_h,K_h,V_h;M)\bigr) W_O
\]</div>
<p>Intuition: different heads can specialize (local vs global dependencies, syntax vs coreference, etc.).</p>
<p><img src="../assets/figures/fig2.png" alt="Fig 2. Multi-head attention" width="50%" /></p>
<h3 id="minimal-pytorch-code-self-attention">Minimal PyTorch code (self-attention)<a class="headerlink" href="#minimal-pytorch-code-self-attention" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="k">class</span><span class="w"> </span><span class="nc">SimpleMHA</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Minimal multi-head self-attention with fused QKV projection.&quot;&quot;&quot;</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">n_heads</span> <span class="o">==</span> <span class="mi">0</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">n_heads</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">qkv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a>        <span class="c1"># x: (B, T, C)</span>
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a>        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>        <span class="n">qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qkv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B, T, 3C)</span>
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">qkv</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a>
<a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a>        <span class="c1"># (B, nh, T, hs)</span>
<a id="__codelineno-0-26" name="__codelineno-0-26" href="#__codelineno-0-26"></a>        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-0-27" name="__codelineno-0-27" href="#__codelineno-0-27"></a>        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-0-28" name="__codelineno-0-28" href="#__codelineno-0-28"></a>        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-0-29" name="__codelineno-0-29" href="#__codelineno-0-29"></a>
<a id="__codelineno-0-30" name="__codelineno-0-30" href="#__codelineno-0-30"></a>        <span class="n">att</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>  <span class="c1"># (B, nh, T, T)</span>
<a id="__codelineno-0-31" name="__codelineno-0-31" href="#__codelineno-0-31"></a>        <span class="k">if</span> <span class="n">causal</span><span class="p">:</span>
<a id="__codelineno-0-32" name="__codelineno-0-32" href="#__codelineno-0-32"></a>            <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">))</span>
<a id="__codelineno-0-33" name="__codelineno-0-33" href="#__codelineno-0-33"></a>            <span class="n">att</span> <span class="o">=</span> <span class="n">att</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">))</span>
<a id="__codelineno-0-34" name="__codelineno-0-34" href="#__codelineno-0-34"></a>
<a id="__codelineno-0-35" name="__codelineno-0-35" href="#__codelineno-0-35"></a>        <span class="n">att</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">att</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-36" name="__codelineno-0-36" href="#__codelineno-0-36"></a>        <span class="n">att</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">att</span><span class="p">)</span>
<a id="__codelineno-0-37" name="__codelineno-0-37" href="#__codelineno-0-37"></a>        <span class="n">y</span> <span class="o">=</span> <span class="n">att</span> <span class="o">@</span> <span class="n">v</span>  <span class="c1"># (B, nh, T, hs)</span>
<a id="__codelineno-0-38" name="__codelineno-0-38" href="#__codelineno-0-38"></a>
<a id="__codelineno-0-39" name="__codelineno-0-39" href="#__codelineno-0-39"></a>        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>  <span class="c1"># merge heads</span>
<a id="__codelineno-0-40" name="__codelineno-0-40" href="#__codelineno-0-40"></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<a id="__codelineno-0-41" name="__codelineno-0-41" href="#__codelineno-0-41"></a>
<a id="__codelineno-0-42" name="__codelineno-0-42" href="#__codelineno-0-42"></a>
<a id="__codelineno-0-43" name="__codelineno-0-43" href="#__codelineno-0-43"></a><span class="c1"># Example:</span>
<a id="__codelineno-0-44" name="__codelineno-0-44" href="#__codelineno-0-44"></a><span class="c1"># x = torch.randn(2, 128, 768)</span>
<a id="__codelineno-0-45" name="__codelineno-0-45" href="#__codelineno-0-45"></a><span class="c1"># y = SimpleMHA(768, 12)(x, causal=True)</span>
</code></pre></div>
<h3 id="notes-on-compute">Notes on compute<a class="headerlink" href="#notes-on-compute" title="Permanent link">&para;</a></h3>
<ul>
<li>The attention matrix is <span class="arithmatex">\(T\times T\)</span> for sequence length <span class="arithmatex">\(T\)</span>, so naive attention costs <span class="arithmatex">\(O(T^2)\)</span> memory/time per layer.</li>
<li>During autoregressive decoding, KV cache avoids recomputing past <span class="arithmatex">\(K,V\)</span> and makes each new token step incremental.</li>
</ul>
<h3 id="discussion-compute">Discussion (Compute)<a class="headerlink" href="#discussion-compute" title="Permanent link">&para;</a></h3>
<ul>
<li>Compute reality check (what actually gets multiplied): for a decoder-only Transformer layer with batch <span class="arithmatex">\(B\)</span>,
  sequence length <span class="arithmatex">\(T\)</span>, model width <span class="arithmatex">\(d_{model}\)</span>, and <span class="arithmatex">\(h\)</span> heads (so <span class="arithmatex">\(d_{head}=d_{model}/h\)</span>), the main
  multiplications are:</li>
<li>QKV projection: <span class="arithmatex">\(X\in\mathbb{R}^{B\times T\times d_{model}}\)</span> times <span class="arithmatex">\(W_{QKV}\in\mathbb{R}^{d_{model}\times 3d_{model}}\)</span>
    <span class="arithmatex">\(\Rightarrow\)</span> about <span class="arithmatex">\(3\,B\,T\,d_{model}^2\)</span> scalar multiplies.</li>
<li>Attention scores: <span class="arithmatex">\(QK^\top\)</span> per head is <span class="arithmatex">\((T\times d_{head})\cdot(d_{head}\times T)\Rightarrow (T\times T)\)</span>, so across all
    heads it is about <span class="arithmatex">\(B\,h\,T^2\,d_{head}=B\,T^2\,d_{model}\)</span> multiplies.<ul>
<li>What “each head computes” concretely:<ul>
<li>For a fixed head <span class="arithmatex">\(i\)</span>, each score entry between a query position <span class="arithmatex">\(t\)</span> and a key position <span class="arithmatex">\(s\)</span> is a dot product between two
    <span class="arithmatex">\(d_{head}\)</span>-dimensional vectors: <span class="arithmatex">\(q_t^{(i)}\cdot k_s^{(i)}\)</span>. Computing one score uses <span class="arithmatex">\(d_{head}\)</span> scalar multiplications and
    <span class="arithmatex">\(d_{head}-1\)</span> scalar additions (sum-reduction).</li>
<li>The softmax turns the <span class="arithmatex">\(T\)</span> scores for a fixed query position <span class="arithmatex">\(t\)</span> into a weight vector <span class="arithmatex">\(\alpha_{t,1:T}^{(i)}\)</span> over positions.</li>
<li>The head output at position <span class="arithmatex">\(t\)</span> is a weighted sum over the value vectors <span class="arithmatex">\(v_{1:T}^{(i)}\)</span>: it multiplies each <span class="arithmatex">\(v_s^{(i)}\)</span> by
    <span class="arithmatex">\(\alpha_{t,s}^{(i)}\)</span> and sums over <span class="arithmatex">\(s\)</span>. This is exactly the matrix multiply <span class="arithmatex">\(\alpha^{(i)}V^{(i)}\)</span> (often written as
    <span class="arithmatex">\(\mathrm{Attn}\cdot V\)</span>).</li>
</ul>
</li>
</ul>
</li>
<li>Weighted sum: <span class="arithmatex">\(\mathrm{Attn}\cdot V\)</span> has the same order as scores, another <span class="arithmatex">\(B\,T^2\,d_{model}\)</span> multiplies.</li>
<li>Output projection: concatenated heads <span class="arithmatex">\((B\times T\times d_{model})\)</span> times <span class="arithmatex">\(W_O\in\mathbb{R}^{d_{model}\times d_{model}}\)</span>
    <span class="arithmatex">\(\Rightarrow\)</span> about <span class="arithmatex">\(B\,T\,d_{model}^2\)</span> multiplies.</li>
<li>Takeaway: splitting into heads does <em>not</em> automatically reduce total compute at fixed <span class="arithmatex">\(d_{model}\)</span>; it mostly changes
    how compute is structured/parallelized. Naively, the attention matrix itself has <span class="arithmatex">\(B\,h\,T^2\)</span> elements, which is why
    attention is often described as <span class="arithmatex">\(O(T^2)\)</span> in time/memory (unless using optimized kernels like FlashAttention that avoid
    materializing the full <span class="arithmatex">\(T\times T\)</span> matrix).</li>
</ul>
<h2 id="normalization-layernorm-rmsnorm">Normalization (LayerNorm / RMSNorm)<a class="headerlink" href="#normalization-layernorm-rmsnorm" title="Permanent link">&para;</a></h2>
<p>Transformers rely heavily on normalization for stable optimization at depth.</p>
<h3 id="layernorm-ln">LayerNorm (LN)<a class="headerlink" href="#layernorm-ln" title="Permanent link">&para;</a></h3>
<p>LayerNorm normalizes each token’s feature vector across the model dimension. For a vector <span class="arithmatex">\(x\in\mathbb{R}^{d_{model}}\)</span> (one token, one layer):</p>
<div class="arithmatex">\[
\mathrm{LN}(x) = \gamma\odot \frac{x-\mu}{\sqrt{\sigma^2+\epsilon}} + \beta,
\qquad
\mu=\frac{1}{d_{model}}\sum_j x_j,\ \ \sigma^2=\frac{1}{d_{model}}\sum_j (x_j-\mu)^2.
\]</div>
<p>Key properties:</p>
<ul>
<li>It is <em>per-token</em> (no dependence on batch size), so it works well for variable-length sequences and small batches.</li>
<li>The learned <span class="arithmatex">\(\gamma,\beta\)</span> let the network recover useful scales after normalization.</li>
</ul>
<h3 id="pre-ln-vs-post-ln-where-ln-sits">Pre-LN vs Post-LN (where LN sits)<a class="headerlink" href="#pre-ln-vs-post-ln-where-ln-sits" title="Permanent link">&para;</a></h3>
<ul>
<li>Post-LN (original 2017 diagram): <span class="arithmatex">\(y=\mathrm{LN}(x+\mathrm{Sublayer}(x))\)</span>.</li>
<li>Pre-LN (common in modern LLMs): <span class="arithmatex">\(y=x+\mathrm{Sublayer}(\mathrm{LN}(x))\)</span>.</li>
</ul>
<p>Pre-LN typically trains more reliably for very deep models because gradients can flow through the residual path more directly.</p>
<h3 id="rmsnorm-common-modern-variant">RMSNorm (common modern variant)<a class="headerlink" href="#rmsnorm-common-modern-variant" title="Permanent link">&para;</a></h3>
<p>RMSNorm removes the mean-subtraction and normalizes by root-mean-square only (cheaper, often similar in practice):</p>
<div class="arithmatex">\[
\mathrm{RMSNorm}(x) = \gamma\odot \frac{x}{\sqrt{\frac{1}{d_{model}}\sum_j x_j^2 + \epsilon}}.
\]</div>
<p>Relation to the <span class="arithmatex">\(1/\sqrt{d_k}\)</span> attention scale: LN/RMSNorm helps keep activations in a sane range across layers, but attention still needs an
explicit or implicit temperature to prevent dot-product logits from becoming too large as dimensions change.</p>
<h2 id="positional-encoding-embeddings">Positional encoding / embeddings<a class="headerlink" href="#positional-encoding-embeddings" title="Permanent link">&para;</a></h2>
<p>Self-attention alone is permutation-invariant: without position information, the model cannot distinguish sequences that are
just token-permutations. In Transformers, position is typically injected by adding a position-dependent vector to the token
embedding at the input of each layer (or at least the first layer).</p>
<h3 id="original-sinusoidal-positional-encoding-vaswani-et-al-2017">Original sinusoidal positional encoding (Vaswani et al., 2017)<a class="headerlink" href="#original-sinusoidal-positional-encoding-vaswani-et-al-2017" title="Permanent link">&para;</a></h3>
<p>The paper uses a fixed (non-learned) sinusoidal encoding with dimension <span class="arithmatex">\(d_{model}\)</span>. For position <span class="arithmatex">\(pos\)</span> and dimension index
<span class="arithmatex">\(i\in\{0,\ldots, d_{model}/2-1\}\)</span>:</p>
<div class="arithmatex">\[
\mathrm{PE}(pos, 2i) = \sin\bigl(pos / 10000^{2i/d_{model}}\bigr)
\]</div>
<div class="arithmatex">\[
\mathrm{PE}(pos, 2i+1) = \cos\bigl(pos / 10000^{2i/d_{model}}\bigr)
\]</div>
<p>Then the model uses <span class="arithmatex">\(X_{in}(pos) = \mathrm{Embed}(token) + \mathrm{PE}(pos)\)</span>.</p>
<p>Practical note: many modern LLMs use learned absolute positional embeddings, relative position biases, or RoPE; the sinusoidal
version is still useful as a clean reference and can generalize to longer lengths than seen in training.</p>
<h3 id="angle-phase-interpretation-why-sincos-helps">“Angle / phase” interpretation (why sin/cos helps)<a class="headerlink" href="#angle-phase-interpretation-why-sincos-helps" title="Permanent link">&para;</a></h3>
<p>Define a frequency per pair <span class="arithmatex">\(i\)</span>:</p>
<div class="arithmatex">\[
\omega_i = 10000^{-2i/d_{model}}.
\]</div>
<p>Each 2D pair <span class="arithmatex">\((\mathrm{PE}_{2i}, \mathrm{PE}_{2i+1})\)</span> is a unit-circle point at angle <span class="arithmatex">\(\omega_i\,pos\)</span>:</p>
<div class="arithmatex">\[
\bigl[\sin(\omega_i\,pos),\ \cos(\omega_i\,pos)\bigr].
\]</div>
<p>So increasing the position by <span class="arithmatex">\(\Delta\)</span> corresponds to a rotation by angle <span class="arithmatex">\(\omega_i\,\Delta\)</span> in that 2D plane.</p>
<p>Two useful consequences:</p>
<ul>
<li>Relative offsets show up as phase differences: the dot product of the two unit vectors at positions <span class="arithmatex">\(p\)</span> and <span class="arithmatex">\(q\)</span> is
  <span class="arithmatex">\(\cos(\omega_i(p-q))\)</span>. So a simple bilinear form can access functions of <span class="arithmatex">\(p-q\)</span> (relative position), not just <span class="arithmatex">\(p\)</span> itself.</li>
<li>Shift structure is linear in <span class="arithmatex">\(\sin/\cos\)</span>: using trig identities,
  <span class="arithmatex">\(\sin(\omega(p+\Delta))\)</span> and <span class="arithmatex">\(\cos(\omega(p+\Delta))\)</span> can be written as a fixed linear transform of
  <span class="arithmatex">\(\sin(\omega p)\)</span> and <span class="arithmatex">\(\cos(\omega p)\)</span> (with coefficients depending only on <span class="arithmatex">\(\Delta\)</span>). This makes it easier for attention
  layers to learn “relative position” patterns using linear projections.</li>
</ul>
<h3 id="rope-rotary-positional-embedding">RoPE (Rotary Positional Embedding)<a class="headerlink" href="#rope-rotary-positional-embedding" title="Permanent link">&para;</a></h3>
<p>RoPE is a widely used alternative in modern decoder-only LLMs. Instead of <em>adding</em> a positional vector to the token embedding,
it <em>rotates</em> the query/key vectors in 2D subspaces as a function of position. Intuitively, position becomes a phase that lives
inside the attention dot product.</p>
<p>Take one head and one position <span class="arithmatex">\(pos\)</span>. For each 2D pair of channels <span class="arithmatex">\((2i, 2i+1)\)</span>, define a position-dependent angle
<span class="arithmatex">\(θ_i(pos)\)</span> (often using the same frequency schedule as sinusoidal PE):</p>
<div class="arithmatex">\[
θ_i(pos) = pos\cdot 10000^{-2i/d_{head}}.
\]</div>
<p>Apply a rotation to the <span class="arithmatex">\(q\)</span> and <span class="arithmatex">\(k\)</span> components in that 2D plane:</p>
<div class="arithmatex">\[
\begin{bmatrix} q'_{2i} \\ q'_{2i+1} \end{bmatrix}
=
\begin{bmatrix}
\cos(θ_i) &amp; -\sin(θ_i) \\
\sin(θ_i) &amp; \cos(θ_i)
\end{bmatrix}
\begin{bmatrix} q_{2i} \\ q_{2i+1} \end{bmatrix},
\qquad
\begin{bmatrix} k'_{2i} \\ k'_{2i+1} \end{bmatrix}
=
\begin{bmatrix}
\cos(θ_i) &amp; -\sin(θ_i) \\
\sin(θ_i) &amp; \cos(θ_i)
\end{bmatrix}
\begin{bmatrix} k_{2i} \\ k_{2i+1} \end{bmatrix}.
\]</div>
<p>Then attention uses <span class="arithmatex">\(q'\cdot k'\)</span> as usual.</p>
<p>Why this helps:</p>
<ul>
<li>Relative-position behavior “for free”: because rotations compose, the dot product between rotated vectors depends on the
  angle difference, which is a function of <span class="arithmatex">\((pos_q - pos_k)\)</span>. This makes it natural for the model to learn relative-offset
  patterns without explicit relative-position embeddings.</li>
<li>Extrapolation: RoPE often behaves better than learned absolute embeddings when evaluating at longer context lengths.</li>
</ul>
<p>Practical note: RoPE is typically applied only to a prefix of head dimensions (or with variants like partial rotary / scaling)
depending on the architecture.</p>
<h2 id="causal-masking-and-generative-vs-discriminative-discussion">Causal masking and “generative vs discriminative” discussion<a class="headerlink" href="#causal-masking-and-generative-vs-discriminative-discussion" title="Permanent link">&para;</a></h2>
<h3 id="what-is-the-causal-mask">What is the causal mask?<a class="headerlink" href="#what-is-the-causal-mask" title="Permanent link">&para;</a></h3>
<p>In a decoder-only (autoregressive) Transformer, token <span class="arithmatex">\(t\)</span> is only allowed to attend to positions <span class="arithmatex">\(\le t\)</span>. This is enforced by an additive mask <span class="arithmatex">\(M\)</span> inside attention:</p>
<div class="arithmatex">\[
\mathrm{Attn}(Q,K,V;M) = \mathrm{softmax}\!\left(\frac{QK^T}{\sqrt{d_k}} + M\right)V.
\]</div>
<p>A standard causal mask is</p>
<div class="arithmatex">\[
M_{t,s} =
\begin{cases}
0, &amp; s\le t\\
-\infty, &amp; s&gt;t.
\end{cases}
\]</div>
<p>So future positions get <span class="arithmatex">\(-\infty\)</span> logits and therefore zero probability after softmax.</p>
<p>Two practical notes:</p>
<ul>
<li>Training can still be parallel: even though the model is “left-to-right”, we compute all <span class="arithmatex">\(t\)</span> in one forward pass by masking.</li>
<li>In generation, the same constraint is what makes incremental decoding valid: the next-token distribution depends only on the prefix.</li>
</ul>
<h3 id="why-does-causal-masking-make-it-a-generative-model">Why does causal masking make it a generative model?<a class="headerlink" href="#why-does-causal-masking-make-it-a-generative-model" title="Permanent link">&para;</a></h3>
<p>With the causal mask, the model parameterizes a proper left-to-right factorization of a joint distribution over sequences:</p>
<div class="arithmatex">\[
p_\theta(y) = \prod_{t=1}^{T} p_\theta(y_t\mid y_{&lt;t}).
\]</div>
<p>Because the conditionals are defined for every position <span class="arithmatex">\(t\)</span>, we can <em>generate</em> a sequence by sampling or taking argmax repeatedly:
start from a BOS token, sample <span class="arithmatex">\(y_1\sim p(y_1)\)</span>, then <span class="arithmatex">\(y_2\sim p(y_2\mid y_1)\)</span>, etc. (In practice we use decoding algorithms such as greedy, top-<span class="arithmatex">\(k\)</span>, top-<span class="arithmatex">\(p\)</span>, beam search.)</p>
<p>This is what people usually mean by “generative” in the LLM context: the model defines a sequential process that produces the whole string.</p>
<h3 id="softmax-does-not-decide-generative-vs-discriminative">Softmax does not decide generative vs discriminative<a class="headerlink" href="#softmax-does-not-decide-generative-vs-discriminative" title="Permanent link">&para;</a></h3>
<p>Softmax is just a way to turn logits into a normalized categorical distribution.</p>
<ul>
<li>In an LM, softmax is over the vocabulary and represents <span class="arithmatex">\(p_\theta(y_t\mid y_{&lt;t})\)</span> (a conditional distribution used to build a joint by the product rule).</li>
<li>In a classifier, softmax is over labels and represents <span class="arithmatex">\(p_\theta(label\mid x)\)</span> (a conditional distribution directly optimized for prediction).</li>
</ul>
<p>So “using softmax” does not imply discriminative or generative; the <em>training objective and factorization</em> decide that.</p>
<h3 id="where-does-bert-fit">Where does BERT fit?<a class="headerlink" href="#where-does-bert-fit" title="Permanent link">&para;</a></h3>
<p>BERT uses bidirectional self-attention (no causal mask) and is pretrained with masked language modeling (MLM): predict masked tokens given the surrounding context on both sides.</p>
<p>That objective learns strong <em>representations</em> for understanding tasks and is typically used in a discriminative setting (classification, tagging, retrieval): you feed <span class="arithmatex">\(x\)</span> and predict labels <span class="arithmatex">\(y\)</span>, i.e. <span class="arithmatex">\(p(y\mid x)\)</span>.</p>
<p>MLM itself is a conditional / denoising objective and does not directly define a clean left-to-right joint <span class="arithmatex">\(p(x)=\prod_t p(x_t\mid x_{&lt;t})\)</span>, which is why BERT is not the default choice for straightforward next-token generation.
(There are ways to sample from an MLM with iterative masking / Gibbs-style updates, but that is a different generation mechanism and usually less convenient than AR decoding.)</p>
<h2 id="minimal-code-an-attention-block-with-residual-pathway">Minimal code: an attention block (with residual pathway)<a class="headerlink" href="#minimal-code-an-attention-block-with-residual-pathway" title="Permanent link">&para;</a></h2>
<p>Below is a minimal “Transformer block” around self-attention (Pre-LN style). It reuses the <code>SimpleMHA</code> module defined earlier in this note.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span class="k">class</span><span class="w"> </span><span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">),</span>
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>            <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">),</span>
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">),</span>
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a>        <span class="p">)</span>
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a>
<a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<a id="__codelineno-1-17" name="__codelineno-1-17" href="#__codelineno-1-17"></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-1-18" name="__codelineno-1-18" href="#__codelineno-1-18"></a>
<a id="__codelineno-1-19" name="__codelineno-1-19" href="#__codelineno-1-19"></a>
<a id="__codelineno-1-20" name="__codelineno-1-20" href="#__codelineno-1-20"></a><span class="k">class</span><span class="w"> </span><span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<a id="__codelineno-1-21" name="__codelineno-1-21" href="#__codelineno-1-21"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Pre-LN decoder block: x &lt;- x + Attn(LN(x)); x &lt;- x + MLP(LN(x)).&quot;&quot;&quot;</span>
<a id="__codelineno-1-22" name="__codelineno-1-22" href="#__codelineno-1-22"></a>
<a id="__codelineno-1-23" name="__codelineno-1-23" href="#__codelineno-1-23"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
<a id="__codelineno-1-24" name="__codelineno-1-24" href="#__codelineno-1-24"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<a id="__codelineno-1-25" name="__codelineno-1-25" href="#__codelineno-1-25"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">ln1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
<a id="__codelineno-1-26" name="__codelineno-1-26" href="#__codelineno-1-26"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">SimpleMHA</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
<a id="__codelineno-1-27" name="__codelineno-1-27" href="#__codelineno-1-27"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">ln2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
<a id="__codelineno-1-28" name="__codelineno-1-28" href="#__codelineno-1-28"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
<a id="__codelineno-1-29" name="__codelineno-1-29" href="#__codelineno-1-29"></a>
<a id="__codelineno-1-30" name="__codelineno-1-30" href="#__codelineno-1-30"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<a id="__codelineno-1-31" name="__codelineno-1-31" href="#__codelineno-1-31"></a>        <span class="c1"># Residual (skip) pathway:</span>
<a id="__codelineno-1-32" name="__codelineno-1-32" href="#__codelineno-1-32"></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ln1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">causal</span><span class="o">=</span><span class="n">causal</span><span class="p">)</span>
<a id="__codelineno-1-33" name="__codelineno-1-33" href="#__codelineno-1-33"></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ln2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<a id="__codelineno-1-34" name="__codelineno-1-34" href="#__codelineno-1-34"></a>        <span class="k">return</span> <span class="n">x</span>
<a id="__codelineno-1-35" name="__codelineno-1-35" href="#__codelineno-1-35"></a>
<a id="__codelineno-1-36" name="__codelineno-1-36" href="#__codelineno-1-36"></a>
<a id="__codelineno-1-37" name="__codelineno-1-37" href="#__codelineno-1-37"></a><span class="c1"># Example:</span>
<a id="__codelineno-1-38" name="__codelineno-1-38" href="#__codelineno-1-38"></a><span class="c1"># x = torch.randn(2, 128, 768)</span>
<a id="__codelineno-1-39" name="__codelineno-1-39" href="#__codelineno-1-39"></a><span class="c1"># block = TransformerBlock(d_model=768, n_heads=12, d_ff=3072, dropout=0.1)</span>
<a id="__codelineno-1-40" name="__codelineno-1-40" href="#__codelineno-1-40"></a><span class="c1"># y = block(x, causal=True)</span>
</code></pre></div>
<h3 id="passway-the-resnet-style-skip-connection-in-this-context">“Passway” = the ResNet-style skip connection (in this context)<a class="headerlink" href="#passway-the-resnet-style-skip-connection-in-this-context" title="Permanent link">&para;</a></h3>
<p>The key line is <code>x = x + F(x)</code> (here, <code>F</code> is attention or MLP applied to a normalized input). That <code>+ x</code> term is exactly the ResNet skip/residual pathway:</p>
<ul>
<li>In ResNet you often write <span class="arithmatex">\(x_{\ell+1} = x_{\ell} + F(x_{\ell})\)</span>.</li>
<li>In a Pre-LN Transformer block you have <span class="arithmatex">\(x_{\ell+1} = x_{\ell} + \mathrm{Attn}(\mathrm{LN}(x_{\ell}))\)</span> (and then another residual for the MLP).</li>
</ul>
<p>This “identity passway” makes optimization much easier: even if <span class="arithmatex">\(F\)</span> starts near zero, information and gradients can flow through the identity path.</p>
<h3 id="relation-to-odes-depth-as-time-intuition">Relation to ODEs (depth-as-time intuition)<a class="headerlink" href="#relation-to-odes-depth-as-time-intuition" title="Permanent link">&para;</a></h3>
<p>If you view layer index <span class="arithmatex">\(\ell\)</span> as a discrete “time” step, then a residual update</p>
<div class="arithmatex">\[
x_{\ell+1} = x_{\ell} + h\,f(x_{\ell},\ell)
\]</div>
<p>looks like a forward Euler discretization of an ODE <span class="arithmatex">\(\frac{dx}{dt} = f(x,t)\)</span>. In this analogy:</p>
<ul>
<li>the residual branch (attention/MLP) is the vector field <span class="arithmatex">\(f\)</span>,</li>
<li>the skip connection is the <span class="arithmatex">\(+x_{\ell}\)</span> term in Euler’s method,</li>
<li>and (sometimes) you can interpret hyperparameters / scaling as affecting the effective step size <span class="arithmatex">\(h\)</span>.</li>
</ul>
<p>In the Transformer context this is mainly an intuition for why residual networks train stably and how depth composes small updates; it is not required to treat the model as a true continuous-time system.</p>
<h3 id="guide-how-far-this-is-from-gpt-3-level-detail-and-whats-next">Guide: how far this is from GPT-3-level detail (and what’s next)<a class="headerlink" href="#guide-how-far-this-is-from-gpt-3-level-detail-and-whats-next" title="Permanent link">&para;</a></h3>
<p>This note covers the “core math blocks” (AR factorization, attention, masks, normalization, positional methods, residual pathway), but it is still far from a full GPT-3-style training + systems picture.
Below is a concrete checklist of what’s missing mainly in <em>knowledge details</em> which we will talk about in the furute.</p>
<p>What this article is still missing vs a GPT-3-level understanding:</p>
<ul>
<li>Exact architecture details used in modern LLMs: Pre-LN vs Post-LN variants in the wild, RMSNorm vs LayerNorm choices, MLP variants (GeLU vs SwiGLU), and common efficiency tweaks (MQA/GQA, grouped QK/V layouts).</li>
<li>Tokenization and text normalization: BPE/unigram tokenizers, byte-level vs unicode handling, special tokens, and how tokenization choices affect context length and compute.</li>
<li>Training objective details: next-token loss implementation details (label shifting, padding/masking), how perplexity is computed, and what is (and isn’t) “teacher forcing” in practice.</li>
<li>Optimization recipe: AdamW vs variants, weight decay placement, learning-rate schedules (warmup + cosine/linear), gradient clipping, dropout usage, and typical hyperparameter ranges.</li>
<li>Initialization and stability tricks: residual scaling, logit scaling, norm/activation scaling, and what breaks at depth/width.</li>
<li>Data pipeline: dataset composition, deduplication, filtering, mixing ratios, curriculum/order effects, and contamination issues.</li>
<li>Evaluation + monitoring: held-out loss curves, downstream benchmarks, calibration, memorization tests, and how to interpret regressions.</li>
<li>Systems/engineering: distributed training (data/tensor/pipeline parallelism), mixed precision, activation checkpointing, optimizer state sharding, throughput bottlenecks, and kernel-level attention optimizations.</li>
</ul>
<h2 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>Local note: <a href="vit-dit.html">vit-dit.md</a> (ViT and DiT)</p>
</li>
<li>
<p>Paper: https://arxiv.org/pdf/1409.3215 (Sutskever et al., 2014, <em>Sequence to Sequence Learning with Neural Networks</em>)</p>
</li>
<li>Course: https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html (Lena Voita, <em>NLP Course: Seq2Seq and Attention</em>)</li>
<li>Tech report: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf (Radford et al., 2019, <em>Language Models are Unsupervised Multitask Learners</em>; GPT-2)</li>
<li>Paper: https://arxiv.org/abs/2005.14165 (Brown et al., 2020, <em>Language Models are Few-Shot Learners</em>; GPT-3)</li>
<li>Video: https://www.youtube.com/watch?v=kCc8FmEb1nY (Andrej Karpathy, <em>Let's build GPT: from scratch, in code, spelled out.</em>)</li>
<li>Code: https://github.com/karpathy/minGPT (Andrej Karpathy, minGPT code)</li>
<li>Code: https://github.com/karpathy/nanoGPT (Andrej Karpathy, nanoGPT code)</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "..", "features": ["navigation.expand", "navigation.sections", "content.code.copy"], "search": "../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>