---
layout: default
title: Diffusion Models Intro
---

# Diffusion Models Intro

## Chapter 1 — From fluid mechanics to the manifold hypothesis

This chapter stays in the Euclidean setting: states are vectors \(x\in\mathbb{R}^D\) and time is \(t\in[0,1]\). We use “fluid” terminology only as the mathematics of transport: trajectories (ODEs), distribution evolution, and conservation laws.

### 1.1 Continuous Normalizing Flows (CNFs)

We begin from the **Lagrangian (particle) view**: model a velocity field and move samples by an ODE. The corresponding **Eulerian (density) view** will appear in Section 1.2.

Let
$$
v:[0,1]\times\mathbb{R}^D\to\mathbb{R}^D,\qquad (t,x)\mapsto v(t,x)
$$
be a time-dependent vector field (velocity). We will work in the Euclidean data space \(\mathbb{R}^D\), with data points \(x=(x_1,\dots,x_D)\in\mathbb{R}^D\).

Two fundamental objects in flow-based generative modeling are:

- a **probability density path** \(p:[0,1]\times\mathbb{R}^D\to\mathbb{R}_{\ge 0}\), written as \(p_t(x):=p(t,x)\), such that \(\int_{\mathbb{R}^D} p_t(x)\,dx=1\) for every \(t\in[0,1]\);
- a **time-dependent vector field** \(v_t(\cdot):=v(t,\cdot)\) that transports particles (and hence pushes densities forward).

Given \(v_t\), define its **flow map** \(\phi_t:\mathbb{R}^D\to\mathbb{R}^D\) as the solution of the ODE
$$
\frac{d}{dt}\phi_t(x)=v_t(\phi_t(x)),\tag{1}
$$
with initial condition
$$
\phi_0(x)=x.\tag{2}
$$
Under standard regularity assumptions (e.g., \(v_t\) Lipschitz in \(x\) with suitable growth), \(\phi_t\) is well-defined, and for each fixed \(t\) it is invertible (indeed a homeomorphism). If we further assume enough differentiability of \(v\) in \(x\) (e.g., \(C^1\) in \(x\)), then \(\phi_t\) is (locally) a diffeomorphism. This is the continuous-time analog of composing many small invertible maps (normalizing flows).

The defining modeling choice of a **Continuous Normalizing Flow (CNF)** (Chen et al., 2018) is to parameterize the vector field by a neural network \(v_t(x;\theta)\). This induces a learnable family of flow maps \(\phi_t(\cdot;\theta)\) via (1)–(2).

#### Push-forward: how a CNF transports densities

Let \(p_0\) be a simple base density (e.g., \(\mathcal{N}(0,I)\)) and define \(X_t:=\phi_t(X_0)\) with \(X_0\sim p_0\). The time-\(t\) density is the **push-forward** of \(p_0\) by \(\phi_t\):
$$
p_t = (\phi_t)_\# p_0.\tag{3}
$$
When \(\phi_t\) is a diffeomorphism, the push-forward has the familiar change-of-variables form:
$$
(\phi_t)_\# p_0(x)
=
p_0(\phi_t^{-1}(x))\;
\Big|\det\frac{\partial \phi_t^{-1}}{\partial x}(x)\Big|.\tag{4}
$$
Equivalently, \(p_0(x)=p_t(\phi_t(x))\big|\det(\partial_x\phi_t(x))\big|\).

We say that a vector field \(v_t\) **generates** a density path \((p_t)\) if its flow \(\phi_t\) satisfies (3). One practical way to check whether a candidate \((p_t,v_t)\) is consistent is the continuity equation, which we introduce next.

For the full classical fluid-mechanics PDE system (beyond this kinematic ODE viewpoint), see [Interlude \(\alpha\) — Year 1757](interlude-alpha-year-1757.md).

For a more detailed treatment of **first-order ODEs** and the viewpoint of an ODE with a **random initial condition** (random initial value problem) \(\mathbf{X}_t=\phi_t(\mathbf{X}_0)\), see [Mathematics Foundation](mathematics-foundation.md).

### 1.2 Density conservation (Eulerian view): the continuity equation

Switch from particles to distributions. Let \(\mu_t\) be the law of a random variable \(\mathbf{x}_t\in\mathbb{R}^D\). If \(\mu_t\) admits a density \(p_t\) with respect to Lebesgue measure, we write \(\mu_t(dx)=p_t(x)\,dx\).

The conservation-of-mass statement for transport by the vector field \(v_t\) is the **continuity equation**
$$
\partial_t p_t(x) + \nabla\!\cdot\big(p_t(x)\,v(t,x)\big)=0,
\qquad x\in\mathbb{R}^D,\; t\in[0,1].
$$
Interpretation: probability mass is neither created nor destroyed; it is only moved by the vector field \(v\).

Two equivalent “bookkeeping” forms (assuming enough smoothness) are useful:

1) **Along trajectories (material derivative).** If \(x(t)\) solves \(\dot x(t)=v(t,x(t))\), then
$$
\frac{d}{dt}p_t(x(t))
\;=\;
\partial_t p_t(x(t)) + \nabla p_t(x(t))\cdot v(t,x(t))
\;=\;
-\,p_t(x(t))\,\nabla\!\cdot v(t,x(t)).
$$
So regions with positive divergence expand and lower density; negative divergence compress and raise density.

2) **Jacobian form (volume-element conservation).** If the flow map \(\phi_{s\to t}\) is differentiable in \(a\) and \(J_{s\to t}(a):=\det(\nabla_a\phi_{s\to t}(a))\), then
$$
p_t(\phi_{s\to t}(a))\,J_{s\to t}(a)=p_s(a).
$$
This is the precise version of “mass in a moving infinitesimal volume element is conserved”.

Here \(\phi_{s\to t}\) denotes the flow map generated by \(v\) from time \(s\) to time \(t\).

Remark: in measure-theoretic terms, transport by the flow map means \(\mu_t=(\phi_{s\to t})_\#\mu_s\). The PDE above is the density-level expression of this push-forward relation.

### 1.3 The manifold hypothesis (a precise formulation)

Let \(\mu_\text{data}\) be the data distribution on \(\mathbb{R}^D\) (a probability measure; it may or may not have a density).

An idealized, mathematically clean version of the **manifold hypothesis** is:

- There exists a \(d\)-dimensional embedded \(C^1\) submanifold \(\mathcal{M}\subset\mathbb{R}^D\) with \(d\ll D\) such that
$$
\mathrm{supp}(\mu_\text{data}) \subseteq \mathcal{M}.
$$

A more realistic (noise-thickened) version is:

- There exists \(\sigma>0\) such that most mass lies in a tubular neighborhood of \(\mathcal{M}\):
$$
\mu_\text{data}\big(\{x\in\mathbb{R}^D:\mathrm{dist}(x,\mathcal{M})\le \sigma\}\big)\ge 1-\varepsilon,
$$
for a small \(\varepsilon\in[0,1)\).

Important technical point: if \(\mathrm{supp}(\mu_\text{data})\subseteq\mathcal{M}\) with \(d<D\), then \(\mu_\text{data}\) is typically **singular** with respect to Lebesgue measure on \(\mathbb{R}^D\); in particular, an ambient-space density \(p_\text{data}(x)\) may not exist.

### 1.4 How flow models relate to (but do not equal) manifold learning

A (continuous-time) flow model in \(\mathbb{R}^D\) typically chooses a base distribution \(\mu_1\) (often \(\mathcal{N}(0,I)\)) and a diffeomorphic map \(\psi_{1\to 0}\) (generated by an ODE), then defines the model distribution by push-forward:
$$
\mu_\theta := (\psi_{1\to 0})_\# \mu_1.
$$

This connects to the manifold hypothesis in a specific way:

- A diffeomorphism \(\psi_{1\to 0}:\mathbb{R}^D\to\mathbb{R}^D\) maps absolutely continuous measures to absolutely continuous measures. Therefore, starting from \(\mu_1\) with a smooth, full-dimensional density (e.g., Gaussian), an invertible flow model cannot represent an exactly low-dimensional, manifold-supported \(\mu_\text{data}\) in the idealized sense above.
- What a flow can do is **concentrate** mass near low-dimensional structure (a thin neighborhood of \(\mathcal{M}\)) while remaining a full-dimensional density in \(\mathbb{R}^D\). This is a transport statement, not an explicit reconstruction of \(\mathcal{M}\) (no charts/atlas are learned).

This is one reason diffusion-style constructions add noise along a path: intermediate distributions become “thickened” and typically admit well-behaved ambient densities, making training and reverse-time generation well-posed.

### 1.4.1 How the manifold viewpoint helps, and what diffusion is actually doing

In image generation, a “manifold” is typically a shorthand for the empirical observation that natural images occupy a very thin subset of \(\mathbb{R}^D\): most variability can be explained by far fewer degrees of freedom than \(D=H\times W\times C\). In an idealized form, one writes \(\mathrm{supp}(\mu_\text{data})\subseteq\mathcal{M}\) for a low-dimensional \(\mathcal{M}\subset\mathbb{R}^D\); in a realistic form, \(\mu_\text{data}\) concentrates in a small neighborhood of \(\mathcal{M}\).

The manifold viewpoint clarifies why diffusion adds noise. If data are extremely “thin” (close to a low-dimensional set), then objects like an ambient-space density \(p_\text{data}(x)\) or an ambient score \(\nabla_x\log p_\text{data}(x)\) may be ill-behaved or not even well-defined. Diffusion constructs a Gaussian-perturbation path (schematically \(x_t=\alpha(t)x_0+\sigma(t)\epsilon\)), which “thickens” the distribution so that for \(t>0\) the marginal \(p_t\) is typically a regular, full-dimensional density on \(\mathbb{R}^D\). The learned score/velocity field then tells you how to move samples from noise back toward the high-density region near the data, but it does not explicitly recover a geometric object \(\mathcal{M}\) (no charts, projections, or tangent structure are output).

### 1.4.2 What kinds of image generative models are manifold learning?

Models are closer to “manifold learning” when they explicitly posit (and learn) a low-dimensional parameterization of images, i.e., a map \(g:\mathbb{R}^d\to\mathbb{R}^D\) with \(d\ll D\) such that typical images satisfy \(x\approx g(z)\).

- **Autoencoders (AE/DAE) and VAEs:** learn an encoder/decoder pair and a low-dimensional latent \(z\). This makes the “data live near a low-dimensional set” idea explicit (though VAEs add stochasticity and optimize a likelihood bound rather than directly estimating \(\mathcal{M}\)).
- **GANs / implicit generative models:** a generator \(x=g_\theta(z)\) with \(z\in\mathbb{R}^d\) defines (in the idealized deterministic case) a distribution supported on a \(d\)-dimensional set \(\mathrm{Im}(g_\theta)\). This is geometrically close to learning a manifold-like image set, even if the learned set is only approximate and the notion of “manifold” may fail globally.
- **Latent generative models (e.g., latent diffusion):** learn a low-dimensional representation via an autoencoder, then run diffusion (or another density model) in latent space. This is often a practical middle ground: “manifold-like” structure is delegated to the autoencoder, while diffusion handles distribution modeling in the latent.

In particular, a **VAE** makes the low-dimensional structure explicit via a latent variable \(z\in\mathbb{R}^d\) (with \(d\ll D\)) and a decoder distribution \(p_\theta(x\mid z)\) on \(x\in\mathbb{R}^D\). With a prior \(p(z)\) (often \(\mathcal{N}(0,I_d)\)) and an encoder \(q_\phi(z\mid x)\), it is trained by maximizing the ELBO:
$$
\log p_\theta(x)
\;\ge\;
\mathbb{E}_{z\sim q_\phi(z\mid x)}\big[\log p_\theta(x\mid z)\big]
-\mathrm{KL}\big(q_\phi(z\mid x)\,\|\,p(z)\big).
$$
Geometrically, the mean decoder map \(g_\theta(z):=\mathbb{E}[x\mid z]\) can be viewed as a learned low-dimensional “surface”, while the decoder noise determines how thick the model distribution is around that surface; this is why VAEs feel “manifold-flavored” but are not necessarily learning a strict manifold-supported distribution in \(\mathbb{R}^D\).

These approaches make the low-dimensional structure explicit via \(z\mapsto x\). By contrast, standard diffusion/score/flow-matching methods (as used in image generation) are usually framed as learning dynamics in the ambient Euclidean space \(\mathbb{R}^D\) (or in a learned latent space), rather than directly estimating the manifold \(\mathcal{M}\) itself.

### 1.5 Bridge to the rest of the intro

We will reuse the same transport backbone:

- Choose a probability path \((\mu_t)_{t\in[0,1]}\) (or densities \((p_t)\) when they exist).
- Learn dynamics (ODE or SDE) that generate that path.
- Sample by numerically integrating the learned dynamics backward from a simple endpoint distribution.

## Chapter 2 — Flow Matching

Before introducing flow matching, it is helpful to recall what a CNF gives us “in closed form” (as an operator), and what remains expensive in practice.

Consider a CNF defined by the ODE
$$
\frac{d}{dt}X_t = v(t,X_t;\theta),\qquad t\in[0,1],\qquad X_0\sim p_0.
$$
Even when the solution is not available analytically, it always admits the integral form
$$
X_t = X_0 + \int_0^t v(s,X_s;\theta)\,ds,
$$
which is what numerical ODE solvers approximate.

If the flow map \(\phi_t\) is a diffeomorphism and the density \(p_t\) exists, then along trajectories \(t\mapsto X_t\) the log-density obeys the instantaneous change-of-variables formula
$$
\frac{d}{dt}\log p_t(X_t) = -\,\nabla\!\cdot v(t,X_t;\theta),
$$
hence
$$
\log p_1(X_1)=\log p_0(X_0)-\int_0^1 \nabla\!\cdot v(t,X_t;\theta)\,dt.
$$
This provides a principled likelihood bookkeeping rule for CNFs, but it is often computationally heavy in high dimension because it couples an ODE solve with repeated divergence (trace) evaluations.

Flow matching takes the complementary **Eulerian density-evolution** viewpoint from Section 1.2: rather than training the model primarily through likelihood computation, we specify (or construct) a target probability path \((p_t)\) and train a vector field \(v(t,x;\theta)\) so that its induced density evolution matches that path (via the continuity equation).

### 2.1 CONSTRUCTING \(p_t,u_t\) FROM CONDITIONAL PROBABILITY PATHS AND VECTOR FIELDS

Let \(X_1\sim q(x_1)\) denote the (unknown) data distribution; we only have sample access to \(q\). We would like a probability path \((p_t)_{t\in[0,1]}\) such that \(p_0=p\) is simple (e.g., \(\mathcal{N}(0,I)\)) and \(p_1\approx q\), together with a vector field \(u_t\) that generates this path (so that \((p_t,u_t)\) satisfy the continuity equation).

The main difficulty is not that CNFs are “incalculable”, but that the *marginal* objects \(p_t\) and \(u_t\) are generally not available in closed form, so naïvely we cannot (i) sample \(x\sim p_t(x)\) for arbitrary \(t\), nor (ii) evaluate \(u_t(x)\) to supervise a neural vector field.

A key idea in flow matching is to construct the marginal path and vector field by mixing **conditional** paths that are defined per data sample.

**Theorem 1 (marginalizing conditional paths and vector fields).** Fix a family of conditional probability paths \(\{p_t(\cdot\mid x_1)\}_{x_1}\) such that

- \(p_0(x\mid x_1)=p(x)\) for all \(x_1\), and
- \(p_1(x\mid x_1)\) is concentrated around \(x_1\) (e.g., \(p_1(x\mid x_1)=\mathcal{N}(x\mid x_1,\sigma^2 I)\) for a small \(\sigma>0\)).

Define the marginal (mixture) path
$$
p_t(x)=\int_{\mathbb{R}^D} p_t(x\mid x_1)\,q(x_1)\,dx_1.\tag{6}
$$
In particular,
$$
p_1(x)=\int_{\mathbb{R}^D} p_1(x\mid x_1)\,q(x_1)\,dx_1 \approx q(x).\tag{7}
$$
Assume \(p_t(x)>0\) for all \(t\in[0,1]\) and \(x\in\mathbb{R}^D\). Let \(u_t(x\mid x_1)\) be conditional vector fields that generate the conditional paths \(p_t(\cdot\mid x_1)\). Define the marginal vector field
$$
u_t(x)=\int_{\mathbb{R}^D} u_t(x\mid x_1)\,\frac{p_t(x\mid x_1)\,q(x_1)}{p_t(x)}\,dx_1.\tag{8}
$$
Then the pair \((p_t,u_t)\) satisfies the continuity equation. (As the paper emphasizes: “the marginal vector field ... generates the marginal probability path”.)

**Explanation (what this buys us).** Theorem 1 gives a mathematically clean bridge between:

- a per-sample design \(\big(p_t(\cdot\mid x_1),u_t(\cdot\mid x_1)\big)\), where \(x_1\sim q\) is available from the dataset, and
- a global (marginal) density path \(p_t\) and its generator \(u_t\), which are guaranteed to be consistent through the continuity equation.

This resolves the practical bottleneck in the *marginal* formulation: we do not need closed-form access to \(p_t\) or \(u_t\). Instead, we can sample \(x_1\sim q\), sample \(x\sim p_t(\cdot\mid x_1)\), and compute \(u_t(x\mid x_1)\) to build a tractable training objective (developed next via conditional flow matching).

In words: since for each fixed \(x_1\) the conditional pair \(\big(p_t(\cdot\mid x_1),u_t(\cdot\mid x_1)\big)\) satisfies the conditional continuity equation, Theorem 1 shows that the induced marginal pair \((p_t,u_t)\) (via the mixture constructions (6)–(8)) satisfies the marginal continuity equation as well.

The proof of Theorem 1 can be found in the appendix of *Flow Matching for Generative Modeling* (see References). For an additional exposition, see [introduction-B-flow-matching](introduction-B-flow-matching.md).

### 2.2 CONDITIONAL FLOW MATCHING

Operationally, CFM makes training possible by working with per-sample training pairs: sample \(x_1\sim q\), sample \(x\sim p_t(\cdot\mid x_1)\), compute the conditional target \(u_t(x\mid x_1)\), and regress \(v(t,x;\theta)\) toward it.

The mixture constructions (6)–(8) show that a marginal path \(p_t\) and a marginal vector field \(u_t\) exist and satisfy the continuity equation. However, because \(q\) is only accessible through samples, the integrals in (6)–(8) are typically intractable. In particular, we cannot naïvely compute \(u_t(x)\), so we cannot directly build an unbiased estimator of the marginal flow-matching regression loss
$$
\mathcal{L}_{\mathrm{FM}}(\theta)
:=
\mathbb{E}_{t\sim U[0,1],\,x\sim p_t}\Big[\|v(t,x;\theta)-u_t(x)\|^2\Big].\tag{5}
$$

Instead, conditional flow matching replaces the marginal target \(u_t(x)\) by the per-sample conditional target \(u_t(x\mid x_1)\), yielding the CFM objective
$$
\mathcal{L}_{\mathrm{CFM}}(\theta)
:=
\mathbb{E}_{t\sim U[0,1],\,x_1\sim q,\,x\sim p_t(\cdot\mid x_1)}
\Big[\|v(t,x;\theta)-u_t(x\mid x_1)\|^2\Big].\tag{9}
$$
This objective is tractable as long as we can (i) sample \(x\sim p_t(\cdot\mid x_1)\) and (ii) evaluate \(u_t(x\mid x_1)\), both of which can be arranged by design since they are defined on a per-sample basis.

**Theorem 2 (FM and CFM are equivalent up to a constant).** Assume \(p_t(x)>0\) for all \(x\in\mathbb{R}^D\) and \(t\in[0,1]\), and that the constructions in (6)–(8) are well-defined. Then
$$
\mathcal{L}_{\mathrm{CFM}}(\theta)=\mathcal{L}_{\mathrm{FM}}(\theta)+C,
$$
where \(C\) does not depend on \(\theta\). In particular,
$$
\nabla_\theta \mathcal{L}_{\mathrm{FM}}(\theta)=\nabla_\theta \mathcal{L}_{\mathrm{CFM}}(\theta).
$$

**Explanation (why the gradients match).** Intuitively, (8) says that the marginal target is a posterior average:
\(u_t(x)=\mathbb{E}[u_t(x\mid X_1)\mid X_t=x]\).
Expanding the square and applying the law of total expectation shows that the only difference between (5) and (9) is a conditional-variance term
\(\mathbb{E}\big[\|u_t(X_t\mid X_1)\|^2-\|u_t(X_t)\|^2\big]\),
which is independent of \(\theta\). Therefore, optimizing the tractable conditional objective (9) is equivalent (in expectation) to optimizing the intractable marginal objective (5).

The proof of Theorem 2 can be found in *Flow Matching for Generative Modeling* and in *Flow Matching Guide and Code* (see References). For an additional exposition, see [introduction-B-flow-matching](introduction-B-flow-matching.md).

### 2.3 CONDITIONAL PROBABILITY PATHS AND VECTOR FIELDS

The CFM objective is general: it works with any choice of conditional probability paths \(p_t(\cdot\mid x_1)\) and conditional vector fields \(u_t(\cdot\mid x_1)\) (as long as they are sampleable/evaluable). A particularly convenient family is given by Gaussian conditional paths, because both sampling and closed-form conditional targets can be obtained.

#### Gaussian conditional paths

Fix \(x_1\in\mathbb{R}^D\). Consider a conditional probability path of the form
$$
p_t(x\mid x_1)=\mathcal{N}\!\big(x\mid \mu_t(x_1),\sigma_t(x_1)^2 I\big),\qquad t\in[0,1],\tag{10}
$$
where \(\mu:[0,1]\times\mathbb{R}^D\to\mathbb{R}^D\) is a time-dependent mean and \(\sigma:[0,1]\times\mathbb{R}^D\to\mathbb{R}_{>0}\) is a time-dependent scalar standard deviation. We typically set
$$
\mu_0(x_1)=0,\qquad \sigma_0(x_1)=1,
$$
so that \(p_0(x\mid x_1)=\mathcal{N}(0,I)=:p(x)\) for all \(x_1\), and
$$
\mu_1(x_1)=x_1,\qquad \sigma_1(x_1)=\sigma_{\min},
$$
so that \(p_1(\cdot\mid x_1)\) is concentrated near \(x_1\) when \(\sigma_{\min}\) is small.

#### A canonical conditional flow (affine map)

Many vector fields can generate the same density path (e.g., one can add a component \(w_t\) such that \(\nabla\!\cdot(p_t w_t)=0\)), but for Gaussian paths it is natural to choose the simplest canonical transport: an affine map that turns standard Gaussian noise into the desired Gaussian.

Define the conditional (affine) flow map
$$
\psi_t(x_0;x_1):=\sigma_t(x_1)\,x_0+\mu_t(x_1).\tag{11}
$$
If \(x_0\sim \mathcal{N}(0,I)\), then \(\psi_t(x_0;x_1)\sim \mathcal{N}(\mu_t(x_1),\sigma_t(x_1)^2 I)\). In push-forward notation,
$$
(\psi_t(\cdot;x_1))_\#\,p(\cdot)=p_t(\cdot\mid x_1).\tag{12}
$$

This flow induces a conditional vector field \(u_t(\cdot\mid x_1)\) defined implicitly by
$$
\frac{d}{dt}\psi_t(x_0;x_1)=u_t(\psi_t(x_0;x_1)\mid x_1).\tag{13}
$$

Reparameterizing \(x\sim p_t(\cdot\mid x_1)\) via \(x=\psi_t(x_0;x_1)\) with \(x_0\sim p(x_0)=\mathcal{N}(0,I)\), the CFM objective (9) can be written as
$$
\mathcal{L}_{\mathrm{CFM}}(\theta)
=
\mathbb{E}_{t\sim U[0,1],\,x_1\sim q,\,x_0\sim p}
\Big[\big\|v(t,\psi_t(x_0;x_1);\theta)-\tfrac{d}{dt}\psi_t(x_0;x_1)\big\|^2\Big].\tag{14}
$$

**Theorem 3 (closed-form conditional vector field for Gaussian paths).** Let \(p_t(x\mid x_1)\) be the Gaussian conditional path in (10) and \(\psi_t(\cdot;x_1)\) be the affine flow map in (11). Then the (unique) vector field that realizes (13) is
$$
u_t(x\mid x_1)
=
\frac{\partial_t \sigma_t(x_1)}{\sigma_t(x_1)}\big(x-\mu_t(x_1)\big)
\;+\;\partial_t\mu_t(x_1).\tag{15}
$$
Consequently, \(u_t(\cdot\mid x_1)\) generates the Gaussian path \(p_t(\cdot\mid x_1)\) (equivalently, \((p_t(\cdot\mid x_1),u_t(\cdot\mid x_1))\) satisfy the conditional continuity equation).

**Derivation (one line).** Differentiate (11) in time:
\(\frac{d}{dt}\psi_t(x_0;x_1)=\partial_t\sigma_t(x_1)\,x_0+\partial_t\mu_t(x_1)\).
Since \(x=\psi_t(x_0;x_1)=\sigma_t(x_1)x_0+\mu_t(x_1)\), we have \(x_0=(x-\mu_t(x_1))/\sigma_t(x_1)\), which substituted back yields (15).

The proofs and additional discussion can be found in *Flow Matching for Generative Modeling* and *Flow Matching Guide and Code* (see References).

### 2.4 Examples: special instances of Gaussian conditional probability paths

The Gaussian construction above is fully general: we may choose any differentiable \(\mu_t(x_1)\) and \(\sigma_t(x_1)\) satisfying desired boundary conditions and obtain a closed-form conditional target \(u_t(x\mid x_1)\) via Theorem 3. Below are two illustrative design choices.

#### Example I: diffusion-inspired conditional paths (VE / VP)

Many diffusion models induce Gaussian conditionals \(p_t(x\mid x_1)\) at arbitrary times \(t\), with specific \(\mu_t(x_1)\) and \(\sigma_t(x_1)\). In the flow-matching setting, we can simply take these conditionals as a *probability-path design choice* and obtain a deterministic conditional vector field by plugging them into (15).

**(a) Reversed VE (noise \(\to\) data).** A variance-exploding (VE) conditional path can be written as
$$
p_t(x\mid x_1)=\mathcal{N}\!\big(x\mid x_1,\sigma_{1-t}^2 I\big),\tag{16}
$$
where \(\sigma_s\) is increasing with \(\sigma_0=0\) and \(\sigma_1\) large. Here \(t=1\) corresponds to a distribution concentrated at \(x_1\), while \(t=0\) is a very wide Gaussian (an approximation to “noise” in practice). In this case \(\mu_t(x_1)=x_1\) and \(\sigma_t(x_1)=\sigma_{1-t}\), so (15) gives
$$
u_t(x\mid x_1)= -\frac{\sigma'_{1-t}}{\sigma_{1-t}}\,(x-x_1),\tag{17}
$$
where \(\sigma'_s=\frac{d}{ds}\sigma_s\).

**(b) Reversed VP (noise \(\to\) data).** A variance-preserving (VP) conditional path can be written as
$$
p_t(x\mid x_1)=\mathcal{N}\!\big(x\mid \alpha_{1-t}x_1,\,(1-\alpha_{1-t}^2)I\big),\tag{18}
$$
where
$$
\alpha_t := \exp\!\Big(-\frac12 T(t)\Big),\qquad T(t):=\int_0^t \beta(s)\,ds,
$$
and \(\beta\) is a nonnegative noise-rate schedule. When \(T(1)\) is large, \(p_0(\cdot\mid x_1)\approx \mathcal{N}(0,I)\) and \(p_1(\cdot\mid x_1)\) concentrates at \(x_1\).
Here \(\mu_t(x_1)=\alpha_{1-t}x_1\) and \(\sigma_t(x_1)=\sqrt{1-\alpha_{1-t}^2}\), and (15) yields
$$
u_t(x\mid x_1)=\frac{\alpha'_{1-t}}{1-\alpha_{1-t}^2}\big(\alpha_{1-t}x-x_1\big),\tag{19}
$$
where \(\alpha'_s=\frac{d}{ds}\alpha_s\). (Since \(\alpha\) is decreasing when \(\beta>0\), \(\alpha'_s<0\), so the drift indeed points toward \(x_1\) as \(t\to 1\).)

Remark: these diffusion-inspired paths are typically derived from stochastic diffusion processes and may only approach an idealized “pure noise” distribution asymptotically; in practice one works with a finite terminal time and an approximate Gaussian endpoint.

#### Example II: optimal-transport-inspired conditional paths (linear mean/std)

An arguably simpler design is to let the mean and standard deviation evolve linearly in time:
$$
\mu_t(x_1)=t\,x_1,\qquad \sigma_t(x_1)=1-(1-\sigma_{\min})t.\tag{20}
$$
Plugging (20) into (15) gives the conditional vector field
$$
u_t(x\mid x_1)=\frac{x_1-(1-\sigma_{\min})x}{1-(1-\sigma_{\min})t}.\tag{21}
$$
The corresponding affine flow is
$$
\psi_t(x_0;x_1)=(1-(1-\sigma_{\min})t)x_0+t x_1.\tag{22}
$$
In this case the CFM loss (14) becomes
$$
\mathcal{L}_{\mathrm{CFM}}(\theta)
=
\mathbb{E}_{t\sim U[0,1],\,x_1\sim q,\,x_0\sim p}
\Big[\big\|v(t,\psi_t(x_0;x_1);\theta)-\big(x_1-(1-\sigma_{\min})x_0\big)\big\|^2\Big].\tag{23}
$$
Compared with diffusion-inspired choices, the OT-style conditional vector field often has a simpler structure (e.g., its direction can be constant in time up to a scalar factor), which can make the regression task easier.

For practical experimental-design details (e.g., path/schedule choices, loss weighting, and numerical solvers), see [introduction-B-flow-matching](introduction-B-flow-matching.md).

### 2.5 Practical notes (implementation “gotchas”)

This chapter intentionally focuses on the modeling algebra. In practice, a few design choices matter disproportionately:

- **Endpoints and numerical stability.** Many choices make \(u_t(x\mid x_1)\) contain factors like \(1/\sigma_t(x_1)\) or \(1/(1-\alpha_t^2)\); avoid singular endpoints by using \(\sigma_{\min}>0\), clipping \(t\) away from \(\{0,1\}\), or using a schedule that keeps denominators bounded.
- **How to sample time \(t\).** Uniform \(t\sim U[0,1]\) is the default in the theory, but non-uniform sampling or explicit weights \(w(t)\) in the regression loss can improve conditioning (e.g., not over-emphasizing near-singular regions).
- **Parameterization of the learned field.** Even when the target is a conditional vector field, different parameterizations of \(v(t,x;\theta)\) (predicting a velocity vs. predicting an equivalent noise-like quantity) can change optimization behavior while representing the same underlying solution.
- **Choice of conditional path family.** Diffusion-inspired vs. OT-inspired conditionals can change the geometry of trajectories (e.g., “straight” vs. “noisy/curved” conditionals), which changes the difficulty of the regression problem and the behavior of sampling.
- **Sampling solver and error.** Training uses exact conditional targets, but generation still integrates an ODE induced by the learned \(v_\theta\); solver choice and step count (and whether to use adaptive stepping) can materially affect sample quality and speed.

For a more hands-on discussion of these choices (including code-level details), see [introduction-B-flow-matching](introduction-B-flow-matching.md).

## Chapter 3 — Diffusion models: a noising path and a learnable reverse process

In ODE-based flow models, the dynamics are deterministic: under standard regularity, the flow map is (locally) invertible and the Eulerian density evolution obeys the continuity equation. In diffusion models, the forward dynamics are stochastic (a Markov kernel rather than a bijection), so density evolution must account for diffusion and is governed by the Fokker–Planck equation.

The key design choice in diffusion models is to **pick a probability path** that connects data \(\mu_0=\mu_\text{data}\) to a simple distribution \(\mu_1\) (usually close to \(\mathcal{N}(0,I)\)), by **injecting Gaussian noise according to a schedule**. This makes intermediate marginals “thick” (typically admitting nice ambient-space densities), and it gives a principled way to define a reverse-time generative dynamics.

We present both the discrete-time (DDPM-style) and continuous-time (SDE-style) viewpoints. Detailed derivations about SDEs and the Fokker–Planck equation are collected separately in [Mathematics Foundation](mathematics-foundation.md).

### 3.1 Eulerian view: density evolution by Fokker–Planck

To mirror the Eulerian viewpoint of Section 1.2, we start from **density evolution** rather than from a discrete Markov chain.

Let \(t\in[0,1]\). Consider a forward Itô SDE
$$
dX_t = f(X_t,t)\,dt + g(t)\,dW_t,\qquad X_0\sim \mu_\text{data},
$$
where \(W_t\) is Brownian motion. If the time-\(t\) law \(\mu_t=\mathcal{L}(X_t)\) admits a density \(\mu_t(dx)=p_t(x)\,dx\), then \(p_t\) satisfies the Fokker–Planck equation
$$
\partial_t p_t(x)
=
-\nabla\!\cdot\big(f(x,t)p_t(x)\big)+\frac12 g(t)^2\,\Delta p_t(x).
$$
This is the stochastic analog of the continuity equation: the drift \(f\) transports mass (a CE-like term), while the Brownian noise adds a second-order diffusion term.

The “schedule” viewpoint appears here as the choice of coefficients \((f,g)\), which determines the entire marginal path \((p_t)\). In diffusion modeling we choose \((f,g)\) so that \(p_1\) is simple (approximately \(\mathcal{N}(0,I)\)) and \(p_0\) matches the data distribution.

### 3.2 Reverse-time generation: score enters the dynamics

The key generative fact is that the reverse-time dynamics involve the **score** \(\nabla_x\log p_t(x)\). Informally, if we run time backward from \(t=1\) to \(t=0\), the reverse-time SDE has the form (here written for the common case where \(g(t)\) is a scalar diffusion coefficient, i.e., isotropic noise depending only on time)
$$
dX_t
=
\Big(f(X_t,t)-g(t)^2\,\nabla_x\log p_t(X_t)\Big)\,dt
+ g(t)\,d\bar W_t,
\qquad t:1\to 0,
$$
where \(\bar W_t\) is a Brownian motion in reverse time.

For background on SDEs, Itô interpretation, and the Fokker–Planck equation (including derivations), see [Mathematics Foundation](mathematics-foundation.md).

There is also an associated **probability flow ODE** (deterministic dynamics with the same marginals \((p_t)\)):
$$
\frac{d}{dt}X_t
=
f(X_t,t)-\frac12 g(t)^2\,\nabla_x\log p_t(X_t).
$$
Once we can approximate the score, we can sample either by integrating the reverse SDE (stochastic sampler) or the probability flow ODE (deterministic sampler).

### 3.3 Learning the score (why training is tractable)

Even though the marginal density \(p_t(x)\) is unknown (and is generally difficult to model in closed form, regardless of whether it is introduced marginally or via conditional constructions), the forward diffusion/noising mechanism is known and sampleable by design. Concretely, we can draw \(x_0\sim q\) from the dataset, sample noise \(\varepsilon\), and generate \(x_t\sim p_t\) by running the forward corruption process.

This avoids the main intractability encountered in marginal flow matching: we do not need to evaluate a marginal vector field \(u_t(x)\) that involves posterior normalization by \(p_t(x)\). Instead, we obtain unbiased training pairs \((t,x_t)\) directly, and then fit a neural network \(s_\theta(x,t)\approx \nabla_x\log p_t(x)\) via denoising score matching objectives.

### 3.4 Discrete-time DDPM as a practical discretization

After fixing the continuous-time (Eulerian) picture, we can introduce DDPM as a convenient discrete-time construction of the same “design a noising path, learn a reverse process” idea.

Fix an integer \(T\). The forward process is a Markov chain
$$
q(x_{0:T}) = q(x_0)\prod_{k=1}^T q(x_k\mid x_{k-1}),
$$
where \(q(x_0)=\mu_\text{data}\) and
$$
q(x_k\mid x_{k-1})=\mathcal{N}\!\big(\sqrt{1-\beta_k}\,x_{k-1},\;\beta_k I\big),
\qquad k=1,\dots,T,
$$
with noise schedule \((\beta_k)\). Define \(\alpha_k:=1-\beta_k\) and \(\bar\alpha_k:=\prod_{s=1}^k \alpha_s\). Then
$$
q(x_k\mid x_0)=\mathcal{N}\!\big(\sqrt{\bar\alpha_k}\,x_0,\;(1-\bar\alpha_k)I\big),
$$
equivalently
$$
x_k=\sqrt{\bar\alpha_k}\,x_0+\sqrt{1-\bar\alpha_k}\,\varepsilon,\qquad \varepsilon\sim\mathcal{N}(0,I).
$$

Generation runs the chain backward, sampling \(p_\theta(x_{k-1}\mid x_k)\). A common parameterization predicts \(\varepsilon_\theta(x_k,k)\), which can be converted to an estimate \(\hat x_0(x_k,k)\) and hence to a reverse mean. The ELBO view and its reduction to a denoising regression loss are given in Chapter 4.

## Chapter 4 — A flow-first (physical) view of diffusion, with DDPM-ELBO as a special case

The “physical” intuition is: diffusion models are **stochastic flows of particles**. We track particles by an SDE (Lagrangian view), and we track their law by a PDE (Eulerian view). The PDE is not an extra constraint we impose; it is the density-level consequence of the particle dynamics.

### 4.1 Stochastic flows (Lagrangian) \(\Rightarrow\) Fokker–Planck (Eulerian)

Consider the forward SDE on \(\mathbb{R}^D\)
$$
dX_t=f(X_t,t)\,dt+g(t)\,dW_t,\qquad t\in[0,1],\qquad X_0\sim \mu_\text{data}.
$$
If the marginal law \(\mu_t=\mathcal{L}(X_t)\) admits a density \(p_t\) and regularity is sufficient, then \(p_t\) evolves by the Fokker–Planck equation
$$
\partial_t p_t(x) = -\nabla\!\cdot\big(f(x,t)p_t(x)\big) + \frac12 g(t)^2\,\Delta p_t(x).
$$
In other words, the SDE defines a **stochastic flow of measures** \((\mu_t)\), and Fokker–Planck is its Eulerian “mass conservation + diffusion” statement. (A step-by-step derivation is in [Mathematics Foundation](mathematics-foundation.md), Appendix B.)

Reverse-time generation is another stochastic flow. To move probability mass from a simple \(\mu_1\) back to \(\mu_0\), we run a reverse-time dynamics whose drift depends on the score \(\nabla_x\log p_t\). This is the origin of “learn the score/velocity field, then integrate backward”.

### 4.2 The VP diffusion as a canonical forward SDE (a continuous scheduler)

A widely used forward process is the **variance-preserving (VP) SDE**, parameterized by a scalar noise-rate schedule \(\beta(t)\ge 0\):
$$
dX_t = -\frac12\beta(t)\,X_t\,dt + \sqrt{\beta(t)}\,dW_t.
$$
It is a linear SDE, and (conditionally on \(X_0=x_0\)) its marginal is Gaussian:
$$
X_t = \alpha(t)\,x_0 + \sigma(t)\,\varepsilon,\qquad \varepsilon\sim\mathcal{N}(0,I),
$$
for explicit \(\alpha(t)\in(0,1]\) and \(\sigma(t)\ge 0\) determined by \(\beta(t)\). One common convention is
$$
\alpha(t)=\exp\!\Big(-\frac12\int_0^t \beta(s)\,ds\Big),
\qquad
\sigma(t)^2 = 1-\alpha(t)^2,
$$
so that \(X_t\mid X_0=x_0\sim \mathcal{N}(\alpha(t)x_0,\sigma(t)^2 I)\). This is the continuous-time analog of the discrete identity
\(x_t=\sqrt{\bar\alpha_t}x_0+\sqrt{1-\bar\alpha_t}\varepsilon\) from Chapter 3.

### 4.3 DDPM as a time discretization of a stochastic flow

DDPM’s forward Markov chain
$$
q(x_t\mid x_{t-1})=\mathcal{N}\!\big(\sqrt{1-\beta_t}\,x_{t-1},\;\beta_t I\big)
$$
can be viewed as a time discretization of a VP-type SDE (with a suitable mapping between \(\{\beta_t\}\) and \(\beta(t)\)). From the flow viewpoint:

- **the schedule** \(\{\beta_t\}\) (or \(\beta(t)\)) specifies how the stochastic flow gradually destroys information,
- **the model** learns a reverse-time flow that restores structure from noise.

### 4.4 ELBO for DDPM: likelihood as stepwise flow-matching across time

Let the learned reverse process be
$$
p_\theta(x_{0:T}) = p(x_T)\prod_{t=1}^T p_\theta(x_{t-1}\mid x_t),
\qquad p(x_T)=\mathcal{N}(0,I).
$$
Using the forward chain \(q(x_{0:T})=q(x_0)\prod_{t=1}^T q(x_t\mid x_{t-1})\) as a variational distribution over latents \(x_{1:T}\), we get a variational lower bound:
$$
\log p_\theta(x_0)
\ge
\mathbb{E}_{q(x_{1:T}\mid x_0)}\big[\log p_\theta(x_{0:T})-\log q(x_{1:T}\mid x_0)\big].
$$
After algebra, this ELBO decomposes into KL terms at each step:
$$
\begin{aligned}
\log p_\theta(x_0)\;\ge\;&
\mathbb{E}_q\big[\log p_\theta(x_0\mid x_1)\big]
-\mathrm{KL}\big(q(x_T\mid x_0)\,\|\,p(x_T)\big) \\
&-\sum_{t=2}^T
\mathbb{E}_q\Big[
\mathrm{KL}\big(q(x_{t-1}\mid x_t,x_0)\,\|\,p_\theta(x_{t-1}\mid x_t)\big)
\Big].
\end{aligned}
$$
Here \(q(x_{t-1}\mid x_t,x_0)\) is the true reverse posterior induced by the forward Gaussian chain; it has a closed-form Gaussian expression, so these KL terms are tractable once \(p_\theta(x_{t-1}\mid x_t)\) is Gaussian.

### 4.5 The usual noise-prediction loss as a special case of the ELBO

In the common DDPM parameterization, one chooses \(p_\theta(x_{t-1}\mid x_t)\) to be Gaussian with a variance schedule fixed in advance, and a mean that is expressed via a network \(\varepsilon_\theta(x_t,t)\) (or equivalently \(x_{0,\theta}(x_t,t)\)). Under this choice, each KL term above reduces (up to an additive constant and a time-dependent weight) to an \(\ell_2\) regression loss:
$$
\mathbb{E}_{t,x_0,\varepsilon}\big[w(t)\,\|\varepsilon-\varepsilon_\theta(x_t,t)\|^2\big].
$$
So, on the flow axis, the “denoising MSE” objective is one particular way of fitting a reverse-time stochastic flow so that it matches the forward flow’s stepwise posteriors.

Remark: the exact weight \(w(t)\) depends on the schedule \(\beta_t\) and the variance parameterization in \(p_\theta(x_{t-1}\mid x_t)\); different “predict \(\varepsilon\)” vs “predict \(x_0\)” parameterizations correspond to the same ELBO written in different coordinates.

## Chapter 5 — Flow vs diffusion: what is fundamentally different?

Both flow models and diffusion models are “dynamics-based” generative models: they define a time-indexed family of random variables \((X_t)\) and use a learned field to transform a simple distribution into a complicated one. The crucial differences are in the **type of dynamics**, the **mathematical object that evolves**, and the **training signal that is tractable**.

### 5.1 Deterministic ODE flows vs stochastic SDE flows

- **Flow / CNF (ODE).** A continuous normalizing flow evolves particles deterministically:
$$
\frac{d}{dt}X_t=v(t,X_t;\theta).
$$
Under standard regularity (e.g., Lipschitz in \(x\)), the solution is unique and defines a (locally) invertible flow map \(\phi_{s\to t}\). Distribution evolution is push-forward: \(\mu_t=(\phi_{0\to t})_\#\mu_0\).

- **Diffusion (SDE).** A diffusion model starts from a stochastic forward process:
$$
dX_t=f(X_t,t)\,dt+g(t)\,dW_t.
$$
Even with \(X_0\) fixed, \(X_t\) remains random because of Brownian noise. The evolution is therefore not a bijection \(x_0\mapsto x_t\), but a Markov transition kernel \(P_{s\to t}(x,\cdot)\) acting on measures.

### 5.2 Eulerian density evolution: continuity equation vs Fokker–Planck

- **ODE \(\Rightarrow\) continuity equation (CE).** If \(X_t\) follows an ODE and admits a density \(p_t\), then
$$
\partial_t p_t(x)+\nabla\!\cdot\big(p_t(x)\,v(t,x)\big)=0.
$$
This is pure transport: probability mass is conserved and advected by \(v\).

- **SDE \(\Rightarrow\) Fokker–Planck.** If \(X_t\) follows an Itô SDE and admits a density, then
$$
\partial_t p_t(x)=-\nabla\!\cdot\big(f(x,t)p_t(x)\big)+\frac12 g(t)^2\,\Delta p_t(x),
$$
which adds a second-order diffusion term. This difference is not cosmetic: the diffusion term reflects the fact that noise spreads mass and destroys invertibility.

### 5.3 What is “designed” and what is “learned”

- **Diffusion:** the forward noising mechanism (equivalently \((f,g)\) or a discrete schedule) is designed so that we can generate samples \(x_t\sim p_t\) from data \(x_0\sim q\). The learned object is typically a score model \(s_\theta(x,t)\approx \nabla_x\log p_t(x)\) (or an equivalent parameterization such as \(\varepsilon_\theta\)).

- **Flow / CNF:** the learned object is directly a velocity field \(v(t,x;\theta)\). A likelihood formula exists in principle via
\(\frac{d}{dt}\log p_t(X_t)=-\nabla\!\cdot v(t,X_t;\theta)\),
but it can be computationally heavy in high dimension.

- **Flow matching:** sits between the two viewpoints. It also learns an ODE velocity field \(v(t,x;\theta)\), but it trains it using an Eulerian “match a path” idea (via CE), using conditional constructions to avoid intractable marginal objects.

### 5.4 Practical consequence: why training looks different

- In marginal flow matching, the marginal target \(u_t(x)\) is a posterior average and typically involves normalization by \(p_t(x)\), making it hard to compute with only sample access to \(q\). Conditional flow matching (CFM) resolves this by training on per-sample pairs \((x,u_t(x\mid x_1))\) with \(x_1\sim q\).

- In diffusion, we do not need to evaluate \(p_t(x)\) to generate training inputs: by design, the forward corruption mechanism provides unbiased samples \(x_t\sim p_t\) together with supervision signals for denoising/score matching.

### 5.5 Relation (not a contradiction): probability flow ODE

Although diffusion is fundamentally stochastic, the same marginal path \((p_t)\) can often be realized by a deterministic ODE (the probability flow ODE)
$$
\frac{d}{dt}X_t=f(X_t,t)-\frac12 g(t)^2\,\nabla_x\log p_t(X_t),
$$
which shares the same one-time marginals as the SDE when the score is exact. This provides a conceptual bridge: diffusion models can be sampled either stochastically (reverse SDE) or deterministically (PF-ODE), once \(\nabla_x\log p_t\) is learned.

## References

1. Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud. *Neural Ordinary Differential Equations*. NeurIPS (2018). arXiv:1806.07366 (DOI: `https://doi.org/10.48550/arXiv.1806.07366`). `https://arxiv.org/abs/1806.07366`
2. *Flow Matching* (blog post). Cambridge Machine Learning Group (Jan 20, 2024). `https://mlg.eng.cam.ac.uk/blog/2024/01/20/flow-matching.html#mjx-eqn%3Aeq%3Ag2g`
3. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, Matt Le. *Flow Matching for Generative Modeling*. arXiv:2210.02747 (DOI: `https://doi.org/10.48550/arXiv.2210.02747`). `https://arxiv.org/abs/2210.02747`
4. *Lagrangian and Eulerian specification of the flow field*. Wikipedia. `https://en.wikipedia.org/wiki/Lagrangian_and_Eulerian_specification_of_the_flow_field`
5. Yaron Lipman, Marton Havasi, Peter Holderrieth, Neta Shaul, Matt Le, Brian Karrer, Ricky T. Q. Chen, David Lopez-Paz, Heli Ben-Hamu, Itai Gat. *Flow Matching Guide and Code*. arXiv:2412.06264 (DOI: `https://doi.org/10.48550/arXiv.2412.06264`). `https://arxiv.org/abs/2412.06264`
6. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli. *Deep Unsupervised Learning using Nonequilibrium Thermodynamics*. ICML (2015). arXiv:1503.03585 (DOI: `https://doi.org/10.48550/arXiv.1503.03585`). `https://arxiv.org/abs/1503.03585`
7. Jonathan Ho, Ajay Jain, Pieter Abbeel. *Denoising Diffusion Probabilistic Models*. NeurIPS (2020). arXiv:2006.11239 (DOI: `https://doi.org/10.48550/arXiv.2006.11239`). `https://arxiv.org/abs/2006.11239`
8. Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole. *Score-Based Generative Modeling through Stochastic Differential Equations*. ICLR (2021). arXiv:2011.13456 (DOI: `https://doi.org/10.48550/arXiv.2011.13456`). `https://arxiv.org/abs/2011.13456`
