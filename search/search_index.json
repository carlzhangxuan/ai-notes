{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"AI Notes","text":"<p>This is a small public site generated from Markdown via MkDocs.</p>"},{"location":"index.html#posts","title":"Posts","text":"<ul> <li>Diffusion Models Intro</li> <li>Transformer</li> <li>ViT and DiT</li> </ul>"},{"location":"posts/diffusion-models-intro.html","title":"Diffusion Models Intro","text":""},{"location":"posts/diffusion-models-intro.html#chapter-1-from-fluid-mechanics-to-the-manifold-hypothesis","title":"Chapter 1 \u2014 From fluid mechanics to the manifold hypothesis","text":"<p>This chapter stays in the Euclidean setting: states are vectors \\(x\\in\\mathbb{R}^D\\) and time is \\(t\\in[0,1]\\). We use \u201cfluid\u201d terminology only as the mathematics of transport: trajectories (ODEs), distribution evolution, and conservation laws.</p>"},{"location":"posts/diffusion-models-intro.html#11-continuous-normalizing-flows-cnfs","title":"1.1 Continuous Normalizing Flows (CNFs)","text":"<p>We begin from the Lagrangian (particle) view: model a velocity field and move samples by an ODE. The corresponding Eulerian (density) view will appear in Section 1.2.</p> <p>Let $$ v:[0,1]\\times\\mathbb{R}^D\\to\\mathbb{R}^D,\\qquad (t,x)\\mapsto v(t,x) $$ be a time-dependent vector field (velocity). We will work in the Euclidean data space \\(\\mathbb{R}^D\\), with data points \\(x=(x_1,\\dots,x_D)\\in\\mathbb{R}^D\\).</p> <p>Two fundamental objects in flow-based generative modeling are:</p> <ul> <li>a probability density path \\(p:[0,1]\\times\\mathbb{R}^D\\to\\mathbb{R}_{\\ge 0}\\), written as \\(p_t(x):=p(t,x)\\), such that \\(\\int_{\\mathbb{R}^D} p_t(x)\\,dx=1\\) for every \\(t\\in[0,1]\\);</li> <li>a time-dependent vector field \\(v_t(\\cdot):=v(t,\\cdot)\\) that transports particles (and hence pushes densities forward).</li> </ul> <p>Given \\(v_t\\), define its flow map \\(\\phi_t:\\mathbb{R}^D\\to\\mathbb{R}^D\\) as the solution of the ODE $$ \\frac{d}{dt}\\phi_t(x)=v_t(\\phi_t(x)),\\tag{1} $$ with initial condition $$ \\phi_0(x)=x.\\tag{2} $$ Under standard regularity assumptions (e.g., \\(v_t\\) Lipschitz in \\(x\\) with suitable growth), \\(\\phi_t\\) is well-defined, and for each fixed \\(t\\) it is invertible (indeed a homeomorphism). If we further assume enough differentiability of \\(v\\) in \\(x\\) (e.g., \\(C^1\\) in \\(x\\)), then \\(\\phi_t\\) is (locally) a diffeomorphism. This is the continuous-time analog of composing many small invertible maps (normalizing flows).</p> <p>The defining modeling choice of a Continuous Normalizing Flow (CNF) (Chen et al., 2018) is to parameterize the vector field by a neural network \\(v_t(x;\\theta)\\). This induces a learnable family of flow maps \\(\\phi_t(\\cdot;\\theta)\\) via (1)\u2013(2).</p>"},{"location":"posts/diffusion-models-intro.html#push-forward-how-a-cnf-transports-densities","title":"Push-forward: how a CNF transports densities","text":"<p>Let \\(p_0\\) be a simple base density (e.g., \\(\\mathcal{N}(0,I)\\)) and define \\(X_t:=\\phi_t(X_0)\\) with \\(X_0\\sim p_0\\). The time-\\(t\\) density is the push-forward of \\(p_0\\) by \\(\\phi_t\\): $$ p_t = (\\phi_t)# p_0.\\tag{3} $$ When \\(\\phi_t\\) is a diffeomorphism, the push-forward has the familiar change-of-variables form: $$ (\\phi_t)# p_0(x) = p_0(\\phi_t^{-1}(x))\\; \\Big|\\det\\frac{\\partial \\phi_t^{-1}}{\\partial x}(x)\\Big|.\\tag{4} $$ Equivalently, \\(p_0(x)=p_t(\\phi_t(x))\\big|\\det(\\partial_x\\phi_t(x))\\big|\\).</p> <p>We say that a vector field \\(v_t\\) generates a density path \\((p_t)\\) if its flow \\(\\phi_t\\) satisfies (3). One practical way to check whether a candidate \\((p_t,v_t)\\) is consistent is the continuity equation, which we introduce next.</p> <p>For the full classical fluid-mechanics PDE system (beyond this kinematic ODE viewpoint), see Interlude \\(\\alpha\\) \u2014 Year 1757.</p> <p>For a more detailed treatment of first-order ODEs and the viewpoint of an ODE with a random initial condition (random initial value problem) \\(\\mathbf{X}_t=\\phi_t(\\mathbf{X}_0)\\), see Mathematics Foundation.</p>"},{"location":"posts/diffusion-models-intro.html#12-density-conservation-eulerian-view-the-continuity-equation","title":"1.2 Density conservation (Eulerian view): the continuity equation","text":"<p>Switch from particles to distributions. Let \\(\\mu_t\\) be the law of a random variable \\(\\mathbf{x}_t\\in\\mathbb{R}^D\\). If \\(\\mu_t\\) admits a density \\(p_t\\) with respect to Lebesgue measure, we write \\(\\mu_t(dx)=p_t(x)\\,dx\\).</p> <p>The conservation-of-mass statement for transport by the vector field \\(v_t\\) is the continuity equation $$ \\partial_t p_t(x) + \\nabla!\\cdot\\big(p_t(x)\\,v(t,x)\\big)=0, \\qquad x\\in\\mathbb{R}^D,\\; t\\in[0,1]. $$ Interpretation: probability mass is neither created nor destroyed; it is only moved by the vector field \\(v\\).</p> <p>Two equivalent \u201cbookkeeping\u201d forms (assuming enough smoothness) are useful:</p> <p>1) Along trajectories (material derivative). If \\(x(t)\\) solves \\(\\dot x(t)=v(t,x(t))\\), then $$ \\frac{d}{dt}p_t(x(t)) \\;=\\; \\partial_t p_t(x(t)) + \\nabla p_t(x(t))\\cdot v(t,x(t)) \\;=\\; -\\,p_t(x(t))\\,\\nabla!\\cdot v(t,x(t)). $$ So regions with positive divergence expand and lower density; negative divergence compress and raise density.</p> <p>2) Jacobian form (volume-element conservation). If the flow map \\(\\phi_{s\\to t}\\) is differentiable in \\(a\\) and \\(J_{s\\to t}(a):=\\det(\\nabla_a\\phi_{s\\to t}(a))\\), then $$ p_t(\\phi_{s\\to t}(a))\\,J_{s\\to t}(a)=p_s(a). $$ This is the precise version of \u201cmass in a moving infinitesimal volume element is conserved\u201d.</p> <p>Here \\(\\phi_{s\\to t}\\) denotes the flow map generated by \\(v\\) from time \\(s\\) to time \\(t\\).</p> <p>Remark: in measure-theoretic terms, transport by the flow map means \\(\\mu_t=(\\phi_{s\\to t})_\\#\\mu_s\\). The PDE above is the density-level expression of this push-forward relation.</p>"},{"location":"posts/diffusion-models-intro.html#13-the-manifold-hypothesis-a-precise-formulation","title":"1.3 The manifold hypothesis (a precise formulation)","text":"<p>Let \\(\\mu_\\text{data}\\) be the data distribution on \\(\\mathbb{R}^D\\) (a probability measure; it may or may not have a density).</p> <p>An idealized, mathematically clean version of the manifold hypothesis is:</p> <ul> <li>There exists a \\(d\\)-dimensional embedded \\(C^1\\) submanifold \\(\\mathcal{M}\\subset\\mathbb{R}^D\\) with \\(d\\ll D\\) such that $$ \\mathrm{supp}(\\mu_\\text{data}) \\subseteq \\mathcal{M}. $$</li> </ul> <p>A more realistic (noise-thickened) version is:</p> <ul> <li>There exists \\(\\sigma&gt;0\\) such that most mass lies in a tubular neighborhood of \\(\\mathcal{M}\\): $$ \\mu_\\text{data}\\big({x\\in\\mathbb{R}^D:\\mathrm{dist}(x,\\mathcal{M})\\le \\sigma}\\big)\\ge 1-\\varepsilon, $$ for a small \\(\\varepsilon\\in[0,1)\\).</li> </ul> <p>Important technical point: if \\(\\mathrm{supp}(\\mu_\\text{data})\\subseteq\\mathcal{M}\\) with \\(d&lt;D\\), then \\(\\mu_\\text{data}\\) is typically singular with respect to Lebesgue measure on \\(\\mathbb{R}^D\\); in particular, an ambient-space density \\(p_\\text{data}(x)\\) may not exist.</p>"},{"location":"posts/diffusion-models-intro.html#14-how-flow-models-relate-to-but-do-not-equal-manifold-learning","title":"1.4 How flow models relate to (but do not equal) manifold learning","text":"<p>A (continuous-time) flow model in \\(\\mathbb{R}^D\\) typically chooses a base distribution \\(\\mu_1\\) (often \\(\\mathcal{N}(0,I)\\)) and a diffeomorphic map \\(\\psi_{1\\to 0}\\) (generated by an ODE), then defines the model distribution by push-forward: $$ \\mu_\\theta := (\\psi_{1\\to 0})_# \\mu_1. $$</p> <p>This connects to the manifold hypothesis in a specific way:</p> <ul> <li>A diffeomorphism \\(\\psi_{1\\to 0}:\\mathbb{R}^D\\to\\mathbb{R}^D\\) maps absolutely continuous measures to absolutely continuous measures. Therefore, starting from \\(\\mu_1\\) with a smooth, full-dimensional density (e.g., Gaussian), an invertible flow model cannot represent an exactly low-dimensional, manifold-supported \\(\\mu_\\text{data}\\) in the idealized sense above.</li> <li>What a flow can do is concentrate mass near low-dimensional structure (a thin neighborhood of \\(\\mathcal{M}\\)) while remaining a full-dimensional density in \\(\\mathbb{R}^D\\). This is a transport statement, not an explicit reconstruction of \\(\\mathcal{M}\\) (no charts/atlas are learned).</li> </ul> <p>This is one reason diffusion-style constructions add noise along a path: intermediate distributions become \u201cthickened\u201d and typically admit well-behaved ambient densities, making training and reverse-time generation well-posed.</p>"},{"location":"posts/diffusion-models-intro.html#141-how-the-manifold-viewpoint-helps-and-what-diffusion-is-actually-doing","title":"1.4.1 How the manifold viewpoint helps, and what diffusion is actually doing","text":"<p>In image generation, a \u201cmanifold\u201d is typically a shorthand for the empirical observation that natural images occupy a very thin subset of \\(\\mathbb{R}^D\\): most variability can be explained by far fewer degrees of freedom than \\(D=H\\times W\\times C\\). In an idealized form, one writes \\(\\mathrm{supp}(\\mu_\\text{data})\\subseteq\\mathcal{M}\\) for a low-dimensional \\(\\mathcal{M}\\subset\\mathbb{R}^D\\); in a realistic form, \\(\\mu_\\text{data}\\) concentrates in a small neighborhood of \\(\\mathcal{M}\\).</p> <p>The manifold viewpoint clarifies why diffusion adds noise. If data are extremely \u201cthin\u201d (close to a low-dimensional set), then objects like an ambient-space density \\(p_\\text{data}(x)\\) or an ambient score \\(\\nabla_x\\log p_\\text{data}(x)\\) may be ill-behaved or not even well-defined. Diffusion constructs a Gaussian-perturbation path (schematically \\(x_t=\\alpha(t)x_0+\\sigma(t)\\epsilon\\)), which \u201cthickens\u201d the distribution so that for \\(t&gt;0\\) the marginal \\(p_t\\) is typically a regular, full-dimensional density on \\(\\mathbb{R}^D\\). The learned score/velocity field then tells you how to move samples from noise back toward the high-density region near the data, but it does not explicitly recover a geometric object \\(\\mathcal{M}\\) (no charts, projections, or tangent structure are output).</p>"},{"location":"posts/diffusion-models-intro.html#142-what-kinds-of-image-generative-models-are-manifold-learning","title":"1.4.2 What kinds of image generative models are manifold learning?","text":"<p>Models are closer to \u201cmanifold learning\u201d when they explicitly posit (and learn) a low-dimensional parameterization of images, i.e., a map \\(g:\\mathbb{R}^d\\to\\mathbb{R}^D\\) with \\(d\\ll D\\) such that typical images satisfy \\(x\\approx g(z)\\).</p> <ul> <li>Autoencoders (AE/DAE) and VAEs: learn an encoder/decoder pair and a low-dimensional latent \\(z\\). This makes the \u201cdata live near a low-dimensional set\u201d idea explicit (though VAEs add stochasticity and optimize a likelihood bound rather than directly estimating \\(\\mathcal{M}\\)).</li> <li>GANs / implicit generative models: a generator \\(x=g_\\theta(z)\\) with \\(z\\in\\mathbb{R}^d\\) defines (in the idealized deterministic case) a distribution supported on a \\(d\\)-dimensional set \\(\\mathrm{Im}(g_\\theta)\\). This is geometrically close to learning a manifold-like image set, even if the learned set is only approximate and the notion of \u201cmanifold\u201d may fail globally.</li> <li>Latent generative models (e.g., latent diffusion): learn a low-dimensional representation via an autoencoder, then run diffusion (or another density model) in latent space. This is often a practical middle ground: \u201cmanifold-like\u201d structure is delegated to the autoencoder, while diffusion handles distribution modeling in the latent.</li> </ul> <p>In particular, a VAE makes the low-dimensional structure explicit via a latent variable \\(z\\in\\mathbb{R}^d\\) (with \\(d\\ll D\\)) and a decoder distribution \\(p_\\theta(x\\mid z)\\) on \\(x\\in\\mathbb{R}^D\\). With a prior \\(p(z)\\) (often \\(\\mathcal{N}(0,I_d)\\)) and an encoder \\(q_\\phi(z\\mid x)\\), it is trained by maximizing the ELBO: $$ \\log p_\\theta(x) \\;\\ge\\; \\mathbb{E}{z\\sim q\\phi(z\\mid x)}\\big[\\log p_\\theta(x\\mid z)\\big] -\\mathrm{KL}\\big(q_\\phi(z\\mid x)\\,|\\,p(z)\\big). $$ Geometrically, the mean decoder map \\(g_\\theta(z):=\\mathbb{E}[x\\mid z]\\) can be viewed as a learned low-dimensional \u201csurface\u201d, while the decoder noise determines how thick the model distribution is around that surface; this is why VAEs feel \u201cmanifold-flavored\u201d but are not necessarily learning a strict manifold-supported distribution in \\(\\mathbb{R}^D\\).</p> <p>These approaches make the low-dimensional structure explicit via \\(z\\mapsto x\\). By contrast, standard diffusion/score/flow-matching methods (as used in image generation) are usually framed as learning dynamics in the ambient Euclidean space \\(\\mathbb{R}^D\\) (or in a learned latent space), rather than directly estimating the manifold \\(\\mathcal{M}\\) itself.</p>"},{"location":"posts/diffusion-models-intro.html#15-bridge-to-the-rest-of-the-intro","title":"1.5 Bridge to the rest of the intro","text":"<p>We will reuse the same transport backbone:</p> <ul> <li>Choose a probability path \\((\\mu_t)_{t\\in[0,1]}\\) (or densities \\((p_t)\\) when they exist).</li> <li>Learn dynamics (ODE or SDE) that generate that path.</li> <li>Sample by numerically integrating the learned dynamics backward from a simple endpoint distribution.</li> </ul>"},{"location":"posts/diffusion-models-intro.html#chapter-2-flow-matching","title":"Chapter 2 \u2014 Flow Matching","text":"<p>Before introducing flow matching, it is helpful to recall what a CNF gives us \u201cin closed form\u201d (as an operator), and what remains expensive in practice.</p> <p>Consider a CNF defined by the ODE $$ \\frac{d}{dt}X_t = v(t,X_t;\\theta),\\qquad t\\in[0,1],\\qquad X_0\\sim p_0. $$ Even when the solution is not available analytically, it always admits the integral form $$ X_t = X_0 + \\int_0^t v(s,X_s;\\theta)\\,ds, $$ which is what numerical ODE solvers approximate.</p> <p>If the flow map \\(\\phi_t\\) is a diffeomorphism and the density \\(p_t\\) exists, then along trajectories \\(t\\mapsto X_t\\) the log-density obeys the instantaneous change-of-variables formula $$ \\frac{d}{dt}\\log p_t(X_t) = -\\,\\nabla!\\cdot v(t,X_t;\\theta), $$ hence $$ \\log p_1(X_1)=\\log p_0(X_0)-\\int_0^1 \\nabla!\\cdot v(t,X_t;\\theta)\\,dt. $$ This provides a principled likelihood bookkeeping rule for CNFs, but it is often computationally heavy in high dimension because it couples an ODE solve with repeated divergence (trace) evaluations.</p> <p>Flow matching takes the complementary Eulerian density-evolution viewpoint from Section 1.2: rather than training the model primarily through likelihood computation, we specify (or construct) a target probability path \\((p_t)\\) and train a vector field \\(v(t,x;\\theta)\\) so that its induced density evolution matches that path (via the continuity equation).</p>"},{"location":"posts/diffusion-models-intro.html#21-constructing-p_tu_t-from-conditional-probability-paths-and-vector-fields","title":"2.1 CONSTRUCTING \\(p_t,u_t\\) FROM CONDITIONAL PROBABILITY PATHS AND VECTOR FIELDS","text":"<p>Let \\(X_1\\sim q(x_1)\\) denote the (unknown) data distribution; we only have sample access to \\(q\\). We would like a probability path \\((p_t)_{t\\in[0,1]}\\) such that \\(p_0=p\\) is simple (e.g., \\(\\mathcal{N}(0,I)\\)) and \\(p_1\\approx q\\), together with a vector field \\(u_t\\) that generates this path (so that \\((p_t,u_t)\\) satisfy the continuity equation).</p> <p>The main difficulty is not that CNFs are \u201cincalculable\u201d, but that the marginal objects \\(p_t\\) and \\(u_t\\) are generally not available in closed form, so na\u00efvely we cannot (i) sample \\(x\\sim p_t(x)\\) for arbitrary \\(t\\), nor (ii) evaluate \\(u_t(x)\\) to supervise a neural vector field.</p> <p>A key idea in flow matching is to construct the marginal path and vector field by mixing conditional paths that are defined per data sample.</p> <p>Theorem 1 (marginalizing conditional paths and vector fields). Fix a family of conditional probability paths \\(\\{p_t(\\cdot\\mid x_1)\\}_{x_1}\\) such that</p> <ul> <li>\\(p_0(x\\mid x_1)=p(x)\\) for all \\(x_1\\), and</li> <li>\\(p_1(x\\mid x_1)\\) is concentrated around \\(x_1\\) (e.g., \\(p_1(x\\mid x_1)=\\mathcal{N}(x\\mid x_1,\\sigma^2 I)\\) for a small \\(\\sigma&gt;0\\)).</li> </ul> <p>Define the marginal (mixture) path $$ p_t(x)=\\int_{\\mathbb{R}^D} p_t(x\\mid x_1)\\,q(x_1)\\,dx_1.\\tag{6} $$ In particular, $$ p_1(x)=\\int_{\\mathbb{R}^D} p_1(x\\mid x_1)\\,q(x_1)\\,dx_1 \\approx q(x).\\tag{7} $$ Assume \\(p_t(x)&gt;0\\) for all \\(t\\in[0,1]\\) and \\(x\\in\\mathbb{R}^D\\). Let \\(u_t(x\\mid x_1)\\) be conditional vector fields that generate the conditional paths \\(p_t(\\cdot\\mid x_1)\\). Define the marginal vector field $$ u_t(x)=\\int_{\\mathbb{R}^D} u_t(x\\mid x_1)\\,\\frac{p_t(x\\mid x_1)\\,q(x_1)}{p_t(x)}\\,dx_1.\\tag{8} $$ Then the pair \\((p_t,u_t)\\) satisfies the continuity equation. (As the paper emphasizes: \u201cthe marginal vector field ... generates the marginal probability path\u201d.)</p> <p>Explanation (what this buys us). Theorem 1 gives a mathematically clean bridge between:</p> <ul> <li>a per-sample design \\(\\big(p_t(\\cdot\\mid x_1),u_t(\\cdot\\mid x_1)\\big)\\), where \\(x_1\\sim q\\) is available from the dataset, and</li> <li>a global (marginal) density path \\(p_t\\) and its generator \\(u_t\\), which are guaranteed to be consistent through the continuity equation.</li> </ul> <p>This resolves the practical bottleneck in the marginal formulation: we do not need closed-form access to \\(p_t\\) or \\(u_t\\). Instead, we can sample \\(x_1\\sim q\\), sample \\(x\\sim p_t(\\cdot\\mid x_1)\\), and compute \\(u_t(x\\mid x_1)\\) to build a tractable training objective (developed next via conditional flow matching).</p> <p>In words: since for each fixed \\(x_1\\) the conditional pair \\(\\big(p_t(\\cdot\\mid x_1),u_t(\\cdot\\mid x_1)\\big)\\) satisfies the conditional continuity equation, Theorem 1 shows that the induced marginal pair \\((p_t,u_t)\\) (via the mixture constructions (6)\u2013(8)) satisfies the marginal continuity equation as well.</p> <p>The proof of Theorem 1 can be found in the appendix of Flow Matching for Generative Modeling (see References). For an additional exposition, see introduction-B-flow-matching.</p>"},{"location":"posts/diffusion-models-intro.html#22-conditional-flow-matching","title":"2.2 CONDITIONAL FLOW MATCHING","text":"<p>Operationally, CFM makes training possible by working with per-sample training pairs: sample \\(x_1\\sim q\\), sample \\(x\\sim p_t(\\cdot\\mid x_1)\\), compute the conditional target \\(u_t(x\\mid x_1)\\), and regress \\(v(t,x;\\theta)\\) toward it.</p> <p>The mixture constructions (6)\u2013(8) show that a marginal path \\(p_t\\) and a marginal vector field \\(u_t\\) exist and satisfy the continuity equation. However, because \\(q\\) is only accessible through samples, the integrals in (6)\u2013(8) are typically intractable. In particular, we cannot na\u00efvely compute \\(u_t(x)\\), so we cannot directly build an unbiased estimator of the marginal flow-matching regression loss $$ \\mathcal{L}{\\mathrm{FM}}(\\theta) := \\mathbb{E}{t\\sim U[0,1],\\,x\\sim p_t}\\Big[|v(t,x;\\theta)-u_t(x)|^2\\Big].\\tag{5} $$</p> <p>Instead, conditional flow matching replaces the marginal target \\(u_t(x)\\) by the per-sample conditional target \\(u_t(x\\mid x_1)\\), yielding the CFM objective $$ \\mathcal{L}{\\mathrm{CFM}}(\\theta) := \\mathbb{E}{t\\sim U[0,1],\\,x_1\\sim q,\\,x\\sim p_t(\\cdot\\mid x_1)} \\Big[|v(t,x;\\theta)-u_t(x\\mid x_1)|^2\\Big].\\tag{9} $$ This objective is tractable as long as we can (i) sample \\(x\\sim p_t(\\cdot\\mid x_1)\\) and (ii) evaluate \\(u_t(x\\mid x_1)\\), both of which can be arranged by design since they are defined on a per-sample basis.</p> <p>Theorem 2 (FM and CFM are equivalent up to a constant). Assume \\(p_t(x)&gt;0\\) for all \\(x\\in\\mathbb{R}^D\\) and \\(t\\in[0,1]\\), and that the constructions in (6)\u2013(8) are well-defined. Then $$ \\mathcal{L}{\\mathrm{CFM}}(\\theta)=\\mathcal{L}{\\mathrm{FM}}(\\theta)+C, $$ where \\(C\\) does not depend on \\(\\theta\\). In particular, $$ \\nabla_\\theta \\mathcal{L}{\\mathrm{FM}}(\\theta)=\\nabla\\theta \\mathcal{L}_{\\mathrm{CFM}}(\\theta). $$</p> <p>Explanation (why the gradients match). Intuitively, (8) says that the marginal target is a posterior average: \\(u_t(x)=\\mathbb{E}[u_t(x\\mid X_1)\\mid X_t=x]\\). Expanding the square and applying the law of total expectation shows that the only difference between (5) and (9) is a conditional-variance term \\(\\mathbb{E}\\big[\\|u_t(X_t\\mid X_1)\\|^2-\\|u_t(X_t)\\|^2\\big]\\), which is independent of \\(\\theta\\). Therefore, optimizing the tractable conditional objective (9) is equivalent (in expectation) to optimizing the intractable marginal objective (5).</p> <p>The proof of Theorem 2 can be found in Flow Matching for Generative Modeling and in Flow Matching Guide and Code (see References). For an additional exposition, see introduction-B-flow-matching.</p>"},{"location":"posts/diffusion-models-intro.html#23-conditional-probability-paths-and-vector-fields","title":"2.3 CONDITIONAL PROBABILITY PATHS AND VECTOR FIELDS","text":"<p>The CFM objective is general: it works with any choice of conditional probability paths \\(p_t(\\cdot\\mid x_1)\\) and conditional vector fields \\(u_t(\\cdot\\mid x_1)\\) (as long as they are sampleable/evaluable). A particularly convenient family is given by Gaussian conditional paths, because both sampling and closed-form conditional targets can be obtained.</p>"},{"location":"posts/diffusion-models-intro.html#gaussian-conditional-paths","title":"Gaussian conditional paths","text":"<p>Fix \\(x_1\\in\\mathbb{R}^D\\). Consider a conditional probability path of the form $$ p_t(x\\mid x_1)=\\mathcal{N}!\\big(x\\mid \\mu_t(x_1),\\sigma_t(x_1)^2 I\\big),\\qquad t\\in[0,1],\\tag{10} $$ where \\(\\mu:[0,1]\\times\\mathbb{R}^D\\to\\mathbb{R}^D\\) is a time-dependent mean and \\(\\sigma:[0,1]\\times\\mathbb{R}^D\\to\\mathbb{R}_{&gt;0}\\) is a time-dependent scalar standard deviation. We typically set $$ \\mu_0(x_1)=0,\\qquad \\sigma_0(x_1)=1, $$ so that \\(p_0(x\\mid x_1)=\\mathcal{N}(0,I)=:p(x)\\) for all \\(x_1\\), and $$ \\mu_1(x_1)=x_1,\\qquad \\sigma_1(x_1)=\\sigma_{\\min}, $$ so that \\(p_1(\\cdot\\mid x_1)\\) is concentrated near \\(x_1\\) when \\(\\sigma_{\\min}\\) is small.</p>"},{"location":"posts/diffusion-models-intro.html#a-canonical-conditional-flow-affine-map","title":"A canonical conditional flow (affine map)","text":"<p>Many vector fields can generate the same density path (e.g., one can add a component \\(w_t\\) such that \\(\\nabla\\!\\cdot(p_t w_t)=0\\)), but for Gaussian paths it is natural to choose the simplest canonical transport: an affine map that turns standard Gaussian noise into the desired Gaussian.</p> <p>Define the conditional (affine) flow map $$ \\psi_t(x_0;x_1):=\\sigma_t(x_1)\\,x_0+\\mu_t(x_1).\\tag{11} $$ If \\(x_0\\sim \\mathcal{N}(0,I)\\), then \\(\\psi_t(x_0;x_1)\\sim \\mathcal{N}(\\mu_t(x_1),\\sigma_t(x_1)^2 I)\\). In push-forward notation, $$ (\\psi_t(\\cdot;x_1))_#\\,p(\\cdot)=p_t(\\cdot\\mid x_1).\\tag{12} $$</p> <p>This flow induces a conditional vector field \\(u_t(\\cdot\\mid x_1)\\) defined implicitly by $$ \\frac{d}{dt}\\psi_t(x_0;x_1)=u_t(\\psi_t(x_0;x_1)\\mid x_1).\\tag{13} $$</p> <p>Reparameterizing \\(x\\sim p_t(\\cdot\\mid x_1)\\) via \\(x=\\psi_t(x_0;x_1)\\) with \\(x_0\\sim p(x_0)=\\mathcal{N}(0,I)\\), the CFM objective (9) can be written as $$ \\mathcal{L}{\\mathrm{CFM}}(\\theta) = \\mathbb{E}{t\\sim U[0,1],\\,x_1\\sim q,\\,x_0\\sim p} \\Big[\\big|v(t,\\psi_t(x_0;x_1);\\theta)-\\tfrac{d}{dt}\\psi_t(x_0;x_1)\\big|^2\\Big].\\tag{14} $$</p> <p>Theorem 3 (closed-form conditional vector field for Gaussian paths). Let \\(p_t(x\\mid x_1)\\) be the Gaussian conditional path in (10) and \\(\\psi_t(\\cdot;x_1)\\) be the affine flow map in (11). Then the (unique) vector field that realizes (13) is $$ u_t(x\\mid x_1) = \\frac{\\partial_t \\sigma_t(x_1)}{\\sigma_t(x_1)}\\big(x-\\mu_t(x_1)\\big) \\;+\\;\\partial_t\\mu_t(x_1).\\tag{15} $$ Consequently, \\(u_t(\\cdot\\mid x_1)\\) generates the Gaussian path \\(p_t(\\cdot\\mid x_1)\\) (equivalently, \\((p_t(\\cdot\\mid x_1),u_t(\\cdot\\mid x_1))\\) satisfy the conditional continuity equation).</p> <p>Derivation (one line). Differentiate (11) in time: \\(\\frac{d}{dt}\\psi_t(x_0;x_1)=\\partial_t\\sigma_t(x_1)\\,x_0+\\partial_t\\mu_t(x_1)\\). Since \\(x=\\psi_t(x_0;x_1)=\\sigma_t(x_1)x_0+\\mu_t(x_1)\\), we have \\(x_0=(x-\\mu_t(x_1))/\\sigma_t(x_1)\\), which substituted back yields (15).</p> <p>The proofs and additional discussion can be found in Flow Matching for Generative Modeling and Flow Matching Guide and Code (see References).</p>"},{"location":"posts/diffusion-models-intro.html#24-examples-special-instances-of-gaussian-conditional-probability-paths","title":"2.4 Examples: special instances of Gaussian conditional probability paths","text":"<p>The Gaussian construction above is fully general: we may choose any differentiable \\(\\mu_t(x_1)\\) and \\(\\sigma_t(x_1)\\) satisfying desired boundary conditions and obtain a closed-form conditional target \\(u_t(x\\mid x_1)\\) via Theorem 3. Below are two illustrative design choices.</p>"},{"location":"posts/diffusion-models-intro.html#example-i-diffusion-inspired-conditional-paths-ve-vp","title":"Example I: diffusion-inspired conditional paths (VE / VP)","text":"<p>Many diffusion models induce Gaussian conditionals \\(p_t(x\\mid x_1)\\) at arbitrary times \\(t\\), with specific \\(\\mu_t(x_1)\\) and \\(\\sigma_t(x_1)\\). In the flow-matching setting, we can simply take these conditionals as a probability-path design choice and obtain a deterministic conditional vector field by plugging them into (15).</p> <p>(a) Reversed VE (noise \\(\\to\\) data). A variance-exploding (VE) conditional path can be written as $$ p_t(x\\mid x_1)=\\mathcal{N}!\\big(x\\mid x_1,\\sigma_{1-t}^2 I\\big),\\tag{16} $$ where \\(\\sigma_s\\) is increasing with \\(\\sigma_0=0\\) and \\(\\sigma_1\\) large. Here \\(t=1\\) corresponds to a distribution concentrated at \\(x_1\\), while \\(t=0\\) is a very wide Gaussian (an approximation to \u201cnoise\u201d in practice). In this case \\(\\mu_t(x_1)=x_1\\) and \\(\\sigma_t(x_1)=\\sigma_{1-t}\\), so (15) gives $$ u_t(x\\mid x_1)= -\\frac{\\sigma'{1-t}}{\\sigma{1-t}}\\,(x-x_1),\\tag{17} $$ where \\(\\sigma'_s=\\frac{d}{ds}\\sigma_s\\).</p> <p>(b) Reversed VP (noise \\(\\to\\) data). A variance-preserving (VP) conditional path can be written as $$ p_t(x\\mid x_1)=\\mathcal{N}!\\big(x\\mid \\alpha_{1-t}x_1,\\,(1-\\alpha_{1-t}^2)I\\big),\\tag{18} $$ where $$ \\alpha_t := \\exp!\\Big(-\\frac12 T(t)\\Big),\\qquad T(t):=\\int_0^t \\beta(s)\\,ds, $$ and \\(\\beta\\) is a nonnegative noise-rate schedule. When \\(T(1)\\) is large, \\(p_0(\\cdot\\mid x_1)\\approx \\mathcal{N}(0,I)\\) and \\(p_1(\\cdot\\mid x_1)\\) concentrates at \\(x_1\\). Here \\(\\mu_t(x_1)=\\alpha_{1-t}x_1\\) and \\(\\sigma_t(x_1)=\\sqrt{1-\\alpha_{1-t}^2}\\), and (15) yields $$ u_t(x\\mid x_1)=\\frac{\\alpha'{1-t}}{1-\\alpha{1-t}^2}\\big(\\alpha_{1-t}x-x_1\\big),\\tag{19} $$ where \\(\\alpha'_s=\\frac{d}{ds}\\alpha_s\\). (Since \\(\\alpha\\) is decreasing when \\(\\beta&gt;0\\), \\(\\alpha'_s&lt;0\\), so the drift indeed points toward \\(x_1\\) as \\(t\\to 1\\).)</p> <p>Remark: these diffusion-inspired paths are typically derived from stochastic diffusion processes and may only approach an idealized \u201cpure noise\u201d distribution asymptotically; in practice one works with a finite terminal time and an approximate Gaussian endpoint.</p>"},{"location":"posts/diffusion-models-intro.html#example-ii-optimal-transport-inspired-conditional-paths-linear-meanstd","title":"Example II: optimal-transport-inspired conditional paths (linear mean/std)","text":"<p>An arguably simpler design is to let the mean and standard deviation evolve linearly in time: $$ \\mu_t(x_1)=t\\,x_1,\\qquad \\sigma_t(x_1)=1-(1-\\sigma_{\\min})t.\\tag{20} $$ Plugging (20) into (15) gives the conditional vector field $$ u_t(x\\mid x_1)=\\frac{x_1-(1-\\sigma_{\\min})x}{1-(1-\\sigma_{\\min})t}.\\tag{21} $$ The corresponding affine flow is $$ \\psi_t(x_0;x_1)=(1-(1-\\sigma_{\\min})t)x_0+t x_1.\\tag{22} $$ In this case the CFM loss (14) becomes $$ \\mathcal{L}{\\mathrm{CFM}}(\\theta) = \\mathbb{E}{t\\sim U[0,1],\\,x_1\\sim q,\\,x_0\\sim p} \\Big[\\big|v(t,\\psi_t(x_0;x_1);\\theta)-\\big(x_1-(1-\\sigma_{\\min})x_0\\big)\\big|^2\\Big].\\tag{23} $$ Compared with diffusion-inspired choices, the OT-style conditional vector field often has a simpler structure (e.g., its direction can be constant in time up to a scalar factor), which can make the regression task easier.</p> <p>For practical experimental-design details (e.g., path/schedule choices, loss weighting, and numerical solvers), see introduction-B-flow-matching.</p>"},{"location":"posts/diffusion-models-intro.html#25-practical-notes-implementation-gotchas","title":"2.5 Practical notes (implementation \u201cgotchas\u201d)","text":"<p>This chapter intentionally focuses on the modeling algebra. In practice, a few design choices matter disproportionately:</p> <ul> <li>Endpoints and numerical stability. Many choices make \\(u_t(x\\mid x_1)\\) contain factors like \\(1/\\sigma_t(x_1)\\) or \\(1/(1-\\alpha_t^2)\\); avoid singular endpoints by using \\(\\sigma_{\\min}&gt;0\\), clipping \\(t\\) away from \\(\\{0,1\\}\\), or using a schedule that keeps denominators bounded.</li> <li>How to sample time \\(t\\). Uniform \\(t\\sim U[0,1]\\) is the default in the theory, but non-uniform sampling or explicit weights \\(w(t)\\) in the regression loss can improve conditioning (e.g., not over-emphasizing near-singular regions).</li> <li>Parameterization of the learned field. Even when the target is a conditional vector field, different parameterizations of \\(v(t,x;\\theta)\\) (predicting a velocity vs. predicting an equivalent noise-like quantity) can change optimization behavior while representing the same underlying solution.</li> <li>Choice of conditional path family. Diffusion-inspired vs. OT-inspired conditionals can change the geometry of trajectories (e.g., \u201cstraight\u201d vs. \u201cnoisy/curved\u201d conditionals), which changes the difficulty of the regression problem and the behavior of sampling.</li> <li>Sampling solver and error. Training uses exact conditional targets, but generation still integrates an ODE induced by the learned \\(v_\\theta\\); solver choice and step count (and whether to use adaptive stepping) can materially affect sample quality and speed.</li> </ul> <p>For a more hands-on discussion of these choices (including code-level details), see introduction-B-flow-matching.</p>"},{"location":"posts/diffusion-models-intro.html#chapter-3-diffusion-models-a-noising-path-and-a-learnable-reverse-process","title":"Chapter 3 \u2014 Diffusion models: a noising path and a learnable reverse process","text":"<p>In ODE-based flow models, the dynamics are deterministic: under standard regularity, the flow map is (locally) invertible and the Eulerian density evolution obeys the continuity equation. In diffusion models, the forward dynamics are stochastic (a Markov kernel rather than a bijection), so density evolution must account for diffusion and is governed by the Fokker\u2013Planck equation.</p> <p>The key design choice in diffusion models is to pick a probability path that connects data \\(\\mu_0=\\mu_\\text{data}\\) to a simple distribution \\(\\mu_1\\) (usually close to \\(\\mathcal{N}(0,I)\\)), by injecting Gaussian noise according to a schedule. This makes intermediate marginals \u201cthick\u201d (typically admitting nice ambient-space densities), and it gives a principled way to define a reverse-time generative dynamics.</p> <p>We present both the discrete-time (DDPM-style) and continuous-time (SDE-style) viewpoints. Detailed derivations about SDEs and the Fokker\u2013Planck equation are collected separately in Mathematics Foundation.</p>"},{"location":"posts/diffusion-models-intro.html#31-eulerian-view-density-evolution-by-fokkerplanck","title":"3.1 Eulerian view: density evolution by Fokker\u2013Planck","text":"<p>To mirror the Eulerian viewpoint of Section 1.2, we start from density evolution rather than from a discrete Markov chain.</p> <p>Let \\(t\\in[0,1]\\). Consider a forward It\u00f4 SDE $$ dX_t = f(X_t,t)\\,dt + g(t)\\,dW_t,\\qquad X_0\\sim \\mu_\\text{data}, $$ where \\(W_t\\) is Brownian motion. If the time-\\(t\\) law \\(\\mu_t=\\mathcal{L}(X_t)\\) admits a density \\(\\mu_t(dx)=p_t(x)\\,dx\\), then \\(p_t\\) satisfies the Fokker\u2013Planck equation $$ \\partial_t p_t(x) = -\\nabla!\\cdot\\big(f(x,t)p_t(x)\\big)+\\frac12 g(t)^2\\,\\Delta p_t(x). $$ This is the stochastic analog of the continuity equation: the drift \\(f\\) transports mass (a CE-like term), while the Brownian noise adds a second-order diffusion term.</p> <p>The \u201cschedule\u201d viewpoint appears here as the choice of coefficients \\((f,g)\\), which determines the entire marginal path \\((p_t)\\). In diffusion modeling we choose \\((f,g)\\) so that \\(p_1\\) is simple (approximately \\(\\mathcal{N}(0,I)\\)) and \\(p_0\\) matches the data distribution.</p>"},{"location":"posts/diffusion-models-intro.html#32-reverse-time-generation-score-enters-the-dynamics","title":"3.2 Reverse-time generation: score enters the dynamics","text":"<p>The key generative fact is that the reverse-time dynamics involve the score \\(\\nabla_x\\log p_t(x)\\). Informally, if we run time backward from \\(t=1\\) to \\(t=0\\), the reverse-time SDE has the form (here written for the common case where \\(g(t)\\) is a scalar diffusion coefficient, i.e., isotropic noise depending only on time) $$ dX_t = \\Big(f(X_t,t)-g(t)^2\\,\\nabla_x\\log p_t(X_t)\\Big)\\,dt + g(t)\\,d\\bar W_t, \\qquad t:1\\to 0, $$ where \\(\\bar W_t\\) is a Brownian motion in reverse time.</p> <p>For background on SDEs, It\u00f4 interpretation, and the Fokker\u2013Planck equation (including derivations), see Mathematics Foundation.</p> <p>There is also an associated probability flow ODE (deterministic dynamics with the same marginals \\((p_t)\\)): $$ \\frac{d}{dt}X_t = f(X_t,t)-\\frac12 g(t)^2\\,\\nabla_x\\log p_t(X_t). $$ Once we can approximate the score, we can sample either by integrating the reverse SDE (stochastic sampler) or the probability flow ODE (deterministic sampler).</p>"},{"location":"posts/diffusion-models-intro.html#33-learning-the-score-why-training-is-tractable","title":"3.3 Learning the score (why training is tractable)","text":"<p>Even though the marginal density \\(p_t(x)\\) is unknown (and is generally difficult to model in closed form, regardless of whether it is introduced marginally or via conditional constructions), the forward diffusion/noising mechanism is known and sampleable by design. Concretely, we can draw \\(x_0\\sim q\\) from the dataset, sample noise \\(\\varepsilon\\), and generate \\(x_t\\sim p_t\\) by running the forward corruption process.</p> <p>This avoids the main intractability encountered in marginal flow matching: we do not need to evaluate a marginal vector field \\(u_t(x)\\) that involves posterior normalization by \\(p_t(x)\\). Instead, we obtain unbiased training pairs \\((t,x_t)\\) directly, and then fit a neural network \\(s_\\theta(x,t)\\approx \\nabla_x\\log p_t(x)\\) via denoising score matching objectives.</p>"},{"location":"posts/diffusion-models-intro.html#34-discrete-time-ddpm-as-a-practical-discretization","title":"3.4 Discrete-time DDPM as a practical discretization","text":"<p>After fixing the continuous-time (Eulerian) picture, we can introduce DDPM as a convenient discrete-time construction of the same \u201cdesign a noising path, learn a reverse process\u201d idea.</p> <p>Fix an integer \\(T\\). The forward process is a Markov chain $$ q(x_{0:T}) = q(x_0)\\prod_{k=1}^T q(x_k\\mid x_{k-1}), $$ where \\(q(x_0)=\\mu_\\text{data}\\) and $$ q(x_k\\mid x_{k-1})=\\mathcal{N}!\\big(\\sqrt{1-\\beta_k}\\,x_{k-1},\\;\\beta_k I\\big), \\qquad k=1,\\dots,T, $$ with noise schedule \\((\\beta_k)\\). Define \\(\\alpha_k:=1-\\beta_k\\) and \\(\\bar\\alpha_k:=\\prod_{s=1}^k \\alpha_s\\). Then $$ q(x_k\\mid x_0)=\\mathcal{N}!\\big(\\sqrt{\\bar\\alpha_k}\\,x_0,\\;(1-\\bar\\alpha_k)I\\big), $$ equivalently $$ x_k=\\sqrt{\\bar\\alpha_k}\\,x_0+\\sqrt{1-\\bar\\alpha_k}\\,\\varepsilon,\\qquad \\varepsilon\\sim\\mathcal{N}(0,I). $$</p> <p>Generation runs the chain backward, sampling \\(p_\\theta(x_{k-1}\\mid x_k)\\). A common parameterization predicts \\(\\varepsilon_\\theta(x_k,k)\\), which can be converted to an estimate \\(\\hat x_0(x_k,k)\\) and hence to a reverse mean. The ELBO view and its reduction to a denoising regression loss are given in Chapter 4.</p>"},{"location":"posts/diffusion-models-intro.html#chapter-4-a-flow-first-physical-view-of-diffusion-with-ddpm-elbo-as-a-special-case","title":"Chapter 4 \u2014 A flow-first (physical) view of diffusion, with DDPM-ELBO as a special case","text":"<p>The \u201cphysical\u201d intuition is: diffusion models are stochastic flows of particles. We track particles by an SDE (Lagrangian view), and we track their law by a PDE (Eulerian view). The PDE is not an extra constraint we impose; it is the density-level consequence of the particle dynamics.</p>"},{"location":"posts/diffusion-models-intro.html#41-stochastic-flows-lagrangian-rightarrow-fokkerplanck-eulerian","title":"4.1 Stochastic flows (Lagrangian) \\(\\Rightarrow\\) Fokker\u2013Planck (Eulerian)","text":"<p>Consider the forward SDE on \\(\\mathbb{R}^D\\) $$ dX_t=f(X_t,t)\\,dt+g(t)\\,dW_t,\\qquad t\\in[0,1],\\qquad X_0\\sim \\mu_\\text{data}. $$ If the marginal law \\(\\mu_t=\\mathcal{L}(X_t)\\) admits a density \\(p_t\\) and regularity is sufficient, then \\(p_t\\) evolves by the Fokker\u2013Planck equation $$ \\partial_t p_t(x) = -\\nabla!\\cdot\\big(f(x,t)p_t(x)\\big) + \\frac12 g(t)^2\\,\\Delta p_t(x). $$ In other words, the SDE defines a stochastic flow of measures \\((\\mu_t)\\), and Fokker\u2013Planck is its Eulerian \u201cmass conservation + diffusion\u201d statement. (A step-by-step derivation is in Mathematics Foundation, Appendix B.)</p> <p>Reverse-time generation is another stochastic flow. To move probability mass from a simple \\(\\mu_1\\) back to \\(\\mu_0\\), we run a reverse-time dynamics whose drift depends on the score \\(\\nabla_x\\log p_t\\). This is the origin of \u201clearn the score/velocity field, then integrate backward\u201d.</p>"},{"location":"posts/diffusion-models-intro.html#42-the-vp-diffusion-as-a-canonical-forward-sde-a-continuous-scheduler","title":"4.2 The VP diffusion as a canonical forward SDE (a continuous scheduler)","text":"<p>A widely used forward process is the variance-preserving (VP) SDE, parameterized by a scalar noise-rate schedule \\(\\beta(t)\\ge 0\\): $$ dX_t = -\\frac12\\beta(t)\\,X_t\\,dt + \\sqrt{\\beta(t)}\\,dW_t. $$ It is a linear SDE, and (conditionally on \\(X_0=x_0\\)) its marginal is Gaussian: $$ X_t = \\alpha(t)\\,x_0 + \\sigma(t)\\,\\varepsilon,\\qquad \\varepsilon\\sim\\mathcal{N}(0,I), $$ for explicit \\(\\alpha(t)\\in(0,1]\\) and \\(\\sigma(t)\\ge 0\\) determined by \\(\\beta(t)\\). One common convention is $$ \\alpha(t)=\\exp!\\Big(-\\frac12\\int_0^t \\beta(s)\\,ds\\Big), \\qquad \\sigma(t)^2 = 1-\\alpha(t)^2, $$ so that \\(X_t\\mid X_0=x_0\\sim \\mathcal{N}(\\alpha(t)x_0,\\sigma(t)^2 I)\\). This is the continuous-time analog of the discrete identity \\(x_t=\\sqrt{\\bar\\alpha_t}x_0+\\sqrt{1-\\bar\\alpha_t}\\varepsilon\\) from Chapter 3.</p>"},{"location":"posts/diffusion-models-intro.html#43-ddpm-as-a-time-discretization-of-a-stochastic-flow","title":"4.3 DDPM as a time discretization of a stochastic flow","text":"<p>DDPM\u2019s forward Markov chain $$ q(x_t\\mid x_{t-1})=\\mathcal{N}!\\big(\\sqrt{1-\\beta_t}\\,x_{t-1},\\;\\beta_t I\\big) $$ can be viewed as a time discretization of a VP-type SDE (with a suitable mapping between \\(\\{\\beta_t\\}\\) and \\(\\beta(t)\\)). From the flow viewpoint:</p> <ul> <li>the schedule \\(\\{\\beta_t\\}\\) (or \\(\\beta(t)\\)) specifies how the stochastic flow gradually destroys information,</li> <li>the model learns a reverse-time flow that restores structure from noise.</li> </ul>"},{"location":"posts/diffusion-models-intro.html#44-elbo-for-ddpm-likelihood-as-stepwise-flow-matching-across-time","title":"4.4 ELBO for DDPM: likelihood as stepwise flow-matching across time","text":"<p>Let the learned reverse process be $$ p_\\theta(x_{0:T}) = p(x_T)\\prod_{t=1}^T p_\\theta(x_{t-1}\\mid x_t), \\qquad p(x_T)=\\mathcal{N}(0,I). $$ Using the forward chain \\(q(x_{0:T})=q(x_0)\\prod_{t=1}^T q(x_t\\mid x_{t-1})\\) as a variational distribution over latents \\(x_{1:T}\\), we get a variational lower bound: $$ \\log p_\\theta(x_0) \\ge \\mathbb{E}{q(x{1:T}\\mid x_0)}\\big[\\log p_\\theta(x_{0:T})-\\log q(x_{1:T}\\mid x_0)\\big]. $$ After algebra, this ELBO decomposes into KL terms at each step: $$ \\begin{aligned} \\log p_\\theta(x_0)\\;\\ge\\;&amp; \\mathbb{E}q\\big[\\log p\\theta(x_0\\mid x_1)\\big] -\\mathrm{KL}\\big(q(x_T\\mid x_0)\\,|\\,p(x_T)\\big) \\ &amp;-\\sum_{t=2}^T \\mathbb{E}q\\Big[ \\mathrm{KL}\\big(q(x{t-1}\\mid x_t,x_0)\\,|\\,p_\\theta(x_{t-1}\\mid x_t)\\big) \\Big]. \\end{aligned} $$ Here \\(q(x_{t-1}\\mid x_t,x_0)\\) is the true reverse posterior induced by the forward Gaussian chain; it has a closed-form Gaussian expression, so these KL terms are tractable once \\(p_\\theta(x_{t-1}\\mid x_t)\\) is Gaussian.</p>"},{"location":"posts/diffusion-models-intro.html#45-the-usual-noise-prediction-loss-as-a-special-case-of-the-elbo","title":"4.5 The usual noise-prediction loss as a special case of the ELBO","text":"<p>In the common DDPM parameterization, one chooses \\(p_\\theta(x_{t-1}\\mid x_t)\\) to be Gaussian with a variance schedule fixed in advance, and a mean that is expressed via a network \\(\\varepsilon_\\theta(x_t,t)\\) (or equivalently \\(x_{0,\\theta}(x_t,t)\\)). Under this choice, each KL term above reduces (up to an additive constant and a time-dependent weight) to an \\(\\ell_2\\) regression loss: $$ \\mathbb{E}{t,x_0,\\varepsilon}\\big[w(t)\\,|\\varepsilon-\\varepsilon\\theta(x_t,t)|^2\\big]. $$ So, on the flow axis, the \u201cdenoising MSE\u201d objective is one particular way of fitting a reverse-time stochastic flow so that it matches the forward flow\u2019s stepwise posteriors.</p> <p>Remark: the exact weight \\(w(t)\\) depends on the schedule \\(\\beta_t\\) and the variance parameterization in \\(p_\\theta(x_{t-1}\\mid x_t)\\); different \u201cpredict \\(\\varepsilon\\)\u201d vs \u201cpredict \\(x_0\\)\u201d parameterizations correspond to the same ELBO written in different coordinates.</p>"},{"location":"posts/diffusion-models-intro.html#chapter-5-flow-vs-diffusion-what-is-fundamentally-different","title":"Chapter 5 \u2014 Flow vs diffusion: what is fundamentally different?","text":"<p>Both flow models and diffusion models are \u201cdynamics-based\u201d generative models: they define a time-indexed family of random variables \\((X_t)\\) and use a learned field to transform a simple distribution into a complicated one. The crucial differences are in the type of dynamics, the mathematical object that evolves, and the training signal that is tractable.</p>"},{"location":"posts/diffusion-models-intro.html#51-deterministic-ode-flows-vs-stochastic-sde-flows","title":"5.1 Deterministic ODE flows vs stochastic SDE flows","text":"<ul> <li> <p>Flow / CNF (ODE). A continuous normalizing flow evolves particles deterministically: $$ \\frac{d}{dt}X_t=v(t,X_t;\\theta). $$ Under standard regularity (e.g., Lipschitz in \\(x\\)), the solution is unique and defines a (locally) invertible flow map \\(\\phi_{s\\to t}\\). Distribution evolution is push-forward: \\(\\mu_t=(\\phi_{0\\to t})_\\#\\mu_0\\).</p> </li> <li> <p>Diffusion (SDE). A diffusion model starts from a stochastic forward process: $$ dX_t=f(X_t,t)\\,dt+g(t)\\,dW_t. $$ Even with \\(X_0\\) fixed, \\(X_t\\) remains random because of Brownian noise. The evolution is therefore not a bijection \\(x_0\\mapsto x_t\\), but a Markov transition kernel \\(P_{s\\to t}(x,\\cdot)\\) acting on measures.</p> </li> </ul>"},{"location":"posts/diffusion-models-intro.html#52-eulerian-density-evolution-continuity-equation-vs-fokkerplanck","title":"5.2 Eulerian density evolution: continuity equation vs Fokker\u2013Planck","text":"<ul> <li> <p>ODE \\(\\Rightarrow\\) continuity equation (CE). If \\(X_t\\) follows an ODE and admits a density \\(p_t\\), then $$ \\partial_t p_t(x)+\\nabla!\\cdot\\big(p_t(x)\\,v(t,x)\\big)=0. $$ This is pure transport: probability mass is conserved and advected by \\(v\\).</p> </li> <li> <p>SDE \\(\\Rightarrow\\) Fokker\u2013Planck. If \\(X_t\\) follows an It\u00f4 SDE and admits a density, then $$ \\partial_t p_t(x)=-\\nabla!\\cdot\\big(f(x,t)p_t(x)\\big)+\\frac12 g(t)^2\\,\\Delta p_t(x), $$ which adds a second-order diffusion term. This difference is not cosmetic: the diffusion term reflects the fact that noise spreads mass and destroys invertibility.</p> </li> </ul>"},{"location":"posts/diffusion-models-intro.html#53-what-is-designed-and-what-is-learned","title":"5.3 What is \u201cdesigned\u201d and what is \u201clearned\u201d","text":"<ul> <li> <p>Diffusion: the forward noising mechanism (equivalently \\((f,g)\\) or a discrete schedule) is designed so that we can generate samples \\(x_t\\sim p_t\\) from data \\(x_0\\sim q\\). The learned object is typically a score model \\(s_\\theta(x,t)\\approx \\nabla_x\\log p_t(x)\\) (or an equivalent parameterization such as \\(\\varepsilon_\\theta\\)).</p> </li> <li> <p>Flow / CNF: the learned object is directly a velocity field \\(v(t,x;\\theta)\\). A likelihood formula exists in principle via \\(\\frac{d}{dt}\\log p_t(X_t)=-\\nabla\\!\\cdot v(t,X_t;\\theta)\\), but it can be computationally heavy in high dimension.</p> </li> <li> <p>Flow matching: sits between the two viewpoints. It also learns an ODE velocity field \\(v(t,x;\\theta)\\), but it trains it using an Eulerian \u201cmatch a path\u201d idea (via CE), using conditional constructions to avoid intractable marginal objects.</p> </li> </ul>"},{"location":"posts/diffusion-models-intro.html#54-practical-consequence-why-training-looks-different","title":"5.4 Practical consequence: why training looks different","text":"<ul> <li> <p>In marginal flow matching, the marginal target \\(u_t(x)\\) is a posterior average and typically involves normalization by \\(p_t(x)\\), making it hard to compute with only sample access to \\(q\\). Conditional flow matching (CFM) resolves this by training on per-sample pairs \\((x,u_t(x\\mid x_1))\\) with \\(x_1\\sim q\\).</p> </li> <li> <p>In diffusion, we do not need to evaluate \\(p_t(x)\\) to generate training inputs: by design, the forward corruption mechanism provides unbiased samples \\(x_t\\sim p_t\\) together with supervision signals for denoising/score matching.</p> </li> </ul>"},{"location":"posts/diffusion-models-intro.html#55-relation-not-a-contradiction-probability-flow-ode","title":"5.5 Relation (not a contradiction): probability flow ODE","text":"<p>Although diffusion is fundamentally stochastic, the same marginal path \\((p_t)\\) can often be realized by a deterministic ODE (the probability flow ODE) $$ \\frac{d}{dt}X_t=f(X_t,t)-\\frac12 g(t)^2\\,\\nabla_x\\log p_t(X_t), $$ which shares the same one-time marginals as the SDE when the score is exact. This provides a conceptual bridge: diffusion models can be sampled either stochastically (reverse SDE) or deterministically (PF-ODE), once \\(\\nabla_x\\log p_t\\) is learned.</p>"},{"location":"posts/diffusion-models-intro.html#references","title":"References","text":"<ol> <li>Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud. Neural Ordinary Differential Equations. NeurIPS (2018). arXiv:1806.07366 (DOI: <code>https://doi.org/10.48550/arXiv.1806.07366</code>). <code>https://arxiv.org/abs/1806.07366</code></li> <li>Flow Matching (blog post). Cambridge Machine Learning Group (Jan 20, 2024). <code>https://mlg.eng.cam.ac.uk/blog/2024/01/20/flow-matching.html#mjx-eqn%3Aeq%3Ag2g</code></li> <li>Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, Matt Le. Flow Matching for Generative Modeling. arXiv:2210.02747 (DOI: <code>https://doi.org/10.48550/arXiv.2210.02747</code>). <code>https://arxiv.org/abs/2210.02747</code></li> <li>Lagrangian and Eulerian specification of the flow field. Wikipedia. <code>https://en.wikipedia.org/wiki/Lagrangian_and_Eulerian_specification_of_the_flow_field</code></li> <li>Yaron Lipman, Marton Havasi, Peter Holderrieth, Neta Shaul, Matt Le, Brian Karrer, Ricky T. Q. Chen, David Lopez-Paz, Heli Ben-Hamu, Itai Gat. Flow Matching Guide and Code. arXiv:2412.06264 (DOI: <code>https://doi.org/10.48550/arXiv.2412.06264</code>). <code>https://arxiv.org/abs/2412.06264</code></li> <li>Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli. Deep Unsupervised Learning using Nonequilibrium Thermodynamics. ICML (2015). arXiv:1503.03585 (DOI: <code>https://doi.org/10.48550/arXiv.1503.03585</code>). <code>https://arxiv.org/abs/1503.03585</code></li> <li>Jonathan Ho, Ajay Jain, Pieter Abbeel. Denoising Diffusion Probabilistic Models. NeurIPS (2020). arXiv:2006.11239 (DOI: <code>https://doi.org/10.48550/arXiv.2006.11239</code>). <code>https://arxiv.org/abs/2006.11239</code></li> <li>Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equations. ICLR (2021). arXiv:2011.13456 (DOI: <code>https://doi.org/10.48550/arXiv.2011.13456</code>). <code>https://arxiv.org/abs/2011.13456</code></li> </ol>"},{"location":"posts/transformer.html","title":"Transformer","text":"<ul> <li>Paper: https://arxiv.org/abs/1706.03762 (Vaswani et al., 2017, Attention Is All You Need)</li> <li>Original code (Tensor2Tensor): https://github.com/tensorflow/tensor2tensor</li> </ul> <p>Scope note: we focus on techniques relevant to modern LLMs (primarily decoder-only / autoregressive Transformers); encoder-only architectures (e.g., BERT) and purely supervised encoder models are out of scope here.</p>"},{"location":"posts/transformer.html#what-is-the-model-mathematical-representation","title":"What is the model? (Mathematical representation)","text":"<p>In standard NLP usage, you plug the Transformer into a familiar likelihood:</p>"},{"location":"posts/transformer.html#conditional-sequence-modeling-seq2seq-translation","title":"Conditional sequence modeling (seq2seq / translation)","text":"<ul> <li>Definition (seq2seq): https://en.wikipedia.org/wiki/Seq2seq</li> </ul> <p>Given a source sequence \\(x\\) and a target sequence \\(y = (y_1, \\ldots, y_T)\\), we model</p> \\[ p_\\theta(y \\mid x) = \\prod_{t=1}^{T} p_\\theta\\bigl(y_t \\mid y_{&lt;t}, x\\bigr) \\] <p>Training is maximum likelihood (cross-entropy) with teacher forcing:</p> \\[ \u03b8^* \\,=\\, \\arg\\max_\\theta \\prod_{(x,y)\\in\\mathcal{D}} p_\\theta(y\\mid x) \\,=\\, \\arg\\max_\\theta \\sum_{(x,y)\\in\\mathcal{D}} \\log p_\\theta(y\\mid x) \\,=\\, \\arg\\max_\\theta \\sum_{(x,y)\\in\\mathcal{D}} \\sum_{t=1}^{T} \\log p_\\theta\\bigl(y_t \\mid y_{&lt;t}, x\\bigr) \\]"},{"location":"posts/transformer.html#autoregressive-language-modeling-decoder-only","title":"Autoregressive language modeling (decoder-only)","text":"<p>If there is no conditioning input \\(x\\), the same factorization becomes</p> \\[ p_\\theta(y) = \\prod_{t=1}^{T} p_\\theta\\bigl(y_t \\mid y_{&lt;t}\\bigr) \\] <p>This is the \u201cAR\u201d view: the Transformer block is just a parameterization of the conditional distributions via masked self-attention.</p>"},{"location":"posts/transformer.html#discussion","title":"Discussion","text":"<ul> <li>No explicit latent probabilistic state: unlike classical sequence models such as HMMs or state-space/Markov processes, the standard Transformer LM does not introduce a stochastic hidden state \\(z_t\\) with an explicit transition \\(p(z_t\\mid z_{t-1})\\). The \u201chidden states\u201d in a Transformer are deterministic neural representations produced by forward computation, so there is no marginalization/inference over latent variables in the probabilistic formulation.</li> <li>Truncation is usually practical, not theoretical: the chain-rule factorization conditions on the entire prefix \\(y_{&lt;t}\\) in principle. In practice, models operate under a finite context window (and sometimes additional approximations such as sparse/blocked attention or external memory), which effectively limits what information the model can condition on.</li> <li>Decoding choices: implementations often expose top-\\(k\\) (or top-\\(p\\)) sampling, applied at the final next-token distribution (logits/softmax); this usually does not reduce the forward-pass compute per step, but can simplify the sampling step by restricting to a subset of the vocabulary.</li> <li>Beam search: an alternative decoding procedure that keeps multiple partial hypotheses and selects the highest-scoring sequence under the model, rather than sampling a single next token.</li> <li>Why self-attention matters next: self-attention is the key computation that constructs a context-dependent representation of the prefix, which is then used to parameterize each conditional distribution \\(p_\\theta(y_t\\mid \\cdot)\\).</li> </ul>"},{"location":"posts/transformer.html#attention-self-attention-multi-head","title":"Attention / Self-Attention / Multi-Head","text":""},{"location":"posts/transformer.html#scaled-dot-product-attention","title":"Scaled dot-product attention","text":"<p>Attention takes queries \\(Q\\), keys \\(K\\), values \\(V\\), and returns a weighted sum of values. The weights are content-dependent similarities between queries and keys.</p> \\[ \\mathrm{Attn}(Q,K,V;M) \\,=\\, \\mathrm{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}} + M\\right) V \\] <ul> <li>\\(d_k\\) is the key/query dimension per head.</li> <li>\\(M\\) is an additive mask (typically \\(0\\) for allowed positions and \\(-\\infty\\) for disallowed ones). For decoder-only LLMs, \\(M\\) encodes the causal constraint \u201cdo not attend to the future\u201d.</li> </ul> <p></p>"},{"location":"posts/transformer.html#self-attention-vs-cross-attention","title":"Self-attention vs cross-attention","text":"<ul> <li>Self-attention: \\(Q,K,V\\) are all computed from the same sequence representation \\(X\\).</li> <li>Cross-attention (encoder-decoder): \\(Q\\) comes from the decoder states, while \\(K,V\\) come from the encoder outputs.</li> </ul> <p>In practice you form \\(Q,K,V\\) by learned linear projections, e.g. \\(Q = XW_Q\\), \\(K = XW_K\\), \\(V = XW_V\\).</p>"},{"location":"posts/transformer.html#why-the-1sqrtd_k-scale","title":"Why the \\(1/\\sqrt{d_k}\\) scale?","text":"<p>Without scaling, the dot products \\(QK^\\top\\) tend to grow with \\(d_k\\), pushing softmax into saturation and making optimization harder. The scale roughly stabilizes the magnitude.</p> <p>A common (approximate) variance argument is:</p> \\[ s \\,=\\, q^\\top k \\,=\\, \\sum_{i=1}^{d_k} q_i k_i \\] <p>Assume components are roughly zero-mean with unit variance and weakly correlated, e.g. \\(q_i, k_i\\) behave like i.i.d. with \\(\\mathbb{E}[q_i]=\\mathbb{E}[k_i]=0\\), \\(\\mathrm{Var}(q_i)=\\mathrm{Var}(k_i)=1\\). Then \\(\\mathbb{E}[q_i k_i]=0\\) and</p> \\[ \\mathrm{Var}(s) \\approx \\sum_{i=1}^{d_k} \\mathrm{Var}(q_i k_i) \\approx d_k. \\] <p>So \\(s\\) has typical scale \\(\\sqrt{d_k}\\). Dividing by \\(\\sqrt{d_k}\\) makes the logits fed to softmax stay \\(\\mathcal{O}(1)\\):</p> \\[ \\frac{s}{\\sqrt{d_k}} \\text{ has } \\mathrm{Var}\\!\\left(\\frac{s}{\\sqrt{d_k}}\\right) \\approx 1. \\] <p>This is not a fully rigorous derivation (the learned projections and LayerNorm change the exact statistics), but it explains why scaling helps: it reduces softmax saturation and keeps gradients healthier.</p> <p>Concrete example: if \\(d_k=64\\), the unscaled dot-product has a typical magnitude around \\(\\sqrt{64}=8\\). Softmax over numbers with magnitude \\(\\sim 8\\) can become very peaky early in training; scaling brings those logits back to a gentler range.</p>"},{"location":"posts/transformer.html#how-to-kill-the-sqrtd_k-where-to-put-the-scale","title":"How to \u201ckill\u201d the \\(\\sqrt{d_k}\\) (where to put the scale)","text":"<p>You can\u2019t really make the \\(\\sqrt{d_k}\\) issue disappear; you can only move the scale so the attention logits stay \\(\\mathcal{O}(1)\\). The standard choice is the explicit \\(1/\\sqrt{d_k}\\), but there are equivalent alternatives:</p> <ul> <li> <p>Use an explicit (possibly learnable) temperature: \\(\\mathrm{softmax}((QK^T)\\cdot \\alpha + M)V\\), where \\(\\alpha\\) is a scalar (or per-head scalar). Initialize \\(\\alpha=1/\\sqrt{d_k}\\). Some implementations keep \\(\\alpha\\) fixed; others learn it (a learnable \u201ctemperature\u201d \\(\u03c4\\)).</p> </li> <li> <p>Normalize queries/keys (cosine attention): replace \\(q,k\\) with \\(\\hat q=q/\\|q\\|\\) and \\(\\hat k=k/\\|k\\|\\). Then \\(\\hat q\\cdot \\hat k\\in[-1,1]\\), so logits no longer grow with \\(d_k\\). In practice you often re-introduce a learnable scale \\(g\\) (temperature) because \\([-1,1]\\) may be too \u201ccold\u201d for sharp attention.</p> </li> <li> <p>Bake the scale into the projections: if the components of \\(q\\) and \\(k\\) have variance \\(\\sigma^2\\), then \\(\\mathrm{Var}(q\\cdot k) \\approx d_k\\,\\sigma^4\\). Picking \\(\\sigma^2\\approx 1/\\sqrt{d_k}\\) (i.e., std \\(\\approx d_k^{-1/4}\\) for both \\(q\\) and \\(k\\)) keeps \\(q\\cdot k\\) at \\(\\mathcal{O}(1)\\) even without dividing by \\(\\sqrt{d_k}\\). This is equivalent to putting a factor \\(d_k^{-1/4}\\) into both \\(W_Q\\) and \\(W_K\\).</p> </li> </ul> <p>Important nuance: LayerNorm/RMSNorm stabilizes the residual stream scale, but it does not automatically control the statistics of the attention logits in a way that removes the need for a temperature/scale.</p>"},{"location":"posts/transformer.html#does-this-relate-to-temperature-or-top-k","title":"Does this relate to temperature or top-\\(k\\)?","text":"<ul> <li>Temperature: you can view the attention softmax as having a fixed \u201ctemperature\u201d \\(\\tau\\), since \\(\\mathrm{softmax}(s/\\sqrt{d_k})\\) is equivalent to \\(\\mathrm{softmax}(s/\\tau)\\) with \\(\\tau=\\sqrt{d_k}\\). This is separate from decoding temperature, which is applied to the final vocabulary logits for next-token sampling.</li> <li>Top-\\(k\\): top-\\(k\\) sampling is a decoding-time heuristic applied to the vocabulary distribution (keep the top \\(k\\) tokens, renormalize). Standard attention does not use top-\\(k\\) over positions; it computes a full distribution over allowed positions. (There are research/engineering variants like sparse/top-\\(k\\) attention, but that\u2019s a different idea.)</li> </ul>"},{"location":"posts/transformer.html#multi-head-attention-mha","title":"Multi-head attention (MHA)","text":"<p>Multi-head attention runs several attention \u201cheads\u201d in parallel with different learned projections, then concatenates their outputs:</p> \\[ \\mathrm{MHA}(X) \\,=\\, \\mathrm{Concat}\\bigl(\\mathrm{Attn}(Q_1,K_1,V_1;M),\\ldots,\\mathrm{Attn}(Q_h,K_h,V_h;M)\\bigr) W_O \\] <p>Intuition: different heads can specialize (local vs global dependencies, syntax vs coreference, etc.).</p> <p></p>"},{"location":"posts/transformer.html#minimal-pytorch-code-self-attention","title":"Minimal PyTorch code (self-attention)","text":"<pre><code>import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SimpleMHA(nn.Module):\n    \"\"\"Minimal multi-head self-attention with fused QKV projection.\"\"\"\n\n    def __init__(self, d_model, n_heads, dropout=0.0, bias=True):\n        super().__init__()\n        assert d_model % n_heads == 0\n        self.n_heads = n_heads\n        self.d_head = d_model // n_heads\n        self.qkv = nn.Linear(d_model, 3 * d_model, bias=bias)\n        self.proj = nn.Linear(d_model, d_model, bias=bias)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, causal=False):\n        # x: (B, T, C)\n        B, T, C = x.shape\n        qkv = self.qkv(x)  # (B, T, 3C)\n        q, k, v = qkv.chunk(3, dim=-1)\n\n        # (B, nh, T, hs)\n        q = q.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n        k = k.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n        v = v.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n\n        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_head)  # (B, nh, T, T)\n        if causal:\n            mask = torch.tril(torch.ones(T, T, device=x.device, dtype=torch.bool))\n            att = att.masked_fill(~mask, float(\"-inf\"))\n\n        att = F.softmax(att, dim=-1)\n        att = self.dropout(att)\n        y = att @ v  # (B, nh, T, hs)\n\n        y = y.transpose(1, 2).contiguous().view(B, T, C)  # merge heads\n        return self.proj(y)\n\n\n# Example:\n# x = torch.randn(2, 128, 768)\n# y = SimpleMHA(768, 12)(x, causal=True)\n</code></pre>"},{"location":"posts/transformer.html#notes-on-compute","title":"Notes on compute","text":"<ul> <li>The attention matrix is \\(T\\times T\\) for sequence length \\(T\\), so naive attention costs \\(O(T^2)\\) memory/time per layer.</li> <li>During autoregressive decoding, KV cache avoids recomputing past \\(K,V\\) and makes each new token step incremental.</li> </ul>"},{"location":"posts/transformer.html#discussion-compute","title":"Discussion (Compute)","text":"<ul> <li>Compute reality check (what actually gets multiplied): for a decoder-only Transformer layer with batch \\(B\\),   sequence length \\(T\\), model width \\(d_{model}\\), and \\(h\\) heads (so \\(d_{head}=d_{model}/h\\)), the main   multiplications are:</li> <li>QKV projection: \\(X\\in\\mathbb{R}^{B\\times T\\times d_{model}}\\) times \\(W_{QKV}\\in\\mathbb{R}^{d_{model}\\times 3d_{model}}\\) \\(\\Rightarrow\\) about \\(3\\,B\\,T\\,d_{model}^2\\) scalar multiplies.</li> <li>Attention scores: \\(QK^\\top\\) per head is \\((T\\times d_{head})\\cdot(d_{head}\\times T)\\Rightarrow (T\\times T)\\), so across all     heads it is about \\(B\\,h\\,T^2\\,d_{head}=B\\,T^2\\,d_{model}\\) multiplies.<ul> <li>What \u201ceach head computes\u201d concretely:<ul> <li>For a fixed head \\(i\\), each score entry between a query position \\(t\\) and a key position \\(s\\) is a dot product between two     \\(d_{head}\\)-dimensional vectors: \\(q_t^{(i)}\\cdot k_s^{(i)}\\). Computing one score uses \\(d_{head}\\) scalar multiplications and     \\(d_{head}-1\\) scalar additions (sum-reduction).</li> <li>The softmax turns the \\(T\\) scores for a fixed query position \\(t\\) into a weight vector \\(\\alpha_{t,1:T}^{(i)}\\) over positions.</li> <li>The head output at position \\(t\\) is a weighted sum over the value vectors \\(v_{1:T}^{(i)}\\): it multiplies each \\(v_s^{(i)}\\) by     \\(\\alpha_{t,s}^{(i)}\\) and sums over \\(s\\). This is exactly the matrix multiply \\(\\alpha^{(i)}V^{(i)}\\) (often written as     \\(\\mathrm{Attn}\\cdot V\\)).</li> </ul> </li> </ul> </li> <li>Weighted sum: \\(\\mathrm{Attn}\\cdot V\\) has the same order as scores, another \\(B\\,T^2\\,d_{model}\\) multiplies.</li> <li>Output projection: concatenated heads \\((B\\times T\\times d_{model})\\) times \\(W_O\\in\\mathbb{R}^{d_{model}\\times d_{model}}\\) \\(\\Rightarrow\\) about \\(B\\,T\\,d_{model}^2\\) multiplies.</li> <li>Takeaway: splitting into heads does not automatically reduce total compute at fixed \\(d_{model}\\); it mostly changes     how compute is structured/parallelized. Naively, the attention matrix itself has \\(B\\,h\\,T^2\\) elements, which is why     attention is often described as \\(O(T^2)\\) in time/memory (unless using optimized kernels like FlashAttention that avoid     materializing the full \\(T\\times T\\) matrix).</li> </ul>"},{"location":"posts/transformer.html#normalization-layernorm-rmsnorm","title":"Normalization (LayerNorm / RMSNorm)","text":"<p>Transformers rely heavily on normalization for stable optimization at depth.</p>"},{"location":"posts/transformer.html#layernorm-ln","title":"LayerNorm (LN)","text":"<p>LayerNorm normalizes each token\u2019s feature vector across the model dimension. For a vector \\(x\\in\\mathbb{R}^{d_{model}}\\) (one token, one layer):</p> \\[ \\mathrm{LN}(x) = \\gamma\\odot \\frac{x-\\mu}{\\sqrt{\\sigma^2+\\epsilon}} + \\beta, \\qquad \\mu=\\frac{1}{d_{model}}\\sum_j x_j,\\ \\ \\sigma^2=\\frac{1}{d_{model}}\\sum_j (x_j-\\mu)^2. \\] <p>Key properties:</p> <ul> <li>It is per-token (no dependence on batch size), so it works well for variable-length sequences and small batches.</li> <li>The learned \\(\\gamma,\\beta\\) let the network recover useful scales after normalization.</li> </ul>"},{"location":"posts/transformer.html#pre-ln-vs-post-ln-where-ln-sits","title":"Pre-LN vs Post-LN (where LN sits)","text":"<ul> <li>Post-LN (original 2017 diagram): \\(y=\\mathrm{LN}(x+\\mathrm{Sublayer}(x))\\).</li> <li>Pre-LN (common in modern LLMs): \\(y=x+\\mathrm{Sublayer}(\\mathrm{LN}(x))\\).</li> </ul> <p>Pre-LN typically trains more reliably for very deep models because gradients can flow through the residual path more directly.</p>"},{"location":"posts/transformer.html#rmsnorm-common-modern-variant","title":"RMSNorm (common modern variant)","text":"<p>RMSNorm removes the mean-subtraction and normalizes by root-mean-square only (cheaper, often similar in practice):</p> \\[ \\mathrm{RMSNorm}(x) = \\gamma\\odot \\frac{x}{\\sqrt{\\frac{1}{d_{model}}\\sum_j x_j^2 + \\epsilon}}. \\] <p>Relation to the \\(1/\\sqrt{d_k}\\) attention scale: LN/RMSNorm helps keep activations in a sane range across layers, but attention still needs an explicit or implicit temperature to prevent dot-product logits from becoming too large as dimensions change.</p>"},{"location":"posts/transformer.html#positional-encoding-embeddings","title":"Positional encoding / embeddings","text":"<p>Self-attention alone is permutation-invariant: without position information, the model cannot distinguish sequences that are just token-permutations. In Transformers, position is typically injected by adding a position-dependent vector to the token embedding at the input of each layer (or at least the first layer).</p>"},{"location":"posts/transformer.html#original-sinusoidal-positional-encoding-vaswani-et-al-2017","title":"Original sinusoidal positional encoding (Vaswani et al., 2017)","text":"<p>The paper uses a fixed (non-learned) sinusoidal encoding with dimension \\(d_{model}\\). For position \\(pos\\) and dimension index \\(i\\in\\{0,\\ldots, d_{model}/2-1\\}\\):</p> \\[ \\mathrm{PE}(pos, 2i) = \\sin\\bigl(pos / 10000^{2i/d_{model}}\\bigr) \\] \\[ \\mathrm{PE}(pos, 2i+1) = \\cos\\bigl(pos / 10000^{2i/d_{model}}\\bigr) \\] <p>Then the model uses \\(X_{in}(pos) = \\mathrm{Embed}(token) + \\mathrm{PE}(pos)\\).</p> <p>Practical note: many modern LLMs use learned absolute positional embeddings, relative position biases, or RoPE; the sinusoidal version is still useful as a clean reference and can generalize to longer lengths than seen in training.</p>"},{"location":"posts/transformer.html#angle-phase-interpretation-why-sincos-helps","title":"\u201cAngle / phase\u201d interpretation (why sin/cos helps)","text":"<p>Define a frequency per pair \\(i\\):</p> \\[ \\omega_i = 10000^{-2i/d_{model}}. \\] <p>Each 2D pair \\((\\mathrm{PE}_{2i}, \\mathrm{PE}_{2i+1})\\) is a unit-circle point at angle \\(\\omega_i\\,pos\\):</p> \\[ \\bigl[\\sin(\\omega_i\\,pos),\\ \\cos(\\omega_i\\,pos)\\bigr]. \\] <p>So increasing the position by \\(\\Delta\\) corresponds to a rotation by angle \\(\\omega_i\\,\\Delta\\) in that 2D plane.</p> <p>Two useful consequences:</p> <ul> <li>Relative offsets show up as phase differences: the dot product of the two unit vectors at positions \\(p\\) and \\(q\\) is   \\(\\cos(\\omega_i(p-q))\\). So a simple bilinear form can access functions of \\(p-q\\) (relative position), not just \\(p\\) itself.</li> <li>Shift structure is linear in \\(\\sin/\\cos\\): using trig identities,   \\(\\sin(\\omega(p+\\Delta))\\) and \\(\\cos(\\omega(p+\\Delta))\\) can be written as a fixed linear transform of   \\(\\sin(\\omega p)\\) and \\(\\cos(\\omega p)\\) (with coefficients depending only on \\(\\Delta\\)). This makes it easier for attention   layers to learn \u201crelative position\u201d patterns using linear projections.</li> </ul>"},{"location":"posts/transformer.html#rope-rotary-positional-embedding","title":"RoPE (Rotary Positional Embedding)","text":"<p>RoPE is a widely used alternative in modern decoder-only LLMs. Instead of adding a positional vector to the token embedding, it rotates the query/key vectors in 2D subspaces as a function of position. Intuitively, position becomes a phase that lives inside the attention dot product.</p> <p>Take one head and one position \\(pos\\). For each 2D pair of channels \\((2i, 2i+1)\\), define a position-dependent angle \\(\u03b8_i(pos)\\) (often using the same frequency schedule as sinusoidal PE):</p> \\[ \u03b8_i(pos) = pos\\cdot 10000^{-2i/d_{head}}. \\] <p>Apply a rotation to the \\(q\\) and \\(k\\) components in that 2D plane:</p> \\[ \\begin{bmatrix} q'_{2i} \\\\ q'_{2i+1} \\end{bmatrix} = \\begin{bmatrix} \\cos(\u03b8_i) &amp; -\\sin(\u03b8_i) \\\\ \\sin(\u03b8_i) &amp; \\cos(\u03b8_i) \\end{bmatrix} \\begin{bmatrix} q_{2i} \\\\ q_{2i+1} \\end{bmatrix}, \\qquad \\begin{bmatrix} k'_{2i} \\\\ k'_{2i+1} \\end{bmatrix} = \\begin{bmatrix} \\cos(\u03b8_i) &amp; -\\sin(\u03b8_i) \\\\ \\sin(\u03b8_i) &amp; \\cos(\u03b8_i) \\end{bmatrix} \\begin{bmatrix} k_{2i} \\\\ k_{2i+1} \\end{bmatrix}. \\] <p>Then attention uses \\(q'\\cdot k'\\) as usual.</p> <p>Why this helps:</p> <ul> <li>Relative-position behavior \u201cfor free\u201d: because rotations compose, the dot product between rotated vectors depends on the   angle difference, which is a function of \\((pos_q - pos_k)\\). This makes it natural for the model to learn relative-offset   patterns without explicit relative-position embeddings.</li> <li>Extrapolation: RoPE often behaves better than learned absolute embeddings when evaluating at longer context lengths.</li> </ul> <p>Practical note: RoPE is typically applied only to a prefix of head dimensions (or with variants like partial rotary / scaling) depending on the architecture.</p>"},{"location":"posts/transformer.html#causal-masking-and-generative-vs-discriminative-discussion","title":"Causal masking and \u201cgenerative vs discriminative\u201d discussion","text":""},{"location":"posts/transformer.html#what-is-the-causal-mask","title":"What is the causal mask?","text":"<p>In a decoder-only (autoregressive) Transformer, token \\(t\\) is only allowed to attend to positions \\(\\le t\\). This is enforced by an additive mask \\(M\\) inside attention:</p> \\[ \\mathrm{Attn}(Q,K,V;M) = \\mathrm{softmax}\\!\\left(\\frac{QK^T}{\\sqrt{d_k}} + M\\right)V. \\] <p>A standard causal mask is</p> \\[ M_{t,s} = \\begin{cases} 0, &amp; s\\le t\\\\ -\\infty, &amp; s&gt;t. \\end{cases} \\] <p>So future positions get \\(-\\infty\\) logits and therefore zero probability after softmax.</p> <p>Two practical notes:</p> <ul> <li>Training can still be parallel: even though the model is \u201cleft-to-right\u201d, we compute all \\(t\\) in one forward pass by masking.</li> <li>In generation, the same constraint is what makes incremental decoding valid: the next-token distribution depends only on the prefix.</li> </ul>"},{"location":"posts/transformer.html#why-does-causal-masking-make-it-a-generative-model","title":"Why does causal masking make it a generative model?","text":"<p>With the causal mask, the model parameterizes a proper left-to-right factorization of a joint distribution over sequences:</p> \\[ p_\\theta(y) = \\prod_{t=1}^{T} p_\\theta(y_t\\mid y_{&lt;t}). \\] <p>Because the conditionals are defined for every position \\(t\\), we can generate a sequence by sampling or taking argmax repeatedly: start from a BOS token, sample \\(y_1\\sim p(y_1)\\), then \\(y_2\\sim p(y_2\\mid y_1)\\), etc. (In practice we use decoding algorithms such as greedy, top-\\(k\\), top-\\(p\\), beam search.)</p> <p>This is what people usually mean by \u201cgenerative\u201d in the LLM context: the model defines a sequential process that produces the whole string.</p>"},{"location":"posts/transformer.html#softmax-does-not-decide-generative-vs-discriminative","title":"Softmax does not decide generative vs discriminative","text":"<p>Softmax is just a way to turn logits into a normalized categorical distribution.</p> <ul> <li>In an LM, softmax is over the vocabulary and represents \\(p_\\theta(y_t\\mid y_{&lt;t})\\) (a conditional distribution used to build a joint by the product rule).</li> <li>In a classifier, softmax is over labels and represents \\(p_\\theta(label\\mid x)\\) (a conditional distribution directly optimized for prediction).</li> </ul> <p>So \u201cusing softmax\u201d does not imply discriminative or generative; the training objective and factorization decide that.</p>"},{"location":"posts/transformer.html#where-does-bert-fit","title":"Where does BERT fit?","text":"<p>BERT uses bidirectional self-attention (no causal mask) and is pretrained with masked language modeling (MLM): predict masked tokens given the surrounding context on both sides.</p> <p>That objective learns strong representations for understanding tasks and is typically used in a discriminative setting (classification, tagging, retrieval): you feed \\(x\\) and predict labels \\(y\\), i.e. \\(p(y\\mid x)\\).</p> <p>MLM itself is a conditional / denoising objective and does not directly define a clean left-to-right joint \\(p(x)=\\prod_t p(x_t\\mid x_{&lt;t})\\), which is why BERT is not the default choice for straightforward next-token generation. (There are ways to sample from an MLM with iterative masking / Gibbs-style updates, but that is a different generation mechanism and usually less convenient than AR decoding.)</p>"},{"location":"posts/transformer.html#minimal-code-an-attention-block-with-residual-pathway","title":"Minimal code: an attention block (with residual pathway)","text":"<p>Below is a minimal \u201cTransformer block\u201d around self-attention (Pre-LN style). It reuses the <code>SimpleMHA</code> module defined earlier in this note.</p> <pre><code>import torch\nimport torch.nn as nn\n\n\nclass MLP(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.0):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_ff, d_model),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Pre-LN decoder block: x &lt;- x + Attn(LN(x)); x &lt;- x + MLP(LN(x)).\"\"\"\n\n    def __init__(self, d_model, n_heads, d_ff, dropout=0.0):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(d_model)\n        self.attn = SimpleMHA(d_model, n_heads, dropout=dropout)\n        self.ln2 = nn.LayerNorm(d_model)\n        self.mlp = MLP(d_model, d_ff, dropout=dropout)\n\n    def forward(self, x, causal=True):\n        # Residual (skip) pathway:\n        x = x + self.attn(self.ln1(x), causal=causal)\n        x = x + self.mlp(self.ln2(x))\n        return x\n\n\n# Example:\n# x = torch.randn(2, 128, 768)\n# block = TransformerBlock(d_model=768, n_heads=12, d_ff=3072, dropout=0.1)\n# y = block(x, causal=True)\n</code></pre>"},{"location":"posts/transformer.html#passway-the-resnet-style-skip-connection-in-this-context","title":"\u201cPassway\u201d = the ResNet-style skip connection (in this context)","text":"<p>The key line is <code>x = x + F(x)</code> (here, <code>F</code> is attention or MLP applied to a normalized input). That <code>+ x</code> term is exactly the ResNet skip/residual pathway:</p> <ul> <li>In ResNet you often write \\(x_{\\ell+1} = x_{\\ell} + F(x_{\\ell})\\).</li> <li>In a Pre-LN Transformer block you have \\(x_{\\ell+1} = x_{\\ell} + \\mathrm{Attn}(\\mathrm{LN}(x_{\\ell}))\\) (and then another residual for the MLP).</li> </ul> <p>This \u201cidentity passway\u201d makes optimization much easier: even if \\(F\\) starts near zero, information and gradients can flow through the identity path.</p>"},{"location":"posts/transformer.html#relation-to-odes-depth-as-time-intuition","title":"Relation to ODEs (depth-as-time intuition)","text":"<p>If you view layer index \\(\\ell\\) as a discrete \u201ctime\u201d step, then a residual update</p> \\[ x_{\\ell+1} = x_{\\ell} + h\\,f(x_{\\ell},\\ell) \\] <p>looks like a forward Euler discretization of an ODE \\(\\frac{dx}{dt} = f(x,t)\\). In this analogy:</p> <ul> <li>the residual branch (attention/MLP) is the vector field \\(f\\),</li> <li>the skip connection is the \\(+x_{\\ell}\\) term in Euler\u2019s method,</li> <li>and (sometimes) you can interpret hyperparameters / scaling as affecting the effective step size \\(h\\).</li> </ul> <p>In the Transformer context this is mainly an intuition for why residual networks train stably and how depth composes small updates; it is not required to treat the model as a true continuous-time system.</p>"},{"location":"posts/transformer.html#guide-how-far-this-is-from-gpt-3-level-detail-and-whats-next","title":"Guide: how far this is from GPT-3-level detail (and what\u2019s next)","text":"<p>This note covers the \u201ccore math blocks\u201d (AR factorization, attention, masks, normalization, positional methods, residual pathway), but it is still far from a full GPT-3-style training + systems picture. Below is a concrete checklist of what\u2019s missing mainly in knowledge details which we will talk about in the furute.</p> <p>What this article is still missing vs a GPT-3-level understanding:</p> <ul> <li>Exact architecture details used in modern LLMs: Pre-LN vs Post-LN variants in the wild, RMSNorm vs LayerNorm choices, MLP variants (GeLU vs SwiGLU), and common efficiency tweaks (MQA/GQA, grouped QK/V layouts).</li> <li>Tokenization and text normalization: BPE/unigram tokenizers, byte-level vs unicode handling, special tokens, and how tokenization choices affect context length and compute.</li> <li>Training objective details: next-token loss implementation details (label shifting, padding/masking), how perplexity is computed, and what is (and isn\u2019t) \u201cteacher forcing\u201d in practice.</li> <li>Optimization recipe: AdamW vs variants, weight decay placement, learning-rate schedules (warmup + cosine/linear), gradient clipping, dropout usage, and typical hyperparameter ranges.</li> <li>Initialization and stability tricks: residual scaling, logit scaling, norm/activation scaling, and what breaks at depth/width.</li> <li>Data pipeline: dataset composition, deduplication, filtering, mixing ratios, curriculum/order effects, and contamination issues.</li> <li>Evaluation + monitoring: held-out loss curves, downstream benchmarks, calibration, memorization tests, and how to interpret regressions.</li> <li>Systems/engineering: distributed training (data/tensor/pipeline parallelism), mixed precision, activation checkpointing, optimizer state sharding, throughput bottlenecks, and kernel-level attention optimizations.</li> </ul>"},{"location":"posts/transformer.html#references","title":"References","text":"<ul> <li> <p>Local note: vit-dit.md (ViT and DiT)</p> </li> <li> <p>Paper: https://arxiv.org/pdf/1409.3215 (Sutskever et al., 2014, Sequence to Sequence Learning with Neural Networks)</p> </li> <li>Course: https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html (Lena Voita, NLP Course: Seq2Seq and Attention)</li> <li>Tech report: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf (Radford et al., 2019, Language Models are Unsupervised Multitask Learners; GPT-2)</li> <li>Paper: https://arxiv.org/abs/2005.14165 (Brown et al., 2020, Language Models are Few-Shot Learners; GPT-3)</li> <li>Video: https://www.youtube.com/watch?v=kCc8FmEb1nY (Andrej Karpathy, Let's build GPT: from scratch, in code, spelled out.)</li> <li>Code: https://github.com/karpathy/minGPT (Andrej Karpathy, minGPT code)</li> <li>Code: https://github.com/karpathy/nanoGPT (Andrej Karpathy, nanoGPT code)</li> </ul>"},{"location":"posts/vit-dit.html","title":"ViT and DiT","text":""},{"location":"posts/vit-dit.html#quick-map","title":"Quick map","text":"<ul> <li>ViT (Vision Transformer): use the Transformer encoder (bidirectional self-attention; no causal mask) over a sequence of image patch tokens for classification and representation learning.</li> <li>DiT (Diffusion Transformer): (standard DiT) also uses an encoder-style Transformer backbone (bidirectional self-attention; no causal mask) inside a diffusion model to predict noise / velocity / score-like targets over latent/image tokens.</li> </ul> <p>If you\u2019re coming from the LLM note, see the self-attention recap in transformer.md.</p>"},{"location":"posts/vit-dit.html#vit-vision-transformer","title":"ViT (Vision Transformer)","text":""},{"location":"posts/vit-dit.html#core-idea","title":"Core idea","text":"<ul> <li>Tokenization: split an image into fixed-size patches (e.g., 16\u00d716), flatten each patch, and project to an embedding dimension.</li> <li>Add positional embeddings (learned or sinusoidal-like), then run a Transformer encoder.</li> <li>Use a special classification token <code>[CLS]</code> (original ViT) or pooled token features for classification.</li> </ul> <p>A common abstraction:</p> <ul> <li>Patches: \\(x \\in \\mathbb{R}^{H\\times W\\times C}\\) \u2192 \\(N\\) patches \u2192 \\(X \\in \\mathbb{R}^{N\\times d}\\)</li> <li>Encoder: \\(Z = \\mathrm{Encoder}(X + \\mathrm{PosEmbed})\\)</li> <li>Class head: \\(\\hat y = \\mathrm{head}(Z_{\\mathrm{cls}})\\) (if using <code>[CLS]</code>)</li> </ul> <p></p>"},{"location":"posts/vit-dit.html#what-to-pay-attention-to-implementation-details","title":"What to pay attention to (implementation details)","text":"<ul> <li>Patch embedding layer: typically <code>Conv2d(kernel=p, stride=p)</code> is equivalent to patchify + linear.</li> <li>Positional embeddings: learned absolute embeddings are common; resizing/interpolation is needed when changing resolution.</li> <li>Regularization and data: ViT often needs strong augmentation and large-scale pretraining to match CNN baselines.</li> </ul> <p>Minimal code sketch (patch embedding + tokens):</p> <pre><code>import torch\nimport torch.nn as nn\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"Image -&gt; patch tokens using a strided conv.\"\"\"\n\n    def __init__(self, in_chans=3, d_model=768, patch_size=16):\n        super().__init__()\n        self.patch_size = patch_size\n        self.proj = nn.Conv2d(in_chans, d_model, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        # x: (B, C, H, W) with H,W divisible by patch_size\n        x = self.proj(x)              # (B, d_model, H/p, W/p)\n        x = x.flatten(2).transpose(1, 2)  # (B, N, d_model)\n        return x\n\n\ndef make_vit_tokens(x, patch_embed, cls_token, pos_embed):\n    \"\"\"Returns token sequence ready for a Transformer encoder.\"\"\"\n    tok = patch_embed(x)  # (B, N, d)\n    B = tok.shape[0]\n    cls = cls_token.expand(B, -1, -1)  # (B, 1, d)\n    tok = torch.cat([cls, tok], dim=1)  # (B, 1+N, d)\n    tok = tok + pos_embed[:, : tok.shape[1]]\n    return tok\n\n\n# Example shapes (not a full model):\n# x = torch.randn(2, 3, 224, 224)\n# patch = PatchEmbed(in_chans=3, d_model=192, patch_size=16)\n# cls_token = nn.Parameter(torch.zeros(1, 1, 192))\n# pos_embed = nn.Parameter(torch.zeros(1, 1 + (224 // 16) ** 2, 192))\n# tokens = make_vit_tokens(x, patch, cls_token, pos_embed)\n</code></pre> <p>Minimal attention block (ViT encoder block):</p> <pre><code>import torch\nimport torch.nn as nn\n\n\nclass ViTEncoderBlock(nn.Module):\n    \"\"\"Pre-LN encoder block used in most ViT-style implementations.\"\"\"\n\n    def __init__(self, d_model, n_heads, d_ff, dropout=0.0):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(d_model)\n        self.attn = nn.MultiheadAttention(\n            embed_dim=d_model,\n            num_heads=n_heads,\n            dropout=dropout,\n            batch_first=True,  # x: (B, N, C)\n        )\n        self.ln2 = nn.LayerNorm(d_model)\n        self.mlp = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_ff, d_model),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        # x: (B, N, d_model)\n        # Residual pathway (same ResNet-style \"passway\"): x &lt;- x + F(x)\n        attn_out, _ = self.attn(self.ln1(x), self.ln1(x), self.ln1(x), need_weights=False)\n        x = x + attn_out\n        x = x + self.mlp(self.ln2(x))\n        return x\n</code></pre> <p>Minimal full ViT encoder skeleton (patch embed -&gt; blocks -&gt; head):</p> <pre><code>import torch\nimport torch.nn as nn\n\n\nclass ViT(nn.Module):\n    def __init__(\n        self,\n        image_size=224,\n        patch_size=16,\n        in_chans=3,\n        n_classes=1000,\n        d_model=768,\n        depth=12,\n        n_heads=12,\n        d_ff=3072,\n        dropout=0.0,\n    ):\n        super().__init__()\n        assert image_size % patch_size == 0\n        n_patches = (image_size // patch_size) ** 2\n\n        self.patch = PatchEmbed(in_chans=in_chans, d_model=d_model, patch_size=patch_size)\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + n_patches, d_model))\n        self.drop = nn.Dropout(dropout)\n\n        self.blocks = nn.ModuleList(\n            [ViTEncoderBlock(d_model, n_heads, d_ff, dropout=dropout) for _ in range(depth)]\n        )\n        self.norm = nn.LayerNorm(d_model)\n        self.head = nn.Linear(d_model, n_classes)\n\n    def forward(self, x):\n        # x: (B, C, H, W)\n        tok = make_vit_tokens(x, self.patch, self.cls_token, self.pos_embed)\n        tok = self.drop(tok)\n        for blk in self.blocks:\n            tok = blk(tok)\n        tok = self.norm(tok)\n        cls = tok[:, 0]  # [CLS]\n        return self.head(cls)\n\n\n# Example:\n# model = ViT(image_size=224, patch_size=16, d_model=192, depth=6, n_heads=3, d_ff=768)\n# x = torch.randn(2, 3, 224, 224)\n# logits = model(x)\n</code></pre>"},{"location":"posts/vit-dit.html#references-paper-code","title":"References (paper + code)","text":"<ul> <li>Paper: https://arxiv.org/abs/2010.11929 (Dosovitskiy et al., 2020, An Image is Worth 16x16 Words)</li> <li>Code (Google research): https://github.com/google-research/vision_transformer</li> <li>Code (PyTorch models / baseline impls): https://github.com/rwightman/pytorch-image-models (timm)</li> </ul>"},{"location":"posts/vit-dit.html#dit-diffusion-transformer","title":"DiT (Diffusion Transformer)","text":""},{"location":"posts/vit-dit.html#core-idea_1","title":"Core idea","text":"<p>Diffusion models learn to denoise a noisy sample \\(x_t\\) at time \\(t\\). A DiT replaces the usual U-Net backbone with a Transformer that operates on tokenized image/latent representations.</p> <p>Typical training target choices include (depending on formulation):</p> <ul> <li>Predict noise \\(\\epsilon\\)</li> <li>Predict velocity \\(v\\)</li> <li>Predict the clean sample \\(x_0\\)</li> </ul> <p>The Transformer is conditioned on the diffusion timestep \\(t\\) (and sometimes on class labels / text embeddings). Conditioning is often injected via adaptive normalization (FiLM/AdaLN-style) rather than plain concatenation.</p> <p></p>"},{"location":"posts/vit-dit.html#what-to-pay-attention-to-implementation-details_1","title":"What to pay attention to (implementation details)","text":"<ul> <li>Tokenization: operate in pixel space or (more common) in VAE latent space (Stable Diffusion-style).</li> <li>Timestep conditioning: how \\(t\\) embeds and modulates blocks (often via AdaLN).</li> <li>Attention pattern: full attention vs windowed/local attention for high-resolution.</li> <li>Objective choice: \\(\\epsilon\\) vs \\(v\\) parameterization and how it affects stability/sampling.</li> </ul> <p>Minimal code sketch (DDPM-specific parts: timestep + conditioning injection):</p> <pre><code>import math\nimport torch\nimport torch.nn as nn\n\n\ndef timestep_embedding(t, dim, max_period=10000):\n    \"\"\"Sinusoidal embedding for diffusion timesteps.\n\n    t: (B,) int/float tensor\n    Returns: (B, dim)\n    \"\"\"\n    half = dim // 2\n    freqs = torch.exp(\n        -math.log(max_period) * torch.arange(0, half, device=t.device, dtype=torch.float32) / half\n    )\n    args = t.float()[:, None] * freqs[None]\n    emb = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2 == 1:\n        emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=-1)\n    return emb\n\n\nclass AdaLN(nn.Module):\n    \"\"\"Adaptive LayerNorm: produces per-channel scale/shift from a condition vector.\"\"\"\n\n    def __init__(self, d_model, cond_dim):\n        super().__init__()\n        self.ln = nn.LayerNorm(d_model, elementwise_affine=False)\n        self.to_scale_shift = nn.Linear(cond_dim, 2 * d_model)\n\n    def forward(self, x, cond):\n        # x: (B, N, C), cond: (B, cond_dim)\n        x = self.ln(x)\n        s, b = self.to_scale_shift(cond).chunk(2, dim=-1)  # (B, C), (B, C)\n        return x * (1 + s[:, None, :]) + b[:, None, :]\n\n\nclass DiTBlock(nn.Module):\n    \"\"\"Encoder-style Transformer block with timestep/class conditioning via AdaLN.\n\n    This is the key structural difference vs a plain ViT block: the diffusion timestep (and optional semantic condition)\n    modulates normalization inside each block.\n    \"\"\"\n\n    def __init__(self, d_model, n_heads, d_ff, cond_dim, dropout=0.0):\n        super().__init__()\n        self.ada1 = AdaLN(d_model, cond_dim)\n        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n        self.ada2 = AdaLN(d_model, cond_dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_ff, d_model),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x, cond):\n        # x: (B, N, C)\n        a, _ = self.attn(self.ada1(x, cond), self.ada1(x, cond), self.ada1(x, cond), need_weights=False)\n        x = x + a\n        x = x + self.mlp(self.ada2(x, cond))\n        return x\n\n\nclass DiT(nn.Module):\n    \"\"\"Minimal DiT-like backbone (latent tokens -&gt; denoising prediction).\"\"\"\n\n    def __init__(self, d_model=768, depth=12, n_heads=12, d_ff=3072, n_classes=None, time_dim=256, dropout=0.0):\n        super().__init__()\n        cond_dim = time_dim\n        self.time_mlp = nn.Sequential(\n            nn.Linear(time_dim, cond_dim),\n            nn.SiLU(),\n            nn.Linear(cond_dim, cond_dim),\n        )\n        self.class_emb = nn.Embedding(n_classes, cond_dim) if n_classes is not None else None\n        self.blocks = nn.ModuleList(\n            [DiTBlock(d_model, n_heads, d_ff, cond_dim=cond_dim, dropout=dropout) for _ in range(depth)]\n        )\n        self.norm = nn.LayerNorm(d_model)\n        self.out = nn.Linear(d_model, d_model)  # placeholder: map tokens back to token-space prediction\n\n    def forward(self, x_tokens, t, y=None):\n        # x_tokens: (B, N, C) tokens of x_t (often VAE latents patchified)\n        cond = self.time_mlp(timestep_embedding(t, self.time_mlp[0].in_features))\n        if self.class_emb is not None and y is not None:\n            cond = cond + self.class_emb(y)\n        for blk in self.blocks:\n            x_tokens = blk(x_tokens, cond)\n        x_tokens = self.norm(x_tokens)\n        return self.out(x_tokens)\n\n\n# Notes:\n# - In a real diffusion model, x_tokens represent a noised sample x_t, and the output is interpreted as epsilon/v/x0 in the same space.\n# - The *DDPM-specific* part is the conditioning on t (and optional y/text) + the training/sampling process around this network.\n</code></pre> <p>Quick contrast vs VGG/CNNs (why this is a \u201cstructure\u201d story):</p> <ul> <li>VGG/CNNs bake in strong inductive biases (locality + translation equivariance via convolutions). ViT/DiT rely more on data/scale and attention\u2019s global mixing.</li> <li>ViT is usually a single-pass supervised encoder (classification/representation). DiT is used inside an iterative denoising procedure (DDPM/DDIM sampling), so the same network is called many times.</li> <li>The key DiT-specific knobs are timestep embedding \\(t\\), how conditioning is injected (AdaLN/FiLM vs cross-attention vs extra tokens), and which target you predict (\\(\\epsilon\\)/\\(v\\)/\\(x_0\\)).</li> </ul>"},{"location":"posts/vit-dit.html#references-paper-code_1","title":"References (paper + code)","text":"<ul> <li>Paper: https://arxiv.org/abs/2212.09748 (Peebles &amp; Xie, 2022, Scalable Diffusion Models with Transformers)</li> <li>Code (official-ish / common reference): https://github.com/facebookresearch/DiT</li> <li>Video: https://www.youtube.com/watch?v=vXtapCFctTI</li> </ul>"},{"location":"posts/vit-dit.html#file-references-inside-this-repo","title":"File references (inside this repo)","text":"<ul> <li>LLM/AR Transformer background: transformer.md</li> <li>Figures used by the Transformer note: assets/figures/</li> </ul>"},{"location":"posts/vit-dit.html#code-references-external","title":"Code references (external)","text":"<ul> <li>ViT implementations: timm ViT variants and Google\u2019s JAX reference (links above)</li> <li>DiT implementation: Meta\u2019s DiT repo (link above)</li> </ul>"},{"location":"posts/vit-dit.html#next-topics-to-expand-pick-one-per-installment","title":"Next topics to expand (pick one per installment)","text":"<ul> <li>ViT patch embedding math + positional embedding resizing</li> <li>ViT pretraining recipes (augmentation, regularization, distillation)</li> <li>Diffusion basics: forward/noise schedule and what the model predicts (\\(\\epsilon\\), \\(v\\), \\(x_0\\))</li> <li>DiT conditioning (AdaLN/FiLM) and why U-Nets were dominant historically</li> <li>Sampling: DDPM/DDIM and classifier-free guidance (CFG)</li> </ul>"}]}